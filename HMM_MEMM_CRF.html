<h2 id="hmm">HMM</h2>
<ul>
<li><strong>Formal definition</strong><ul>
<li>$Q = {q_1, ..., q_N}$, a set of $N$ states<ul>
<li>$q_0, q_F$, start andfinal state</li>
</ul>
</li>
<li>$O = {o_1, ..., o_T}$, a sequence of $T$ observations</li>
<li>$A = {a_{ij}}$, a transition probability matrix $A$<ul>
<li>each $a_{ij}$ representing the probability of moving from state $i$ to state $j$</li>
</ul>
</li>
<li>$B = b_i(o_t)$, a sequence of observation likelihoods, aka emission probabilities<ul>
<li>each expressing the probability of an observation $o_t$ being generated from a state $i$</li>
</ul>
</li>
</ul>
</li>
<li><strong>First-order Markov model assumptions</strong><ul>
<li>Markov assumption: $P(q_i|q<em>1...q</em>{i-1}) = P(q<em>i|q</em>{i-1})$</li>
<li>Output independence: $P(o_i|q_1...q_i...q_T, o_1...o_i...o_T) = P(o_i|q_i)$</li>
</ul>
</li>
<li><strong>Likelihood computation</strong><ul>
<li>Given an HMM $\lambda = (A,B)$ and an observation sequence $O$, determine the likelihood $P(O|\lambda)$</li>
<li>In NLP, assigns a probability to each string saying how likely that string was to have been generated by the language</li>
<li>Useful for figuring out the<ul>
<li>sequence classification: for a given observation, which category&#39;s HMM is most likely to have generated it?</li>
<li>most likely sequence: of two or more possible sequences, which one was most likely generated by a given model?</li>
</ul>
</li>
<li>Naive solution in $O(TN^T)$: $P(O) = \sum_Q P(O,Q) = \sum_Q P(O|Q)P(Q)$</li>
</ul>
</li>
<li><strong>Forward algorithm (DP)</strong> in $O(TN^2)$<ul>
<li>$\alpha_t(j)$ represents the probability of being in state $j$ after seeing the first $t$ observations</li>
<li>$\alpha_t(j) = P(o_1^t, q<em>t=j|\lambda) = \sum</em>{i=1}^N {\alpha<em>{t-1}(i) \  a</em>{ij}\  b_j(o_t)}$, where<ul>
<li>$\alpha_{t-1}(i)$, the previous forward path probability from the previous time step</li>
<li>$a_{ij}$, the transition probability from previous state $q_i$ to current state $q_j$</li>
<li>$b_j(o_t)$, the state observation likelihood of the observation $o_t$ given the current state $j$</li>
</ul>
</li>
<li>Computing the forward probabilities<ul>
<li>Initialization: $\alpha<em>1(j) = a</em>{0j}b_jo(1)$, for $1\leq j\leq N$</li>
<li>Recursion: $\alpha<em>t(j) = \sum</em>{i=1}^N \alpha<em>{t-1}(i) \  a</em>{ij}\  b_j(o_t)$, for $1 \leq j \leq N, 1 &lt; t \leq T$</li>
<li>Termination: $P(O|\lambda) = \alpha_T(q<em>F) = \sum</em>{i=1}^N \alpha<em>T(i) \alpha</em>{iF}$</li>
</ul>
</li>
</ul>
</li>
<li><strong>Decoding</strong>, most likely state sequence<ul>
<li>Given as input an HMM $\lambda$ and a sequence observations $O$, find the most probable sequence of states $Q$</li>
<li>In NLP, used for sequence labeling, determines the globally best assignment of tags to all tokens</li>
</ul>
</li>
<li><strong>Viterbi algorithm (DP)</strong> in $O(TN^2)$<ul>
<li>$v_t(j)$ represents the probability that the HMM is in state $j$ after seeing the first $t$ observations and passing through the most probable state sequence $q_0^{t-1}$. The value of each $v_t(j)$ is computed by recursively taking the most probable path that could lead to this cell</li>
<li>$v<em>t(j) = \max</em>{q_0^{t-1}}P(q_0^{t-1}, o_1^t, q<em>t=j | \lambda) = \max</em>{i=1}^N v<em>{t-1}(i)a</em>{ij}b_j(o_t)$, where<ul>
<li>$v_{t-1}(i)$, the previous Viterbi path probability from the previous step</li>
</ul>
</li>
<li>The Viterbi algorithm is identical to the forward algorithm except that it takes the $\max$ over the previous path probabilities whereas the forward algorithm takes the $\text{sum}$.</li>
<li>Computing the Viterbi probabilities<ul>
<li>Initialization: $v<em>1(j) = \alpha</em>{0j} b_j(o_1)$, for $1 \leq j \leq N$ and $bt_1(j) = 0$</li>
<li>Recursion: $v<em>t(j) = \max</em>{i=1}^N v<em>{t-1}(i)a</em>{ij}b_j(o_t)$ and $bt<em>t(j) = \text{arg}\max</em>{i=1}^N v<em>{t-1}(i)a</em>{ij}b_j(o_t)$, for $1 \leq j \leq N, 1 &lt; t \leq T$</li>
<li>Termination: best score: $P^<em> = v_T(q<em>F) = \max</em>{i=1}^N v<em>T(i)a</em>{iF}$, the start of backtrace: $q_T^</em> = bt_T(q<em>F) = \text{arg}\max</em>{i=1}^N v<em>T(i)a</em>{iF}$</li>
</ul>
</li>
</ul>
</li>
<li><strong>HMM Learning</strong><ul>
<li>Supervised learning: all training sequences are completely labeled</li>
<li>Unsupervised learning: unlabeled, generally know the number of tags (states)<ul>
<li>Baum-Welch algorithm / Forward-Backward algorithm</li>
<li>A special case of Expectation Maximization (EM)</li>
</ul>
</li>
<li>Supervised parameter estimation<ul>
<li>Estimate state transition probabilities based on tag bigram and unigram statistics in the labeled data<ul>
<li>$a_{ij} = C(q_t=s<em>i, q</em>{t+1}=s_j) / C(q_t=s_i)$</li>
</ul>
</li>
<li>Estimate the observation probabilities based on tag co-occurence statistics in the labeled data<ul>
<li>$b_j(k) = C(q_i=s_j, o_i=v_k) / C(q_i=s_j)$</li>
</ul>
</li>
</ul>
</li>
<li>Evaluating taggers<ul>
<li>Train on training set of labeled sequences</li>
<li>Tune parameters based on performance on a development set</li>
<li>Measure accuracy on a disjoint test set</li>
</ul>
</li>
<li>Issues with standard HMMs<ul>
<li>Standard HMMs only use word identity as features, cannot use richer representations</li>
<li>HMMs are trained to maximize joint distributions during training, but in test, conditional prob. is asked</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="memm">MEMM</h2>
<ul>
<li><strong>Basics</strong><ul>
<li>MEMM is a sequence model adaptation of the MaxEnt (multinomial logistic regression) classifer</li>
<li>The reason to use a discriminative sequence model is that discriminative models make it much easier to incorporate a much wider variety of features. In HMM,s, all computation in based on the two probabilities $P(t|t)$ and $P(w|t)$. It&#39;s hard to add more features in a HMM-like model.</li>
<li>A basic MEMM POS tagger conditions on the observation word itself, neighboring words, and previous tags, and various combinations, using feature templates.</li>
<li>The simplest way to turn the MaxEnt classifier into a sequence model is to build a local classifier that classifies each word left to right, making a hard classification of the first word in the sentence, then a hard decision on the second word, and so on. This is a greedy decoding algorithm, because we greedily choose the best tag for each word. But a greedy classifier cannot temper its decision with information from future decisions, causing sufficient drop in performance that we don&#39;t use it.</li>
<li>MEMM taggers train logistic regression models to pick the best tag given an observation word and its context and the previous tags, and then use Viterbi to choos eth ebest sequence of tags for the sentence. More complex augmentions of the MEMM exist, like CRF tagger.</li>
</ul>
</li>
<li><strong>HMM vs. MEMM</strong><ul>
<li>HMM: generative sequence model<ul>
<li>easy to train generative model, features for a state must be independent</li>
<li>Tagging using Bayes&#39; rule: $\hat{T} = \text{arg}\max_T P(W|T)P(T) = \text{arg}\max_T \prod_i P(w_i|t_i) \prod_i P(t<em>i|t</em>{i-1})$</li>
</ul>
</li>
<li>MEMM: discriminative sequence model<ul>
<li>multiple cascaded classifiers, features can be arbitrary</li>
<li>Tagging directlying useing the posterior: $\hat{T} = \text{arg}\max_T P(T|W) = \text{arg}\max_T \prod_i P(t_i|w<em>i, t</em>{i-1})$</li>
</ul>
</li>
</ul>
</li>
<li><strong>Naive Bayes like feature-rich HMMs</strong><ul>
<li>extend the HMM so that each state generates multiple features</li>
</ul>
</li>
<li>Instead of an HMM, classify each token. Don&#39;t learn transition probabilities, instead constrain them at test time<ul>
<li>Idea: Replace generative model in HMM with a logistic regression, aka, max entropy model</li>
<li>Idea: Replace generative model in HMM with a maxent model, where state depends on observations and previous state, find $P(s_t|x<em>t, s</em>{t-1})$</li>
</ul>
</li>
<li><strong>From HMM to MEMM</strong><ul>
<li>MEMM solution: use more descriptive features (e.g., $b_0$: is capitalized, $b_1$: is in plural, et.c.)</li>
<li>Real-valued features can also be handled</li>
<li>Here features can be pairs $\langle b,s \rangle$, where $b$ is feature of observations, and $s$ is destination state</li>
<li>A separate logistic regression classifier for each state</li>
</ul>
</li>
<li><strong>MEMM inference</strong><ul>
<li>$\alpha<em>{t+1}(s) = \sum</em>{s&#39;\in S} \alpha<em>t(s&#39;) P</em>{s&#39;}(s|o_{t+1})$</li>
<li>$v<em>{t+1}(s) = \max</em>{s&#39;\in S} v<em>t(s&#39;) P</em>{s&#39;}(s|o_{t+1})$</li>
</ul>
</li>
<li><strong>Issues of MEMM</strong><ul>
<li>Local bias problem: no global view of features; probabilities of outgoing arcs normalized separately for each state.<ul>
<li>Don&#39;t normalize probabilities locally. Preference of states with lower number of transitions over others</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="probabilistic-graphical-models">Probabilistic Graphical Models</h2>
<ul>
<li>GMs use directed or undirected graphs over a set of random variables to explicitly specify variable dependencies</li>
<li><strong>Bayesian networks</strong>: directed acyclic graphs (DAG) that indicate causal structure<ul>
<li>In a DAG, notes are random variables, edges indicate causal influences</li>
<li>Each node has a conditional probability table (CPT) that gives the probability of each of its values given every possible combination of values for its parents (conditioning case). Roots of the DAG taht have no parents are given prior probabilities.</li>
<li>A Bayesian network implicitly defines a joint distribution<ul>
<li>$P(x_1...x<em>n) = \prod</em>{i=1}^n P(x_i|\text{Parents}(x_i))$</li>
<li>If $x_i$ is the root node, just use it as prior $P(x_i)$</li>
</ul>
</li>
</ul>
</li>
<li><strong>Markov networks</strong>: undirected graphics that capture general dependencies<ul>
<li>The Markov blanket of a node $X$ in a Markov net is the set of its neighbors in the graph</li>
<li>The disribution of a Markov net is most compactly described in terms of a set of potential functions (aka. factors, compatibility functions), $\phi_k$ for each clique $k$ in the graph</li>
</ul>
</li>
<li><strong>Generative vs. discriminative</strong>: sequence labeling modesl<ul>
<li>HMMs are generative models and are not directly designed to maximize the performance of sequence labeling. They model the joint distribution $P(O,Q)$</li>
<li>Conditional random fields (CRFs) are specifically designed and trained to maximize performance of sequence labeling. They model the conditional distribution $P(Q|O)$</li>
<li>Classification: Naive Bayes (generative) -&gt; logistic regression (discriminative)</li>
<li>Sequence labeling: HMM (generative) -&gt; linear-chain CRF (discriminative)</li>
</ul>
</li>
<li><strong>Conditional random fields</strong> (CRFs)<ul>
<li>CRFs are a discriminative approach to sequence labeling whereas HMMs are generative</li>
<li>Discriminative methods are usually more accurate since they are trained for a specific performance task</li>
<li>CRFs also easily allow adding additional token features without making additional indepenence assumptions</li>
<li>MEMM does local normalization, which has label bias issue, whereas CRFs use global normalization to fix it</li>
<li>CRFs are a state-of-art method for sequence labeling</li>
</ul>
</li>
</ul>
