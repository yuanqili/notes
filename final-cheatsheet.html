<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<h1 id="cs170-review">CS170 Review</h1>
<h2 id="kernels-and-processes">Kernels and processes</h2>
<h3 id="four-fundamental-os-concepts">Four fundamental OS concepts</h3>
<ul>
<li><strong>Thread</strong>: single unique execution context; program counter, registers, execution flags, stack</li>
<li><strong>Address space with translation</strong>: programs execute in an address space that is distinct from the memory space of the physical machine</li>
<li><strong>Process</strong>: an instance of an executing program is a process consisting of an address space and one or more threads of control; a process in memory includes a program counter, stack and heap, and data/instruction section (PCB)</li>
<li><strong>Dual mode operation</strong>: onely the system has the ability to access certain resouces; the OS and the hardware are protected from user programs and user programs are isolated from one another by controlling the translation from the program virtual addresses to machine physical addresses
<ul>
<li>User to kernel: sets system mode and saves the user PC; OS code carefully puts aside user state then performs the necessary operations</li>
<li>Kernel to user: user transition clears system mode and restores appropriate user PC; user program returns from interrupt</li>
</ul></li>
</ul>
<p>To run a program, the OS loads code and data segments of executable file into memory, creates stack and heap, trnasfers control to it and provides ervices to it. As a process executes, it changes <strong>state</strong>:</p>
<ul>
<li><strong>New</strong>: the process is being created</li>
<li><strong>Running</strong>: instructions are being executed</li>
<li><strong>Waiting</strong>: the process is waiting for some event to occur</li>
<li><strong>Ready</strong>: the process is waiting to be assigned to a processor</li>
<li><strong>Terminated</strong>: the process has finished execution</li>
</ul>
<p><strong>Process control block</strong> (PCB) stores information associated with each process: process state, program counter, copy of CPU registers, memory-management information (page table), accounting information, IO status information, etc.</p>
<p>When CPU switches to another process with a new address space, the system must save the state of the old process and load the saved state for the new process via a <strong>context switch</strong>. The context of a process represented in the PCB.</p>
<h3 id="process-creation">Process creation</h3>
<ul>
<li>Parent process create child processes, identified via a process id (pid)</li>
<li>Options in resource sharing: parent and children share all resources; children share subset of parent's resources; parent and children share no resources</li>
<li>Parent and children execute concurrently; parent watis until children terminate (by join)</li>
<li>Options in address space: child duplicate of parent, or has another program being loaded</li>
<li>In UNIX: <code>fork</code> creates new process with duplicated address space with a copy of some code and data; <code>exec</code> used after a <code>fork</code> to replace the process' memory space with a new program</li>
<li>Summary of <code>fork</code>
<ul>
<li>creates a child PCB with new pid</li>
<li>allocates memory for the child and duplicate everything from parent, including text/heap/stack, but notice child space is a copy of parent space; all fds that are open in hte parent are duplicated in the child (file <code>read()/write()</code> set offsets are shared); many environmental setings and ahred segments are inherited by child (parent and child can communicate through shared segments)</li>
<li>add the child process to ready queue</li>
</ul></li>
</ul>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">int</span> main() {
    <span class="dt">int</span> pid = fork();
    <span class="cf">if</span> (pid &lt; <span class="dv">0</span>) { exit(<span class="dv">-1</span>); } <span class="co">// error occured</span>
    <span class="cf">else</span> <span class="cf">if</span> (pid == <span class="dv">0</span>) { execlp(<span class="st">&quot;/bin/ls&quot;</span>, <span class="st">&quot;ls&quot;</span>, NULL); } <span class="co">// child process</span>
    <span class="cf">else</span> { wait(NULL); exit(<span class="dv">0</span>); } <span class="co">// parent process</span>
}</code></pre></div>
<h3 id="process-termination">Process termination</h3>
<ul>
<li>Process executes last statement and asks the OS to delete it (exit)</li>
<li>Output data from child to parent (via wait), process resources are deallocated</li>
<li>Parent may terminate children processes: task assigned to child is no longer required of if parent exits</li>
</ul>
<h3 id="unix-signals">UNIX signals</h3>
<ul>
<li>A event similar to hardware interrupt without priorities</li>
<li>Used to informa a user process of an event, e.g., user pressed delete key</li>
<li>Each signal is represented by a numeric value: e.g., SIGINT(2), SIGKILL(9), etc.</li>
<li>A UNIX signal is raised by a process using a system call <code>kill(signal, pid)</code> or a shell command <code>kill -s signal pid</code></li>
<li>Each signal is maintained as a single bit in the process table entry of the receving process: the bit is set when the corresponding signal arrives (no waiting queues)</li>
<li>A signal is processed as soon as the process enters in user mode</li>
<li>3 ways to handle a signal
<ul>
<li>ignore it: <code>signal(signum, SIG_IGN)</code></li>
<li>run the default handler: <code>signal(signum, SIG_DFL)</code></li>
<li>run a user-defined handler: <code>signal(signum, handler)</code></li>
</ul></li>
</ul>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="pp">#include </span><span class="im">&lt;signal.h&gt;</span>
<span class="dt">void</span> cnt(<span class="dt">int</span> sig) {
    <span class="at">static</span> <span class="dt">int</span> count = <span class="dv">0</span>;
    <span class="cf">if</span> (count == <span class="dv">1</span>) signal(SIGINT, SIG_DFL);
}
<span class="dt">int</span> main() {
    signal(SIGINT, cnt);
    <span class="cf">while</span>(<span class="dv">1</span>);
}</code></pre></div>
<h3 id="unix-pipes-for-ipc">UNIX pipes for IPC</h3>
<ul>
<li>The pipe interface is intended to look a file interface</li>
<li><code>pipe(fd)</code> creates the pipe and kernel allocates a buffer with two pointers <code>fd[0]</code> to read and <code>fd[1]</code> to write</li>
<li>pipe handles are copied on <code>fork</code> (just like a usual fd)</li>
<li><code>ls|wc</code>
<ul>
<li>process1 <code>dup2(fd[1], 1); close(fd[0]); execvp(&quot;ls, ...&quot;)</code></li>
<li>process2 <code>dup2(fd[0], 0); close(fd[1]); execvp(&quot;wc&quot;, ...)</code></li>
</ul></li>
</ul>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">int</span> fd[<span class="dv">2</span>]; pipe(fd); <span class="co">// fd[0] is read end, fd[1] is write end</span>
<span class="cf">if</span> (fork() == <span class="dv">0</span>) { read(fd[<span class="dv">0</span>], buffer, size); } <span class="co">// child process</span>
<span class="cf">else</span> { write(fd[<span class="dv">1</span>], <span class="st">&quot;hello&quot;</span>, size); } <span class="co">// parent process</span></code></pre></div>
<h2 id="concurrency-and-threads">Concurrency and threads</h2>
<p>A thread is a fundamental unit of CPU utilization that forms the basis of multithreaded computer systems</p>
<ul>
<li>Shared state: heap, global variables, code</li>
<li>Per-thread state: thread control block (TCB), stack information, saved registers, thread metadata</li>
<li>Benefits: responsiveness, resource sharing, economy, scalability</li>
<li>Thread abstraction: infinite number of processors, threads execute with variable speed</li>
<li>Programs must be designed to work with any schedule</li>
<li>Two threads run concurrently if their logical flows overlap in time; otherwise, they are sequential</li>
<li>Same as process: each has its own logical control flow, each can run concurrently, each is context switched</li>
<li>Different from process: threads share code and data while proceses (typicall) do not, threads are somewhat cheaper than processes with less overhead</li>
</ul>
<h3 id="posix-threads-pthreads-interface">POSIX threads (pthreads) interface</h3>
<ul>
<li>Creating and join threads: <code>pthread_create</code>, <code>pthread_join</code></li>
<li>Determining your tid: <code>pthread_self</code></li>
<li>Terminating threaads: <code>pthread_cancel</code>, <code>pthread_exit</code>
<ul>
<li><code>exit</code> terminates all threads; <code>return</code> terminates current thread</li>
</ul></li>
</ul>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="pp">#include </span><span class="im">&lt;pthread&gt;</span>
<span class="dt">void</span> *PrintHello(<span class="dt">void</span> *id) { printf(<span class="st">&quot;...&quot;</span>); }
<span class="dt">int</span> main() {
    <span class="dt">pthread_t</span> thread0, thread1;
    pthread_create(&amp;thread0, NULL, PrintHell, (<span class="dt">void</span>*) <span class="dv">0</span>);
    pthread_create(&amp;thread1, NULL, PrintHell, (<span class="dt">void</span>*) <span class="dv">1</span>);
    pthread_join(thread0, NULL);
    pthread_join(thread1, NULL);
}</code></pre></div>
<h3 id="nachos-threads">Nachos threads</h3>
<ul>
<li><code>Thread(char *name)</code>: create a thread</li>
<li><code>Fork(VoidFunctionPtr func, int arg)</code>: place this thread in a ready queue for executing a function
<ul>
<li>Allocate thread control block (TCB)</li>
<li>Allocate and setup stack: build stack frame for base of stack; put func and args on stack</li>
<li>Put thread on ready list, and it will run sometime later</li>
<li><code>stub(func, args) { (*func)(args); thread_exit(); }</code></li>
</ul></li>
<li><code>Yield()</code>: suspend the calling thread and the system selects a new ready one for execution</li>
<li><code>Sleep()</code>: suspend the current thread, change its sate to BLOCKED, and remove it from the ready list</li>
<li><code>Finish()</code>, <code>~Thread()</code></li>
</ul>
<h3 id="kernel-threads-vs-user-level-threads">Kernel threads vs user-level threads</h3>
<ul>
<li>Kernel threads are recognized and supported by the OS kernel; OS explicitly performs scheduling and context switching of kernel threads</li>
<li>User-level thread management done by user-level threads library
<ul>
<li>OS kernel does not know/recognize there are multiple threads running in a user program</li>
<li>The user program (library) is responsible for scheduling and context switching of its threads</li>
</ul></li>
</ul>
<h2 id="processthread-synchronization">Process/thread synchronization</h2>
<h3 id="synchronization-problem">Synchronization problem</h3>
<ul>
<li>Race condition: two or more processes (threads) are reading and writing on shared data and the final result depends on who runs precisely and when</li>
<li>Critical section: the part of the program where shared variables are accessed</li>
<li>Deadlock: two or more threads (or processes) are waiting indefinitely for an event that can be only caused by one of these waiting processes</li>
<li>Starvation: indefinite blocking. Deadlock is starvation but not vice versa</li>
</ul>
<h3 id="property-of-cirtical-section-solution">Property of cirtical-section solution</h3>
<ul>
<li>Mutual exclusion: only one can enter the critical section, providing safety</li>
<li>Progress: if some processes wish to enter their critical section and nobody is in the critical section, then one of them will enter in a limited time, providing liveness</li>
<li>Bounded waiting: if one process starts to wait for entering an critical section, there is a limit on the number of times other processes entering the section before this process enters, providing liveness</li>
<li>Locks, semaphore, condition variables</li>
</ul>
<h3 id="lock-prevents-someone-from-doing-something"><strong>Lock</strong> prevents someone from doing something</h3>
<ul>
<li><code>void Acquire()</code> atomically waits until the lock is unlocked, and then sets the lock to be locked.</li>
<li><code>void Release()</code> atomically changes the state to be unlocked. Only the thread who owns the lock can release.</li>
<li>Thread waits if there is a lock.</li>
<li>It enters the critical after acquiring a lock.</li>
<li>Only the thread who locks can unlock.</li>
<li>Lock can be in one of two states: locked or unlocked</li>
<li>Typically associate a lock with a piece of shared data for mutual exclusion. When a thread needs to access, it first acquires the lock, and then accesses data. Once access completes, it releases the lock</li>
</ul>
<h3 id="lock-implementation">Lock implementation</h3>
<ul>
<li>Atomic load/store are complex and error prone</li>
<li>Alternatively, interrupt disabling/enabling and hardware primitives; avoid context-switching by
<ul>
<li>preventing external events by disabling interrupts</li>
<li>Avoid internal events</li>
</ul></li>
<li>But interrupt disabling/enabling doesn't work/scale well on multiprocessor</li>
<li>ALternatively, atomic instruction sequences
<ul>
<li>These instructions read a value from memory and write a new value atomically</li>
<li>Hardware is responsible for implementing this correctly</li>
<li>Unlike disabling interrupts, can be used on both uniprocessors and multiprocessors</li>
</ul></li>
<li>Examples of atomic instructions
<ul>
<li><code>TestAndSet(&amp;address)</code> fetch old value, set it to 1</li>
<li><code>Swap(&amp;address, register)</code> swap value in the address and register</li>
<li><code>CompareAndSwap(&amp;address, reg1, reg2)</code> sets address to reg2 if it equals reg1</li>
</ul></li>
<li>Spin lock using <code>TestAndSet</code>: mutual exclusion but not bounded waiting, busy waiting
<ul>
<li>shared boolean variable as lock: true means it is acquired by others; initialized to false</li>
<li>acquire: <code>while(TestAndSet(&amp;lock))</code></li>
<li>release: <code>lock = false</code></li>
<li>Bette with sleep: minimizes busy waiting, only busy-waiting to atomically check lock value</li>
<li>pro: machine can receive interrupts; work on a multiprocessor</li>
<li>con: inefficient spin lock; no guarantee on bounded waiting</li>
</ul></li>
</ul>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">int</span> guard = <span class="kw">false</span>; <span class="dt">int</span> value = free;
Acquire() {
    <span class="cf">while</span>(test&amp;set(guard));
    <span class="cf">if</span> (value == busy) {
        put thread on waiting queue
        sleep <span class="kw">and</span> set guard to <span class="kw">false</span>
    } <span class="cf">else</span> {
        value = busy
        guard = <span class="kw">false</span>
    }
}
Release() {
    <span class="cf">while</span>(test&amp;set(guard));
    <span class="cf">if</span> anyone on waiting queue {
        take thread off waiting queue
        place it on ready queue
    } <span class="cf">else</span> {
        value = free
    }
    guard = <span class="kw">false</span>
}</code></pre></div>
<h3 id="semaphore">Semaphore</h3>
<ul>
<li><code>void P()</code> atomically waits until the counter is greater than 0 and then decreases the counter</li>
<li><code>void V()</code> atomically decreases the counter</li>
<li>It uses a nonnegative integer variable, can only be accessed/modified via two individual (atomic) operations
<ul>
<li><code>wait(S) { while S &lt;= 0: wiating in a queue; S--; }</code></li>
<li><code>signal(S) { wakeup some waiting thread; S++; }</code></li>
</ul></li>
<li>Counting semaphore: initial value representing how many threads can be in the critical section</li>
<li>Binary semaphore (aka mutex lock): integer value ranged between 0 and 1</li>
<li>Producer thread: <code>space-&gt;P(), data-&gt;V()</code></li>
<li>Consumer thread: <code>data-&gt;P(), space-&gt;V()</code></li>
</ul>
<h3 id="condition-variable">Condition variable</h3>
<ul>
<li><code>void Wait(Lock *myLock)</code> atomically releases the lock and waits. When it is returned, the lock is reacquired again.</li>
<li><code>void Signal(Lock *myLock)</code> wake up one waiting thread to run. The lock is not released.</li>
<li><code>void Broadcast(Lock *myLock)</code> wake up all threads waiting on the condition. The lock is not released.</li>
<li>Hoare-style scheduling
<ul>
<li>Signaler gives lock, CPU to waiter; waiter runs immediately</li>
<li>Waiter gives up lock, processor back to signaler when it exits critical section or if it waits again</li>
</ul></li>
<li>Mesa-style shceduling
<ul>
<li>Signaler keeps lock and continues to process</li>
<li>Waiter placed on ready queue with no special priority</li>
<li>Practically, need to check condition again after wait</li>
</ul></li>
</ul>
<h3 id="design-advices-for-synchronization">Design advices for synchronization</h3>
<ul>
<li>Allocate one lock for each shared variable or a group of shared variables</li>
<li>Add condition variables (waking up a thread is triggered by some condition)</li>
<li>Fine-grain: More performance flexibility and scalability but possibly more design complexity and synchronization overhead</li>
<li>Coarse-grain: Easy to ensure correctness but possibly difficult to scale</li>
</ul>
<h3 id="read-writer-lock">Read-writer lock</h3>
<ul>
<li><code>pthread_rwlock_rdlock()</code>: multiple readers can acquire the lock if no writer holds the lock.</li>
<li><code>pthread_rwlock_wrlock()</code>: the calling thread acquires the write lock if no other thread (reader or writer) holds the read-write lock rwlock. Otherwise, the thread blocks until it can acquire the lock. Writers are favored over readers of the same priority to avoid writer starvation.</li>
<li>Reader: wait until no writers; access database; checkout (wake up a waiting writer)</li>
<li>Writer: wait until no active readers or writers; access database; checkout (wakeup waiting readers or writer)</li>
</ul>
<h2 id="main-memory-and-address-translation">Main memory and address translation</h2>
<h3 id="how-to-translate-address">How to translate address</h3>
<ul>
<li>Contiguous memory allocation</li>
<li>Segmentation</li>
<li>Address tarnslation with paging</li>
<li>TLB support and performance impact</li>
</ul>
<h3 id="objective-of-memory-management-run-programs">Objective of memory management: run programs</h3>
<ul>
<li>Get CPU cycle and allocate memory</li>
<li>Load instruction and data segments of executable file into memory</li>
<li>Create stack and heap</li>
<li>Set the starting address and execute</li>
<li>Provide services to it</li>
</ul>
<h3 id="role-of-memory-management">Role of memory management</h3>
<ul>
<li>Manage data/instructions in memory in order to execute</li>
<li>Keep track of which parts of memory are currently being used by whom</li>
<li>Decide which processes (or parts thereof) and data to move into and out of memory</li>
<li>Allocate and deallocate memory space as needed</li>
</ul>
<h3 id="logical-vs.-physical-address-space">Logical vs. physical address space</h3>
<ul>
<li>Logical (virtual) address: generated by the CPU</li>
<li>Physical address: address seen by the memory unit</li>
<li>Same in compile-time and load-time address-binding schemes</li>
<li>Differ in execution-time address-binding scheme</li>
</ul>
<h3 id="binding-of-instructions-and-data-to-memory">Binding of instructions and data to memory</h3>
<ul>
<li>Compile time (gcc): If memory location known a priori, absolute code can be generated</li>
<li>Load time (ld): Must generate relocatable code if memory location is not known at compile time</li>
<li>Execution time (dll): Binding delayed until run time if the process can be moved during its execution from one memory segment to another.</li>
</ul>
<h3 id="address-space-translation">Address space translation</h3>
<ul>
<li>Address space
<ul>
<li>All the addresses and state a process can touch</li>
<li>Each process and kernel has different address space</li>
</ul></li>
<li>Program operates in an address space that is distinct from the physical memory space of the machine</li>
<li>CPU issues virtual address to MMU which translates to physical memory to memory</li>
<li>Translation provides protection and process address space isolation. With translation, every program can be linked/loaded into same region of user address space</li>
</ul>
<h3 id="allocation-method-contiguous-allocation">Allocation method: Contiguous allocation</h3>
<ul>
<li>Main memory has two partitions: Resident OS usually held in low memory; user processes then held in high momery</li>
<li>Hardware support and protection
<ul>
<li>Base register contains value of smallest physical address</li>
<li>Limit register contains range of logical addresses (each logical address must be less than the limit register)</li>
<li>MMU maps logical address dynamically</li>
</ul></li>
<li>During switch, kernel loads new base/limit from PCB</li>
<li>Issues with simple R&amp;B method
<ul>
<li><strong>External fragment</strong> problem (free gaps between used slots): Not every process is the same size; over time, memory space becomes fragmented</li>
<li>Missing support for sparse address space: Would like to have multiple chunks/program, e.g., code, data, stack</li>
<li>Hard to do inter-process share</li>
</ul></li>
</ul>
<h3 id="allocation-method-segmentation">Allocation method: Segmentation</h3>
<ul>
<li>Memory-management scheme that supports user semantic view of memory</li>
<li>A program is a collection of segments</li>
<li>A segment is a logical unit such as code, data, stack (can be shared)</li>
<li>Each segment is given region of contiguous memory: base and limit, can reside anywhere in physical memory</li>
<li>CPU issues a virtual address <code>(seg, offset)</code>, if <code>offset &lt; ST[seg].limit</code>, then the physical address is <code>ST[seg].base + offset</code>; otherwise raises an exception</li>
<li>Issues with segmentation
<ul>
<li>High overhead of managing a large number of variable size and dynamically growing memory: Memory is divided into many used/unused regions over time; not easy to find space to fit for a new segment</li>
<li>External fragmentation: Free gaps between allocated chunks</li>
<li>Internal fragmentation: Don't need all memory within allocated chunks</li>
</ul></li>
</ul>
<h3 id="allocation-method-paging">Allocation method: Paging</h3>
<ul>
<li>Divide physical memory into fixed-sized blocks called frames or pages</li>
<li>Divide logical memory into blocks of same size called pages or logical pages</li>
<li>Translation conducted by page table which is kept in main memory
<ul>
<li>Page-table base register (PTBR) points to the page table</li>
<li>Page-table length register (PRLR) indicates size of the page table</li>
</ul></li>
<li>Useful constants: 2^10 = 1K, 2^20 = 1M, 2^30 = 1G, 2^40 = 1T</li>
<li>A page table per process is needed to translate logical to physical addresses</li>
<li>We can use a vector of bits to represent availability of each page</li>
<li>Address generated by CPU is divided into a page number (p) and a page offset</li>
</ul>
<ol start="4" style="list-style-type: lower-alpha">
<li><ul>
<li>Offset from virtual address copied to physical address</li>
<li>Virtual page number is all remaining bit, the translated physical page number is copied from page table into physical address</li>
<li>Physical address = <code>PageTable[LogicalPageNumber] + PageOffset</code></li>
</ul></li>
</ol>
<ul>
<li>Each page is associated with permission bits (e.g., valid, read, write, etc.)</li>
<li>Page table size is determined by the number of pages in virtual space</li>
</ul>
<h3 id="tlb-for-faster-address-translation"><strong>TLB</strong> for faster address translation</h3>
<ul>
<li>Every data/instruction access requires two memory access. Speed can be improved by using a special fast-lookup hardware cache called associative memory or translation look-aside buffers (TLBs)</li>
<li>The TLB stores the recent translations (i.e., entries) of virtual memory to physical memory and can be called an address-translation cache.</li>
<li>Address translation for logical page p
<ul>
<li>If p is in associative register, get physical page number out</li>
<li>Otherwise get frame number from page table in memory</li>
</ul></li>
<li>Typical TLB: 8~4096 entries; 0.5~1 clock cycle to access; 10~100 cycles miss penalty; 0.01~10% miss rate</li>
<li>Effective (average) address translation cost: cost(TLB lookup) + cost(full translation) * TLB miss rate</li>
<li>Total memory access cost: cost(avg address translation) + cost(memory access)</li>
<li>Memory control infomation are stored infomation in
<ul>
<li>Thread control block (TCB): stack info, registers and machine state</li>
<li>Process control block (PCB): process id data, state data, control data (memory space management, pointer to a page table)</li>
</ul></li>
</ul>
<h3 id="shared-pages-through-paging">Shared pages through paging</h3>
<ul>
<li>Shared code
<ul>
<li>One copy of read-only code shared among processes</li>
<li>Shared code must appear in same location in the logical address space of all processes</li>
</ul></li>
<li>Private code and data
<ul>
<li>Each process keeps a separate copy of the code and data</li>
<li>The pages for the private code and data can appear anywhere in the logical address space</li>
</ul></li>
<li>Copy-on-write (COW): Lazy copy during process creation
<ul>
<li>Optimization of Unix system call <code>fork</code>: A child process copies address space of parent. Most of time it is wasted as the child performs <code>exec</code>.</li>
<li>COW allows both parent and child processes to initially share the same pages in memory. A shared page is duplicated only when modified. It allows more efficient process creation as only modified pages are copied.</li>
</ul></li>
</ul>
<h3 id="page-table-entry-pte">Page table entry (PTE)</h3>
<ul>
<li>Used to memorize a page is shared, detect need for duplication</li>
<li>Invalid PTE can imply different things: region of address space is actually invalid or the page/directory is just somewhere else than memory</li>
<li>Validaty checked first: OS can use other bits for location info</li>
<li>COW uses PTE which contains bit indicating a page is shared with a parent</li>
<li>Demand paging keeps only active pages in memory, place others on disk and mark their PTEs invalid</li>
<li>x86 PTE
<ul>
<li>Address format: (10, 10, 12-bit offset)</li>
<li>Intermediate page tables called directories: (20-bit page frame number, 12-bit flags)</li>
</ul></li>
<li>Zero fill on demand
<ul>
<li>Security and performance advantages: New pages carry no information</li>
<li>Give new pages to a process initially with PTEs marked as invalid</li>
<li>During access time, page fault causes physical frames to be allocated and filled with zeros</li>
<li>Often, OS creates zeroed pages in background</li>
</ul></li>
</ul>
<h3 id="one-level-page-table">One-level page table</h3>
<ul>
<li>Maximum size of logical space = # entry * page size</li>
<li>Each page table needs to fit into a physical memory page because a page table needs consecutive space. Memory allocated to a process is a sparse set of nonconsecutive pages.</li>
<li>One-level page table cannot handle large space, for example
<ul>
<li>32-bit address space with 4KB per page</li>
<li>Page table would contain 2<sup>32/2</sup>12 = 1 million entries</li>
<li>4B per entry: Need a 4MB page table with contiguous space (impossible)</li>
<li>Only 4KB / 4B = 1K entries, hence 1K * 4KB = 4MB logical space</li>
</ul></li>
<li>Pro: Simple memory allocation, easy to share</li>
<li>Con: Cannot handle a large (sparse) virtual address space, e.g., on Unix, code starts at 0, stack starts at 2^32-1</li>
<li>Con: Not all pages are used all the time. It would be nice to have working set of page table in memory</li>
</ul>
<h3 id="two-level-page-table">Two-level page table</h3>
<ul>
<li>Address translation scheme: Logical address is of the form (p1, p2, d), where p1 is an index into the outer level-1 page table, p2 is the displacement within the page of the level-2 inner page table</li>
<li>Tables are of fixed size (1K entries, 4K per entry, 4KB in total)</li>
<li>Valid bits on PTE: Not every level-2 table is needed. Even when exist, level-2 tables can reside on disk if not in use</li>
<li>Design consideration on paging
<ul>
<li>Bits of d = log (page size)</li>
<li>Bits of p1 &gt;= log (# entries in level-1 table)</li>
<li>Bits of p2 = log (# entries in level-2 table)</li>
<li># physical pages &lt;= 2^(PTE size of level-2 table)</li>
<li>Physical space size = # physical pages * page size</li>
<li>Logical space size = # entry in level-1 table * # entry in level-2 table * page size</li>
</ul></li>
<li>Example: A logical address (32-bit machine with 4KB page size) is divded into
<ul>
<li>A page number area with 20 bits, a page offset consisting of 12 bits</li>
<li>Each PTE uses 4B: Hence it is divided into 10-10-12-bit</li>
<li>Maximum logical space size: 1K * 1K * 4KB = 4GB</li>
<li>What if we use 2B for each PTE?
<ul>
<li>Logical space size remains the same (0.5K * 2K * 4KB)</li>
<li>Physical space decreases (2^16 * 4K)</li>
</ul></li>
</ul></li>
</ul>
<h3 id="three-level-page-table">Three-level page table</h3>
<ul>
<li>Design consideration on paging
<ul>
<li>Bits of d = log (page size)</li>
<li>Bits of p1 &gt;= log (# entries in level-1 table)</li>
<li>Bits of p2 = log (# entries in level-2 table)</li>
<li>Bits of p3 = log (# entries in level-3 table)</li>
<li># physical pages &lt;= 2^(entry size of level-3 table)</li>
<li>Physical space size = # physical pages * page size</li>
<li>Logical space size = # entry in level-1 table * # entry in level-2 table <em> # entry in level-3 table </em> page size</li>
</ul></li>
</ul>
<h3 id="hashed-page-table">Hashed page table</h3>
<ul>
<li>Common in address spaces &gt; 32bits</li>
<li>Size of page table grows proportionally as large as amount of virtual memory allocated to processes</li>
<li>Use hash table to limit the cost of search
<ul>
<li>To one, or at most a few, page-table entries</li>
<li>One hash table per process</li>
<li>This page table contains a chain of elements hasing to the same location</li>
</ul></li>
<li>Use this hash table to find the physical page of each logical page: If a match is found, the corresponding physical frame is extracted</li>
</ul>
<h3 id="inverted-page-table">Inverted page table</h3>
<ul>
<li>One hash table for all processes</li>
<li>One entry for each real page of memory
<ul>
<li>Entry consists of the virtual address of the page stored in that real memory location, with information about the process that owns that page</li>
</ul></li>
<li>Decreases memory needed to store each page table, but increases time needed to search the table when a page reference occurs</li>
</ul>
<h3 id="summary">Summary</h3>
<table style="width:43%;">
<colgroup>
<col width="8%" />
<col width="15%" />
<col width="19%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Segmentation</td>
<td>Fast context switching: Segment mapping maintained by CPU</td>
<td>External fragmentation</td>
</tr>
<tr class="even">
<td>Paging (single level)</td>
<td>No external fragmentation, fast easy allocation</td>
<td>Large table size ~ virtual memory, internal fragmentation</td>
</tr>
<tr class="odd">
<td>Paged segmentation, two-level paging</td>
<td>Table size ~ # of pages in virtual memory, fast easy allocation</td>
<td>Multiple memory references per page access</td>
</tr>
<tr class="even">
<td>Inverted table</td>
<td>Table ~ # of pages in physical memory</td>
<td>Hash function more complex</td>
</tr>
</tbody>
</table>
<ul>
<li>Page tables
<ul>
<li>Memory divided into fixed-size chunks of memory</li>
<li>Virtual page number from virutal address mapped through page table to physical page number</li>
<li>Offset of virtual address is same as physical address</li>
<li>Large page tables can be placed into virtual memory</li>
</ul></li>
<li>Useage of PTE: page sharing, copy on write, page on demand, zero fill on demand</li>
<li>Multiple page tables
<ul>
<li>Virtual address mapped to series of tables</li>
<li>Permit sparse population of address space</li>
</ul></li>
<li>Inverted page table: Size of page table related to physical memory size</li>
</ul>
<h2 id="nachos">Nachos</h2>
<h3 id="nachos-thread-operations">Nachos thread operations</h3>
<ul>
<li><code>Thread()</code> creates a thread</li>
<li><code>Fork(VFP func, int arg)</code> lets a thread execute a function</li>
<li><code>Yield()</code> suspends the calling thread and select a new one for execution</li>
<li><code>Sleep()</code> suspends the current thread, changes its state to BLOCKED, and removes it from the ready list</li>
<li><code>Finish()</code></li>
</ul>
<h3 id="code-execution-in-nachos">Code execution in Nachos</h3>
<ul>
<li>User mode executes instructions which only access the user space.</li>
<li>Kernel mode executes when Nachos first starts up or when an instruction causes a trap, e.g., illegal instruction, page fault, or system call, etc.</li>
<li>Load instructions into the machine's memory</li>
<li>Initializes registers (PC, etc.)</li>
<li>Tell the machine to start executing instructions.</li>
<li>The machine fetches the instruction, decodes it, and executes it.</li>
<li>Repeats until all instructions are executed.</li>
<li>Handle interrupt/page fault if necessary.</li>
</ul>
<h3 id="scheduler-object">Scheduler object</h3>
<ul>
<li>Decide which thread to run next.</li>
<li>Invoked when the current thread gives up CPU.</li>
<li>The current Nachos shceduling polycy is round-robin: slecting the front of ready queue list, appending new threads to the end</li>
</ul>
<h3 id="machine-object-implements-a-mips-machine">Machine object: implements a MIPS machine</h3>
<ul>
<li>An instance created when Nachos starts up.</li>
<li>Supported public variables: 40 registers, 4KB memory (32 pages), single linear page table virtual memory.</li>
</ul>
<h3 id="interrupt-object-maintains-an-event-queue-with-simulated-clock">Interrupt object: maintains an event queue with simulated clock</h3>
<ul>
<li><code>SetLevel(IntStatus level)</code> is used to temporarily disable and re-enable interrupts.</li>
<li><code>OneTick()</code> advances 1 clock tick.</li>
<li><code>CheckIfDue(bool advanceClock)</code> examines if some event should be serviced.</li>
<li><code>Idle()</code> advances the clock to the time of the next scheduled event.</li>
</ul>
<h3 id="timer-object">Timer object</h3>
<ul>
<li>Generates interrupts at regular or random intervals, then Nachos invokes the predefined clock event handling procedure.</li>
</ul>
<h3 id="noff-binary-code-format">Noff: binary code format</h3>
<ul>
<li>A Noff-format file contains
<ul>
<li>The Noff header, describing the contents of the rest of the file</li>
<li>TEXT: Executable code segment.</li>
<li>DATA: Initialized data segment.</li>
<li>BSS: Uninitialized data segment: stagically-allocated variables</li>
</ul></li>
<li>Each segment has the following information
<ul>
<li>virtualAddr: virtual address that segment begins at</li>
<li>inFileAddr: pointer within the Noff file where that scetion actually begins</li>
<li>size (in bytes) of that segment</li>
</ul></li>
</ul>
<h3 id="nachos-thread-user-process-for-executing-a-program">Nachos thread: user process for executing a program</h3>
<ul>
<li>A Nachos thread is extended as a process</li>
<li>Each process has its own address space containing executable code (TEXT), initialized data (DATA), uninitialized data (BSS), and stack space for fucntion call/local variables</li>
<li>Each addrspace is as big as <code>code.size + data.size + bss.size + userStackSize</code></li>
<li>A process owns some other objects, such as open file descriptors</li>
<li>Steps in user process creation
<ul>
<li>Create an address space</li>
<li>Zero out all of physical memory</li>
<li>Read the binary into physical memory and initialize data segment</li>
<li>Initialize the translation tables to do a one-to-one mapping between virtual and physical address</li>
<li>Zero all registers, setting PCReg, NextPCReg to 0 and 4 respectively</li>
<li>Set the stackpointer to the largest virtual address of the process</li>
</ul></li>
<li>Actions of <code>nachos -x halt</code>
<ul>
<li>The main thread starts by running function <code>StartProcess()</code> in <code>progtest.cc</code>. This thread is used to run halt binary.</li>
<li><code>StartProcess()</code> allocates a new address space and loads the halt binary. It also initializes registers and sets up the page table.</li>
<li><code>Machine::Run()</code> executes the halt binary using the MIPS emulator: The halt binary invokes the system call <code>Halt()</code>, which causes a trap back to the Nachos kernel via functions <code>RaiseException()</code> and <code>ExceptionHandler()</code>.</li>
<li>The execution handler determines that a <code>Halt()</code> system call was requested from user mode, and it halts Nachos.</li>
</ul></li>
</ul>
<h3 id="thread-execution-flow-when-running-nachos-under-thread-directory">Thread execution flow when running Nachos under thread directory</h3>
<p>When the <em>threads</em> version of NACHOS is started, <code>main()</code> in file <code>main.cc</code> under threads directory performs the following key operations.</p>
<ol style="list-style-type: decimal">
<li>Call <code>Initialize()</code> in file <code>system.cc</code> under threads directory to initialize Nachos global data structure. For example, set random yield or not based on parameters, and create an interrupt object and a scheduler object etc.</li>
<li>Call <code>ThreadTest()</code> in the file <code>threadtest.cc</code> under threads directory. This creates two Nachos threads to execute. This calls two function methods <code>Thread::Fork()</code> and <code>Thread::Yield()</code>.</li>
<li>Call <code>currentThread-&gt;Finish()</code> in file <code>thread.cc</code> which calls <code>Thread:Sleep()</code>, similar to <code>Thread:Yield()</code>. If the procedure &quot;main&quot; returns, then the program &quot;nachos&quot; will exit (as any other normal program would). But there may be other threads on the ready list. We switch to those threads by saying that the &quot;main&quot; thread is finished, preventing it from returning.</li>
</ol>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp">DebugInit(debugArgs);                       <span class="co">// initialize DEBUG messages</span>
stats = <span class="kw">new</span> Statistics();                   <span class="co">// collect statistics</span>
interrupt = <span class="kw">new</span> Interrupt;                  <span class="co">// start up interrupt handling</span>
scheduler = <span class="kw">new</span> Scheduler();                <span class="co">// initialize the ready queue</span>

currentThread = <span class="kw">new</span> Thread(<span class="st">&quot;main&quot;</span>);
currentThread-&gt;setStatus(RUNNING);

interrupt-&gt;Enable();
CallOnUserAbort(Cleanup);                   <span class="co">// if user hits ctl-C</span></code></pre></div>
<p><code>Thread::Fork()</code> is defined in <code>thread.cc</code> under threads directory. It calls <code>StackAllocate(func, arg)</code> defined in <code>thread.cc</code> under threads directory. This function allocates stack space and initialize the thread control block and the register state information. Notice that &quot;func&quot; address is assigned to <code>InitialPCState</code> slot of the thread control block. This address will be used as the entrance point when this thread starts to execute, which will be discussed in two assembly functions <code>SWITCH()</code> and <code>ThreadRoot()</code> below.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="co">//StackSize is 4K defined in thread.h</span>
stack = (<span class="dt">int</span> *) AllocBoundedArray(StackSize * <span class="kw">sizeof</span>(<span class="dt">int</span>));
stackTop = stack + StackSize - <span class="dv">4</span>;   <span class="co">// -4 to be on the safe side!</span>
machineState[PCState] = (<span class="dt">int</span>) ThreadRoot;
machineState[StartupPCState] = (<span class="dt">int</span>) InterruptEnable;
machineState[InitialPCState] = (<span class="dt">int</span>) func;
machineState[InitialArgState] = arg;
machineState[WhenDonePCState] = (<span class="dt">int</span>) ThreadFinish;</code></pre></div>
<p><code>ThreadTest()</code> in the file <code>threadtest.cc</code> calls <code>currentThread-&gt;Yield()</code> to yield CPU to another ready thread and Nachos schedules another thread for execution. <code>Thread::Yield()</code> is defined in <code>thread.cc</code> under threads directory.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp">nextThread = scheduler-&gt;FindNextToRun();
<span class="cf">if</span> (nextThread) { scheduler-&gt;ReadyToRun(<span class="kw">this</span>); scheduler-&gt;Run(nextThread); }</code></pre></div>
<ol style="list-style-type: decimal">
<li><code>FindNextToRun()</code> method defined in <code>scheduler.cc</code> under threads directory finds next thread in the ready queue to run. That essentially removes the first one in the ready queue.</li>
<li><code>ReadyToRun()</code> in <code>scheduler.cc</code> under threads directory marks the selected thread to be ready and append it to the end of ready list.</li>
<li><code>Run()</code> in <code>scheduler.cc</code> under threads directory executes the selected thread. It does context switch from old thread to new thread by calling assembly code <code>SWITCH(oldThread, nextThread)</code> in <code>switch.s</code> under threads directory. <code>SWITCH()</code> shifts execution from the old thread to new thread. It saves register state for the old thread and loads the new data from the new thread's control block. For example, if the host hardware is MIPS (actually not as we run on the Intel chip. I use MIPS because its instructions are easy to read.)</li>
</ol>
<div class="sourceCode"><pre class="sourceCode mips"><code class="sourceCode mips">    <span class="co"># a0 -- pointer to old Thread</span>
    <span class="co"># a1 -- pointer to new Thread</span>

<span class="ot">SWITCH:</span>
    <span class="kw">sw</span>  sp, SP(a0)      <span class="co"># save new stack pointer</span>
    <span class="kw">sw</span>  s0, S0(a0)      <span class="co"># save all the callee-save registers</span>
    <span class="kw">sw</span>  s1, S1(a0)
    <span class="kw">sw</span>  s2, S2(a0)
    <span class="kw">sw</span>  s3, S3(a0)
    <span class="kw">sw</span>  s4, S4(a0)
    <span class="kw">sw</span>  s5, S5(a0)
    <span class="kw">sw</span>  s6, S6(a0)
    <span class="kw">sw</span>  s7, S7(a0)
    <span class="kw">sw</span>  fp, FP(a0)      <span class="co"># save frame pointer</span>
    <span class="kw">sw</span>  ra, PC(a0)      <span class="co"># save return address</span>

    <span class="kw">lw</span>  sp, SP(a1)      <span class="co"># load the new stack pointer</span>
    <span class="kw">lw</span>  s0, S0(a1)      <span class="co"># load the callee-save registers</span>
    <span class="kw">lw</span>  s1, S1(a1)
    <span class="kw">lw</span>  s2, S2(a1)
    <span class="kw">lw</span>  s3, S3(a1)
    <span class="kw">lw</span>  s4, S4(a1)
    <span class="kw">lw</span>  s5, S5(a1)
    <span class="kw">lw</span>  s6, S6(a1)
    <span class="kw">lw</span>  s7, S7(a1)
    <span class="kw">lw</span>  fp, FP(a1)
    <span class="kw">lw</span>  ra, PC(a1)      <span class="co"># load the return address</span>

    <span class="kw">j</span>   ra
    .end SWITCH

<span class="ot">ThreadRoot:</span>
    <span class="kw">jal</span> StartupPC       <span class="co"># call startup procedure</span>
    <span class="fu">move</span> a0, InitialArg
    <span class="kw">jal</span> InitialPC       <span class="co"># call main procedure</span>
    <span class="kw">jal</span> WhenDonePC      <span class="co"># when were done, call clean up procedure</span></code></pre></div>
<p>Constants <code>SP</code>, <code>S0</code>, ..., <code>PC</code> etc are defined in <code>switch.h</code> under threads directory. For example, <code>SP=0</code> for MIPS host. Notice that <code>0</code> means the first position <code>stackTop</code> in the thread control block which is defined in <code>thread.h</code> What does <code>SWITCH()</code> finally call after saving and loading context? Namely what does the return address &quot;ra&quot; point to before executing &quot;j ra&quot;? Notice that <code>StackAllocate(func, arg)</code> has assigned</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp">machineState[PCState] = (<span class="dt">int</span>) ThreadRoot;   <span class="er">#</span> in MIPS, PCState = PC/<span class="dv">4-1</span> = <span class="dv">9</span>
machineState[InitialPCState] = (<span class="dt">int</span>) func;  <span class="er">#</span> in MIPS, InitialPCState = S0/<span class="dv">4-1</span> = <span class="dv">0</span></code></pre></div>
<p>Thus <code>SWITCH()</code> calls <code>j ra</code> which is <code>ThreadRoot</code>. Then <code>ThreadRoot()</code> will call &quot;func&quot; saved and loaded from <code>InitialPCState</code>.</p>
<h3 id="scheduler">Scheduler</h3>
<ul>
<li><code>ReadyToRun</code>. Mark a thread as ready, but not running. Put it on the ready list, for later scheduling onto the CPU.</li>
<li><code>FindNextToRun</code>. Return the next thread to be scheduled onto the CPU. If there are no ready threads, return NULL. Thread is removed from the ready list.</li>
<li><code>Run</code>. Dispatch the CPU to nextThread. Save the state of the old thread, and load the state of the new thread, by calling the machine dependent context switch routine, <code>SWITCH</code>. Note: we assume the state of the previously running thread has already been changed from running to blocked or ready (depending). The global variable currentThread becomes nextThread.</li>
</ul>
<h3 id="thread">Thread</h3>
<p>Data structures for managing threads. A thread represents sequential execution of code within a program. So the state of a thread includes the program counter, the processor registers, and the execution stack. Stack size is fixed.</p>
<ul>
<li><code>Thread</code>. Initialize a thread control block, so that we can then call <code>Fork</code>.</li>
<li><code>~Thread</code>. De-allocate a thread. NOTE: the current thread <em>cannot</em> delete itself directly, since it is still running on the stack that we need to delete. NOTE: if this is the main thread, we can't delete the stack because we didn't allocate it -- we got it automatically as part of starting up Nachos.</li>
<li><code>Fork</code>. Invoke <code>(*func)(arg)</code>, allowing caller and callee to execute concurrently. NOTE: although our definition allows only a single integer argument to be passed to the procedure, it is possible to pass multiple arguments by making them fields of a structure, and passing a pointer to the structure as &quot;arg&quot;. Implemented as the following steps: (1) Allocate a stack,</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Initialize the stack so that a call to <code>SWITCH</code> will cause it to run the procedure, (3) Put the thread on the ready queue.</li>
</ol>
<ul>
<li><code>Finish</code>. Called by <code>ThreadRoot</code> when a thread is done executing the forked procedure. NOTE: we don't immediately de-allocate the thread data structure or the execution stack, because we're still running in the thread and we're still on the stack! Instead, we set <code>threadToBeDestroyed</code>, so that <code>Scheduler::Run()</code> will call the destructor, once we're running in the context of a different thread. NOTE: we disable interrupts, so that we don't get a time slice between setting threadToBeDestroyed, and going to sleep.</li>
<li><code>Yield</code>. Relinquish the CPU if any other thread is ready to run. If so, put the thread on the end of the ready list, so that it will eventually be re-scheduled. NOTE: returns immediately if no other thread on the ready queue. Otherwise returns when the thread eventually works its way to the front of the ready list and gets re-scheduled. NOTE: we disable interrupts, so that looking at the thread on the front of the ready list, and switching to it, can be done atomically. On return, we re-set the interrupt level to its original state, in case we are called with interrupts disabled. Similar to <code>Thread::Sleep()</code>, but a little different.</li>
<li><code>Sleep</code>. Relinquish the CPU, because the current thread is blocked waiting on a synchronization variable (Semaphore, Lock, or Condition). Eventually, some thread will wake this thread up, and put it back on the ready queue, so that it can be re-scheduled. NOTE: if there are no threads on the ready queue, that means we have no thread to run. <code>Interrupt::Idle</code> is called to signify that we should idle the CPU until the next I/O interrupt occurs (the only thing that could cause a thread to become ready to run). NOTE: we assume interrupts are already disabled, because it is called from the synchronization routines which must disable interrupts for atomicity. We need interrupts off so that there can't be a time slice between pulling the first thread off the ready list, and switching to it.</li>
<li><code>StackAllocate</code>. Allocate and initialize an execution stack. The stack is initialized with an initial stack frame for <code>ThreadRoot</code>, which: (1) enables interrupts, (2) calls <code>(*func)(arg)</code> and (3) calls <code>Thread::Finish</code>.</li>
<li><code>SaveUserState</code>. Save the CPU state of a user program on a context switch.</li>
<li><code>RestoreUserState</code>. Restore the CPU state of a user program on a context switch.</li>
</ul>
<h2 id="mips">MIPS</h2>
<h3 id="calling-convention">Calling convention</h3>
<ul>
<li><code>$0</code>. Zero register.</li>
<li><code>$1</code> - <code>$at</code>. The <em>Assembler Temporary</em> used by the assembler in expanding pseudo-ops.</li>
<li><code>$2-$3</code> - <code>$v0-$v1</code>. Used for return values of a subroutine.</li>
<li><code>$4-$7</code> - <code>$a0-$a3</code>. Used for arguments of a subroutine.</li>
<li><code>$8-$15, $24, $25</code> - <code>$t0-t9</code>. Temporary registers.</li>
<li><code>$16-$23</code> - <code>$s0-$s7</code>. Saved registers.</li>
<li><code>$26-27</code> - <code>$k0-$k1</code>. Kernel reserved registers. Do not use.</li>
<li><code>$28</code> - <code>$gp</code>. Globals pointer used for addressing static global variables.</li>
<li><code>$29</code> - <code>$sp</code>. Stack pointer.</li>
<li><code>$30</code> - <code>$fp ($s8)</code>. Frame pointer.</li>
<li><code>$31</code> - <code>$ra</code>. Return address in a subroutine call.</li>
</ul>
<p>Caller and callee save</p>
<ul>
<li>Callee saves
<ul>
<li>a procedure clears out some registers for its own use</li>
<li>register values are preserved across procedure calls</li>
<li>MIPS calls these saved registers, and designates <code>$s0-$s8</code> for this useage</li>
<li>the called procedure saves register values in its AR, uses the registers for local variables, restores register values before it returns.</li>
</ul></li>
<li>Caller saves
<ul>
<li>the calling program saves the registers that it does not want a called procedure to overwrite</li>
<li>register values are NOT preserved across procedure calls</li>
<li>MIPS calls these temporary registers, and designates <code>$t0-$t9</code> for this useage</li>
<li>procedures use these registers for local variables, because the values do not need to be preserved outside the scope of the procedure.</li>
</ul></li>
</ul>
<h2 id="virtual-memory">Virtual memory</h2>
<h3 id="demand-paging">Demand paging</h3>
<ul>
<li>Virtual memory can be much larger than physical memory.
<ul>
<li>Combined memory of running processes much larger than physical memory.</li>
<li>More programs fit into memory, allowing more concurrency.</li>
</ul></li>
<li>Supports flexible placement of physical data. Data could be on disk or somewhere across network.</li>
<li>Variable location of data transparent to user program. Performance issue, not correctness issue.</li>
<li>Bring a page into memory only when it is needed.
<ul>
<li>Less I/O. Less memory. Faster response. More users supported.</li>
</ul></li>
<li>Valid bits in a PTE.
<ul>
<li>Valid means in-memory. Invalid means not-in-memory.</li>
<li>Initially valid bit is set to invalid to on all PTEs.</li>
<li>Not in memory causes page fault.</li>
</ul></li>
<li>Dirty bit means this page has been modified. It needs to be written back to disk.</li>
<li>Steps in handling a page fault.
<ul>
<li>Reference a page.</li>
<li>Page fault exception.</li>
<li>Page is on backing store.</li>
<li>Bring in missing page.</li>
<li>Reset page table.</li>
<li>Restart instruction.</li>
</ul></li>
<li>What does the OS do on a page fault?
<ul>
<li>Choose an old page to replace. If old page is modified (dirty is set), write contents back to disk. Chagne its PTE and any cached TLB to be invalid.</li>
<li>Get an empty physical page. Load new page into memory from disk. Update PTE, invalidate TLB for new entry.</li>
<li>Continue thread from original faulting location. Restart the instruction that caused the page fault.</li>
</ul></li>
<li>Performance of demand paging.
<ul>
<li>Page fault rate = <span class="math inline">\(p\)</span>. If <span class="math inline">\(p=0\)</span>, no page fault. If <span class="math inline">\(p=1\)</span>, every reference is a fault.</li>
<li>Effective access time (EAT) = (1-p) * memory access + p * page fault cost.</li>
<li>Page fault service cost is the sum of
<ul>
<li>Page fault overhead.</li>
<li>Swap page out.</li>
<li>Swap page in.</li>
<li>Restart overhead.</li>
</ul></li>
</ul></li>
<li>Example: demand paging performance.
<ul>
<li>Memory access time = 200 ns.</li>
<li>Average page fault service time = 8 ms.</li>
<li>EAT = (1-p) * 200 ns + p * 8 ms.</li>
<li>If one access out of 1000 causes a page fault, then EAT = 8.2 ms.</li>
<li>What if we want slowdown by less than 10%? p = 1/400,000.</li>
</ul></li>
<li>Factors lead to misses.
<ul>
<li>Compulsory misses: Pages that have never paged into memory before.
<ul>
<li>Prefetching: Loading them into memory before needed.</li>
<li>Need to predict future somehow.</li>
</ul></li>
<li>Capacity misses: Not enough memory. Must somehow increase size.
<ul>
<li>One option: Increase amount of DRAM.</li>
<li>Another option: If multiple processes in memory, adjust percentage of memory allocated to each one.</li>
</ul></li>
<li>Polycy misses: Caused when pages were in memory, but kicked out prematurely because of the replacement policy.
<ul>
<li>So we need a better replacement policy.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="page-replacement-policy">Page replacement policy</h3>
<ul>
<li>Find some page in memory, but not really in use, swap it out.
<ul>
<li>Performance: We want an algorithm which will result in minimum number of page faults.</li>
<li>Same page may be brought into memory several times.</li>
</ul></li>
<li>Basic page replaement.
<ul>
<li>Find the location of the desired page on disk.</li>
<li>Find a free frame: If there is a free frame, use it. If there is no free frame, use a page replacement.
<ul>
<li>It's algorithm to find a victim frame.</li>
</ul></li>
<li>Swap out: Use modify (dirty) bit to reduce overhead of page transfers. Only modified pages are written to disk.</li>
<li>Bring the desired page into the free frame. Update the page and frame tables.</li>
<li>The more physical frames you have, the less page faults there will be.
<ul>
<li>Not necessarily true for FIFO (Belady's anomaly). After adding memory. With FIFO, contents can be completely different. In contrast, with LRU or MIN, contents of memory with X pages are a subset of contents with X+1 pagse.</li>
</ul></li>
</ul></li>
<li>Page replacement policies
<ul>
<li>FIFO. Throws out oldest page.
<ul>
<li>Fair. Let every page live in memory for same amount of time.</li>
<li>Bad. Throws out heavily used pages instead of infrequently used pages.</li>
</ul></li>
<li>MIN (minimum) or OPT (optimum). Replace page that won't be used for the longest time.
<ul>
<li>Great but cannot really know future.</li>
<li>Makes good comparison case, however.</li>
</ul></li>
<li>Random. Pick random page for every replacement.
<ul>
<li>Typical solution for TLB's. Simple hardware.</li>
<li>Pretty unpredictable. Makes it hard to make realtime guarantees.</li>
</ul></li>
</ul></li>
<li>LRU (least recently used)
<ul>
<li>Replace page that hasn't been used for the longest time.</li>
<li>Programs have locality, so if something not used for a while, unlikely to be used in the near future.</li>
<li>Seems like LRU should be a good approximation to MIN.</li>
<li>Use a list to implement LRU. On each use, remove page from list and place at head. LRU page is at tail.</li>
<li>Need to know immediately when each page used so that can change position in list.</li>
<li>Many instructions for each hardware access.</li>
<li>To implement LRU, use a key-value map or bitmap and double linked list. But it is too expensive to implement in reality for many reasons (6 pointer updates when accessing a page).</li>
<li>In practice, people approximate LRU.</li>
</ul></li>
<li>LRU approximation.
<ul>
<li>Clock algorithm: Arrange physical pages in circle with single clock hand. Replace an old page, may not the oldest page.
<ul>
<li>Advances only on page fault. Check for pages not used recently. Mark pages as not used recently.</li>
<li>What if hand moving slowly: Not many page faults and/or find page quickly.</li>
<li>What if hand is moving quickly: Lots of page sfaults and/or lots of reference bits set.</li>
<li>One way to view clock algorithm: Crude partitioning of pages into two groups: young and old. Why not partition into more than 2 groups?</li>
</ul></li>
<li>N-th chance version of clock algorithm.
<ul>
<li>Large N better approx to LRU.</li>
<li>Small N is more efficient.</li>
<li>Takes extra overhead to replace a dirty page, so give dirty pages an extra chance before replacing.</li>
</ul></li>
<li>Hardware 'use' bit per physical page: Hardware sets use bit on each reference. If use bit isn't set, means not referenced in a long time. Nachos hardware sets use bit in the TLB. You have to copy this back to page table when TLB entry gets replaced.</li>
<li>On page fault. Advance clocl hand. Check use bit. If 1 means used recently, clear and leave along. If 0 then select candidate for replacement.</li>
<li>Will always find a page or loop forever? Even if all use bits set, will eventually loop around (FIFO).</li>
</ul></li>
</ul>
<h3 id="swap-storage">SWAP storage</h3>
<ul>
<li>Swap-space: Virtual memory uses disk space as an extension of main memory.</li>
<li>Swap-space can carved out of the normal file system, or, more commonly, it can be in a separate disk partition.</li>
<li>Allocate swap space when process starts; holds text segment (the program) and data segment. Kernel uses swap maps to track swap-space use.</li>
<li>Issues
<ul>
<li>Initial allocation. Each process needs minimum number of pages.
<ul>
<li>Fixed allocation vs. priority allocation.</li>
</ul></li>
<li>Where to find frames.</li>
<li>Global replacement: Find a frame from all processes.</li>
<li>Local replacement: Find only from its own allocated frames.</li>
</ul></li>
<li>Fixed allocation
<ul>
<li>0 allocation.</li>
<li>Equal allocation: For example, if there are 100 frames and 5 processes, give each process 20 frames.</li>
<li>Proportional allocation: Allocate according to the size of process.
<ul>
<li>m=64, s1=10, s2=127, a1=10/137x64, a2=127/137x64</li>
</ul></li>
</ul></li>
<li>Priority allocation
<ul>
<li>Use a proportional allocation scheme using priorities rather than size.</li>
<li>If process Pi generates a page fault
<ul>
<li>select for replacement one of its frames;</li>
<li>select for replacement a frame from a process with lower priority number.</li>
</ul></li>
</ul></li>
<li>Thrashing: If a process doesn't not have enough pages, the page-fault is very high. This leads to: Low CPU utilization, OS thinks that it needs to increase the degree of multiprogramming, another process added to the system.
<ul>
<li>Thrashing means a process is busy swapping pages in and out.</li>
</ul></li>
<li>Working set.
<ul>
<li>The set of memory locations that a program has referenced in the recent past.</li>
<li>Represents data access pattern of program in a time period (locality).</li>
<li>As a program executes, it transitions through a sequence of working sets consisting of varying sized subsets of the address space.</li>
<li>Great performance if working-sets of all active processes fits into memory.</li>
<li>Why does demand paging work? Process migrates from one locality to another. Localities may overlap. Need sufficient memory so that working set fits in memory.</li>
<li>When working sets size is larger than total memory size, thrashing occurs.</li>
</ul></li>
<li>Tradeoffs of page size on performance. Impact of page size selection.
<ul>
<li>TLB hit rate: increase.</li>
<li>Internal fragmentation: decrease.</li>
<li>Total page table size: decrease.</li>
<li>I/O overhead (useless I/O). Depend on data access pattern of a program.</li>
</ul></li>
</ul>
<h3 id="other-related-techniques">Other related techniques</h3>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="pp">#define num_ints 1000</span>
<span class="pp">#define filesize num_ints * sizeof(int)</span>

fd = open(filepath, O_RDWR | O_CREAT | O_TRUNC, <span class="bn">0600</span>);
result = lseek(fd, filesize - <span class="dv">1</span>, SEEK_SET);
result = write(fd, <span class="st">&quot;&quot;</span>, <span class="dv">1</span>);
map = mmap(<span class="dv">0</span>, filesize, PROT_READ | PROT_WRITE | MAP_SHARED, fd, <span class="dv">0</span>);
<span class="cf">for</span> (<span class="dt">int</span> i = <span class="dv">1</span>; i &lt;= numints; i++)
    map[i] = <span class="dv">2</span> * i;
munmap(map, filesize);
close(fd);</code></pre></div>
<ul>
<li>Memory mapped files
<ul>
<li>Allow file I/O to be treated as routine memory access by mapping a disk block to a page in memory.</li>
<li>Simplifies file access through direct memory access rather than <code>read()</code> and <code>write()</code> system calls.</li>
<li>File access is managed with demand paging.</li>
<li>Several processes may map the same file into memory as shared data.</li>
</ul></li>
<li>Review of caching concept
<ul>
<li>Cache: A repository for copies that can be accessed more quickly than the original. Make frequent case fast and infrequent case less dominant.</li>
<li>Caching underlies many of the techniques that are used today to make computers fast. Can cache: memory locations, address translations, pages, file blocks, file names, network routes, etc.</li>
<li>Only good if: Frequent case frequent enough and infrequent case not too expensive.</li>
<li>Average access time = (hit rate) x (hit time) + (miss rate) x (miss time).</li>
<li>Temporal locality: keep recently accessed data items closer to processor.</li>
<li>Spatial locality: data access is contiguous following its layout.</li>
</ul></li>
<li>Zipf distribution
<ul>
<li>Caching behavior of many systems are not well characterized by the working set model.</li>
<li>An alternative is the Zipf distribution: Popularity ~ 1/k^c , for k-th most popular item.</li>
<li>Caching popular items can yield a very high cache hit ratio. LRU is a good replacement policy that assumes recent accessed items may be accessed again.</li>
<li>Cache frequently accessed data in OS.
<ul>
<li>TLB cache. VM. File cache for disk sectors.</li>
</ul></li>
<li>Cache popular web pages in web servers or in ISP:
<ul>
<li>Popular URLs are accessed again and again.</li>
<li>Akamai.com caches popular content in all ISP sites.</li>
</ul></li>
<li>Cache popular queries in Google.com.
<ul>
<li>Popular queries are entered by many users.</li>
<li>Caching results greatly improve search response time.</li>
</ul></li>
<li>Cache popular nodes in a big social graph.
<ul>
<li>Number of followers in Twitter.</li>
</ul></li>
<li>Cache information of popular items sold on Amazon.com.
<ul>
<li>Popular items have more chances to be browsed/purchased.</li>
</ul></li>
</ul></li>
</ul>
<h2 id="cpu-scheduling">CPU Scheduling</h2>
<ul>
<li>Life cycle (states) of a process or thread. Active processes/threads transit from Ready queue to Running to various waiting queues.</li>
<li>Question: How is the OS to select from each queue. Obvious queue to worry about is ready queue. Others can be scheduled as well, however.</li>
<li>Scheduling: Deciding whihc processes/threads are given access to resources.
<ul>
<li>Job scheduling, resource scheduling, request scheduling.</li>
<li>For example, web server scheduling to handle high traffic.</li>
</ul></li>
<li>Process execution consists of a cycle of CPU execution and I/O wait. Scheduling happens among CPU/IO bursts.</li>
<li>Process state change vs. CPU scheduling.
<ul>
<li>CPU scheduling decisions may take place when a process:
<ul>
<li>Switches from running to waiting (blocked) state.</li>
<li>Switches from running to ready state.</li>
<li>Switches from waiting/blocked to ready.</li>
<li>Terminates.</li>
</ul></li>
<li>Preemptive scheduling takes the processor away from one process (job) and give it to another. State change from running to ready.</li>
<li>Non-preemptive scheduling. A process runs continuously until it is blocked or terminates.</li>
</ul></li>
<li>Terminalogy
<ul>
<li>Task/Job. Process, thread. User request. E.g., mouse click, web request, shell command, etc.</li>
<li>CPU utilization. Keep the CPU as busy as possible.</li>
<li><strong>Throughput</strong>. Number of processes that complete their execution per time unit.</li>
<li><strong>Waiting time</strong>. Amount of time a process has been waiting in the ready queue.</li>
<li><strong>Turnaround time</strong>. Amount of time to execute a particular process.
<ul>
<li>Completeion time - arrival time = waiting time + job size + overhead.</li>
</ul></li>
<li><strong>Response time</strong>. Amount of time it takes from when a request was submitted until the first response is produced, not output (for time-sharing environment).</li>
</ul></li>
</ul>
<h3 id="scheduling-algorithm">Scheduling algorithm</h3>
<ul>
<li>Scheduling algorithm optimization criteria.
<ul>
<li>Max CPU utilization. Max throughput. Min turnaround time. Min waiting time. Min response time.</li>
<li>Four preemptive or non-preemptive algorithms are considered: FIFO, SJF, priority scheduling, round robin.</li>
</ul></li>
<li><strong>FIFO</strong>: First-in-first-out, aka first-come-first-served (FCFS)</li>
<li><strong>SJF</strong>: Always do the task that has the shortest remaining amount of work to do. Often called shortest remaining time first (SRTF).
<ul>
<li>SJF is optimal. Gives minimum average waiting time for a given set of processes. The difficulty is knowing the length of the next CPU request.</li>
<li>Weakness: Starvation. Low priority processes may never execute.</li>
</ul></li>
<li>Predicting length of next CPU burst.
<ul>
<li>Why? SJF scheduling requires size information predict by using the length of previous CPU bursts. Using expontial averaging. New prediction is weighted average of previous prediction and actual time.</li>
<li><span class="math inline">\(t_{n+1} = \alpha t_n + (1-\alpha) t_n\)</span>, where
<ul>
<li><span class="math inline">\(t_n\)</span> is the acutal length of n-th CPU burst.</li>
<li><span class="math inline">\(t_{n+1}\)</span> is the predicted value for the next CPU burst.</li>
<li><span class="math inline">\(0 \leq \alpha \leq 1\)</span>.</li>
</ul></li>
</ul></li>
<li><strong>Priority scheduling</strong>.
<ul>
<li>A priority number (integer) is associated with each process.</li>
<li>The CPU is allocated to the process with the highest priority (smaller ingeger means higher priority).</li>
<li>SJF is a priority scheduling where priority is the predicted next CPU burst time.</li>
<li>Problem is starvation. Low priority processes many never execute.</li>
<li>Solution: Aging (as time progresses, decreasing the priority of the long-running process). Round robin.</li>
</ul></li>
<li><strong>Round robin (RR) scheduling</strong>
<ul>
<li>Each process gets a small unit of CPU time (time quantum), usually 10-100 milliseconds.</li>
<li>After this time has elapsed, the process is preempted and added to the end of the ready queue.</li>
<li>Performance. Large quantum = FIFO. Small quantum = context switch overhead is too high.</li>
<li>Typically, higher average turnaround time than SJF, but better response. Also, no starvation.</li>
<li>Compared FIFO: Everybody finishes very late with overhead. Longer turnaround. But faster response. Relatively fair.</li>
</ul></li>
<li><strong>Overhead during job switches</strong>
<ul>
<li>Process context switch: CPU switch from one process to another process.</li>
<li>Code executed in kernel above is overhead. Overhead sets minimum practical switching time. Less overhead with SMT/hyperthreading, but contention for resources instead.</li>
<li>Context switchin in Linux: 3-4 s (Intel i7, E5).</li>
<li>Thread switching only slightly faster than process switching (100 ns).</li>
<li>But switching across cores about 2x more expensive than within-core switching.</li>
<li>Context switching time increases sharply with the size of the working set, and can increase 100x or more.</li>
<li>The working set is the subset of memory used by the process in a time window.</li>
<li>Moral: Context switching depends mostly on cache limits and process or thread's hunger for memory.</li>
</ul></li>
<li><strong>Multi-level feedback queue (MFQ)</strong>
<ul>
<li>Goals: Responsiveness, especially for interactive/high priority jobs. Less overhead. Fairness (among equal priority tasks). No starvation.</li>
<li>Not perfect at any of them: FIFO, SJF, RR. MFQ addresses this: used in Linux and Windows.</li>
<li>Maintain multiple job queues. Each queue receives a fixed percentage of system resource. A process can move between the various queues, representing priority aging so that high priority jobs will gradually lose their priority.</li>
</ul></li>
<li>MFQ Example
<ul>
<li>Ready queue is partitioned into separate queues: foreground (interactivge) and background (batch).</li>
<li>Each queue has its own scheduling algorithm: foreground (RR) and backgorund (FIFO).</li>
<li>Each queue gets a certain amount of CPU time which it can schedule amongst its processes, i.e., 80% to foreground in RR, 20% to background in FIFO.</li>
<li>Q0: RR with time quantum 8 milliseconds, Q1: RR time quantum 16 milliseconds, Q2: FIFO.</li>
<li>Scheduling. A new job enters Q0 which is served FCFS. When it gains CPU, job receives 8 ms. If it does not finish in 8 ms, job is moved to Q1. At Q1 job is again served FCFS and receives 16 ms. If it still doesn't complete, it is preempted and moved to Q2.</li>
</ul></li>
</ul>
<h3 id="scheduling-examples">Scheduling examples</h3>
<ul>
<li>Windows scheduling
<ul>
<li>Real time priority class: static priorities (priorities do not change). Priority values from 16 to 32.</li>
<li>Variable class: variable priorities (e.g., 1-16). If a process has used up its quantum, lower its priority. If a process waits for an I/O event, raise its priority.</li>
<li>Priority-driven scheduler. For real-time class, do RR within each priority. For variable class, do MFQ.</li>
</ul></li>
<li>Linux scheduling
<ul>
<li>Time-sharing scheduling: Each process has a priority and # of credits.
<ul>
<li>Every clock tick the running process lost a credit. Long running jobs lose credits.</li>
<li>When it reached 0, another process with most credit won was chosen.</li>
<li>The crediting rule: Credits = credits / 2 + priority.</li>
<li>I/O event will raise the priority: fast response time when ready.</li>
</ul></li>
<li>Real-time scheduling. Soft real-time. Kernel cannot be preempted by user code.</li>
</ul></li>
<li>Thread scheduling
<ul>
<li>Scheduling user-level threads within a process. Known as process-contention scope (PCS).</li>
<li>Kernel thread scheduled onto available CPU is system-contention scope (SCS)  competition among all threads in system</li>
</ul></li>
<li>Pthread scheduling
<ul>
<li>API allows specifying either PCS or SCS during thread creation.</li>
<li><code>PTHREAD_SCOPE_PROCESS</code> schedules threads using PCS scheduling.</li>
<li><code>PTHREAD_SCOPE_SYSTEM</code> schedules threads using SCS scheduling.</li>
</ul></li>
<li>Multiple-processor scheduling
<ul>
<li>CPU scheduling more complex when multiple CPUs are available.
<ul>
<li>Each processor is self-scheduling. Each has its own private queue of ready processes.</li>
<li>Or all processes in common ready queue.</li>
</ul></li>
<li>Process has affinity for processor on which it is currently running.</li>
</ul></li>
</ul>
<h3 id="summary-1">Summary</h3>
<ul>
<li><strong>Scheduling</strong>. Selecting a process from the ready queue and allocating the CPU to it.</li>
<li><strong>FIFO (FCFS)</strong>
<ul>
<li>Pros: Simple.</li>
<li>Cons: Short jobs get stuck behind long ones.</li>
</ul></li>
<li><strong>RR</strong>
<ul>
<li>Pros: Better for short jobs. Relatively fair.</li>
<li>Cons: Weak when jobs are smae legnth. No prioritization. Longer average turnaround. More context switch overhead.</li>
</ul></li>
<li><strong>SJF</strong>
<ul>
<li>Pros: Optimal (averagfe response time).</li>
<li>Cons: Hard to predict future, unfair.</li>
</ul></li>
<li><strong>MFQ</strong>
<ul>
<li>Fairness while having different priorities. Everybody makes progress.</li>
<li>Automatic promotion/demotion of process priority in order to approximate SJF/SRTF.</li>
<li>Responsiveness, especially for interactive/high priority jobs.</li>
<li>Less context switch overhead with different quantum.</li>
</ul></li>
</ul>
<h2 id="file-systems">File Systems</h2>
<h3 id="files">Files</h3>
<ul>
<li><strong>Files</strong>: Contiguous logical address space in a persistent storage (e.g., disk).</li>
<li><strong>File structure</strong>: The OS and program decides the structure.
<ul>
<li>None: sequence of words and bytes.</li>
<li>Simple record structure: lines, fixed length, variable length.</li>
<li>Complex structure: formatted document.</li>
</ul></li>
<li><strong>Attributes</strong>: Name, identifier, type, location ,size, protection, time, date, user identification, etc.</li>
<li><strong>Operations</strong>: Create, open, close, write, read, reposition within file, delete, truncate.</li>
<li><strong>Access methods</strong>: Sequential access, direct access.</li>
<li><strong>File system abstraction</strong>
<ul>
<li><strong>Directory</strong>: group of named files or subdirectories. Mapping from file name to file metadata location.</li>
<li><strong>Path</strong>: String that uniquely identifies file or directory.</li>
<li><strong>Links</strong>: Hard links from name to metadata location; soft links from name to alternative name.</li>
<li><strong>Mount</strong>: Mapping from name in one file system to root of another.</li>
</ul></li>
</ul>
<h3 id="unix-file-system-interface">UNIX file system interface</h3>
<ul>
<li><strong>UNIX file system API</strong>
<ul>
<li>create, link, unlink, createdir, rmdir.</li>
<li>open, close, read, write, seek.</li>
<li>fsync: File modifications can be cached, fsync forces modifications to disk (like a memory barrier).</li>
</ul></li>
<li><strong>Protection</strong>: File owner/creator should be able to control what can be done by whom.</li>
<li><strong>Access lists and groups in Linux</strong>
<ul>
<li><strong>Mode of access</strong>: Read, write execute.</li>
<li><strong>Three classes of uses</strong>: Owner, group, public.
<ul>
<li>Ask manager to create a group (unique name), say <code>G</code>, and add some users to the group. For a particular file (say game) or subdirectory, define an appropriate access.</li>
</ul></li>
</ul></li>
<li><strong>Directory structure</strong>: A collection of nodes containing information about all files. Operations performed include search, create, delete, list, rename, traverse.
<ul>
<li><strong>Name resolution</strong>: The process of converting a logical name into a physical resource (like a file): (1) Traverse succession of directories until reach target file and (2)Global file system may be spread across the network.</li>
</ul></li>
</ul>
<div class="sourceCode"><pre class="sourceCode c++"><code class="sourceCode cpp"><span class="co">// file content: This is a test file</span>
<span class="dt">int</span> main() {
    <span class="dt">int</span> file = <span class="dv">0</span>; <span class="dt">char</span> buffer[<span class="dv">15</span>];
    <span class="cf">if</span> ((file = open(<span class="st">&quot;testfile.txt&quot;</span>, O_RDONLY)) &lt; <span class="dv">-1</span>) <span class="cf">return</span> <span class="dv">1</span>;
    <span class="cf">if</span> (read(file, buffer, <span class="dv">14</span>) != <span class="dv">14</span>) <span class="cf">return</span> <span class="dv">1</span>;
    printf(<span class="st">&quot;</span><span class="sc">%s\n</span><span class="st">&quot;</span>, buffer); <span class="co">// This is a test</span>
    <span class="cf">if</span> (lseek(file, <span class="dv">5</span>, SEEK_SET) &lt; <span class="dv">0</span>) <span class="cf">return</span> <span class="dv">1</span>;
    <span class="cf">if</span> (read(file, buffer, <span class="dv">19</span>) != <span class="dv">14</span>) <span class="cf">return</span> <span class="dv">1</span>;
    printf(<span class="st">&quot;</span><span class="sc">%s\n</span><span class="st">&quot;</span>, buffer); <span class="co">// is a test file</span>
}</code></pre></div>
<h3 id="file-system-structure">File system structure</h3>
<ul>
<li><strong>File system</strong> is a layer of OS that transforms block interface of disks (or other block devices) into files, directories, etc.</li>
<li><strong>File system components</strong>
<ul>
<li><strong>Disk management</strong>. Collectign disk blocks into files.</li>
<li><strong>Naming</strong>. Interface to find files by name, not by blocks.</li>
<li><strong>Protection</strong>. Layers to keep data secure.</li>
<li><strong>Reliability/Durability</strong>. Keeping of files durable despite crashes, media failure, attacks, and etc.</li>
</ul></li>
<li><strong>User vs. system POV</strong>
<ul>
<li>User's view: Durable data structures.</li>
<li>System call interface: Collection of bytes (UNIX).</li>
<li>System's view (inside OS): Collection of blocks.
<ul>
<li>A block is a logical transfer unit, while a sector is the physical transfer unit on disk).</li>
<li>Block size &gt;= sector size; in UNIX, block size is 4KB.</li>
</ul></li>
</ul></li>
<li><strong>Translating from user to system view</strong>
<ul>
<li>What happens if users says: Give me bytes 2 to 12? (1) Fetch block corresponding to those bytes. (2) Return just the correct portion of the block.</li>
<li>What about: Write bytes 2 to 12? (1) Fetch block. (2) Modify portion. (3) Write out block.</li>
<li>Everything inside file system is in whole size blocks. E.g., <code>getc()</code> and <code>putc()</code> buffer something like 4KB, even if interface is one byte at a time.</li>
<li>From now on, a file is a collection of blocks.</li>
</ul></li>
</ul>
<h3 id="file-system-design">File system design</h3>
<ul>
<li><strong>Data structures</strong>
<ul>
<li>Directores: File name to file metadata. Store dirs as files.</li>
<li>File metadata: How to find file data blocks.</li>
<li>Free map: List of free disk blocks.</li>
</ul></li>
<li>How do we organize these data structures? Device has non-uniform performance.</li>
<li>Design challenges: Index structure, index granularity, free space, locality, reliability.</li>
<li><strong>File system workload</strong>. Studying workload characteristics can help feature prioritization or optimization of design. What should be considered? <strong>File sizes</strong>, and <strong>file access patterns</strong>.
<ul>
<li><strong>Sequential access</strong>: Bytes read in order. Mose of file accesses are of this flavor.</li>
<li><strong>Random access</strong>: Read/write element out of middle of array. Less frequent, but still important. Want this to be fast.</li>
<li><strong>Content-based access</strong>: Many systems don't provide this. Instead, build DBs on top of disk access to index content (requires efficient random access).</li>
</ul></li>
</ul>
<h3 id="file-system-implmentation">File system implmentation</h3>
<ul>
<li><strong>Directories and index structure</strong>. Special root block at a specific location contains the root directory. DIrector structure organizes the files:
<ul>
<li>Given file name, find a file number.</li>
<li>Given a file number which contains the file structure info, locate blocks of this file.</li>
</ul></li>
<li>Per-file <strong>file control block (FCB)</strong> contains many details about the file, called <strong>inode</strong> on *nix.
<ul>
<li>A typical FCB include file permissions, dates, owner, group, ACL, size, data blocks of pointers to file data blocks.</li>
</ul></li>
<li><strong>Layered file system</strong>: APplication programs, logical file system, file-organization module, basic file system, I/O control, and devices.
<ul>
<li>Virtual file systems (VFS) provide an OO way of implementing file systems.</li>
<li>VFS allows the same system call interface (the API) to be used for different types of file systems.</li>
<li>The API is to the VFS interface, rather than any specific type of file system.</li>
</ul></li>
</ul>
<h3 id="directory-implementation">Directory implementation</h3>
<ul>
<li>Naive approaches: Linear list, hash table, search tree.</li>
<li><strong>All information about a file contained in its file header</strong>. inodes are global resources identified by index (inumber). Once you load the header structure, all blocks of file are locatable.
<ul>
<li>The maximum number of inodes is fixed at file system creation, limiting the maximum number of files the file system can hold.</li>
<li>A typical allocation of heuristic for inodes in a file system is one percent of total size.</li>
<li>The inode number indexes a table of inodes in a known location on the device.</li>
</ul></li>
<li><strong>Directory layout</strong>. Directory are stored as a file. Linear search to find filename (for small directories). B-trees are used for large directores.</li>
<li><strong>Example: resolve /my/book/count</strong>
<ul>
<li>Read in file header for root <code>/</code> (fixed spot on disk).</li>
<li>Read in first data block for root <code>/</code>; search for <code>my</code>.
<ul>
<li>Table of filename/index pairs. Search is done in linear. OK since directories typicall very small.</li>
</ul></li>
<li>Read in file header for <code>my</code>.</li>
<li>Read in first data block for <code>my</code>; search for <code>book</code>.</li>
<li>Read in file header for <code>book</code>.</li>
<li>Read in first data block for <code>book</code>; search for <code>count</code>.</li>
<li>Read in file header for <code>count</code>.</li>
</ul></li>
<li><strong>Current working directory (CWD)</strong>. Per-address-space pointer to a directory (inode) used for resolving file names. Allows user to specify relative filename instead of absolute path (say <code>CWD=/my/book/</code> can resolve <code>count</code>).</li>
<li><strong>Open system call</strong>
<ul>
<li>Resolve file name, finds FCB (inode).</li>
<li>Makes entries in per-process and system-wide tables.</li>
<li>Return index (called file descriptor or file handle) in open-file table.</li>
</ul></li>
<li>Several pieces of data are needed to manage open files.
<ul>
<li>File pointer: Pointer to last read/write location, per process that has the file open.</li>
<li>File-open count: Counter of number of times a file is open, allowing removal of data from open-file table when last processes closes it.</li>
<li>Disk location of the file: Cache of data access information.</li>
<li>Access permissions: Per-process access mode information.</li>
<li>Open file locking is proceded by some systems to mediates access to a file.</li>
</ul></li>
<li><strong>Read/write system calls</strong>
<ul>
<li>Use file handle (descriptor) to locate inode.</li>
<li>Perform appropriate reads or writes.</li>
</ul></li>
</ul>
<h3 id="allocation-of-disk-blocks">Allocation of disk blocks</h3>
<ul>
<li>An allocation method refers to how disk are allocated for files:
<ul>
<li>Contiguous allocation.</li>
<li>Linked allocation.</li>
<li>Indexed allocation.</li>
</ul></li>
<li><strong>Contiguous allocation</strong>. Each file occupies a set of contiguous blocks on the disk.
<ul>
<li>Pros: Simple (only starting location (block#) and length (number of blocks) are required). Fast random access.</li>
<li>Cons: Not easy to grow files. Waste in space (e.g., external fragmentation).</li>
</ul></li>
<li><strong>Linked allocation</strong>. Each file is a linked list of disk blocks. Blocks may scattered anywhere on the disk.</li>
<li><strong>Microsoft file allocation table (FAT)</strong>
<ul>
<li>Linked list index structure: Simple, easy to implement. Still widely used.</li>
<li>File table: Linear map of all blocks on disk. Each file is a linked list of blocks.</li>
<li>Pros: Easy to find free block, to append to a file, to delete a file.</li>
<li>Cons: Small file access is slow. Random access is very slow. Fragmentation (File blocks for a given file may be scattered. Files in the same directory may be scattered. Problems becomes worse as disk fills).</li>
</ul></li>
<li><strong>One-level indexed allocation</strong>. Place all direct data pointers together into the index block. E.g., Nachos file control block has 32 data block pointers, 128 bytes per block.
<ul>
<li>Pros: Support random access. No external fragmentation.</li>
<li>Cons: Space overhead (need 1 block for index table).</li>
</ul></li>
<li><strong>Two-level indexed allocation: single indirection</strong>. Max size is 1k * 1k * 4KB = 4GB.</li>
<li><strong>Hybrid multi-level scheme (Unix file system)</strong>. Efficient for small files, but still allow big files. File header contains 13-15 pointers. File header (inode) format: 10-12 direct data pointer (4KB each), 1 indirect block (4MB each), 1 doubly indirect block (4GB each), 1 triple indirect block (4TB each).</li>
<li><strong>Free space management</strong>. Block number calculation = number of bits per word * number of 0-value words + offset of first 1 bit.</li>
<li><strong>Performation optimization</strong>
<ul>
<li>Disk cache: Separate section of main memory for frequently used blocks.</li>
<li>Read-ahead (prefectching): Techniques to optimize sequential access.</li>
<li>Improve PC performance by dedicating section of memory as virtual disk, or RAM disk.</li>
</ul></li>
</ul>
<h2 id="mass-storage-systems">Mass-Storage Systems</h2>
<ul>
<li><strong>Magnetic tape</strong>
<ul>
<li>Relatively permanent and holds large quantities of data.</li>
<li>Random access ~1000 times slower than disk.</li>
<li>Mainly used for backup, storage of infrequently-used data, transfer medium between systems.</li>
<li>Typical storage 1.5-20 TB.</li>
<li>Common technologies are 4mm, 8mm, 19mm, LTO-2 and SDLT.</li>
</ul></li>
<li><strong>Disk attachment</strong>
<ul>
<li>Drive attached to computer via I/O bus. USB, SATA (replacing ATA, PATA, EIDE).</li>
<li>SCSI: Itself is a bus, up to 16 devices on one cable, SCSI initiator requests operation and SCSI targets perform tasks.</li>
<li>FC (fiber channel) is high-speed serial architecture. Can be switched fabric with 24-bit address space. The basis of storage area networs (SANs) in which many hosts attach to many sotarge units. Can be arbitarted loop (FC-AL) of 126 devices.</li>
</ul></li>
<li><strong>Network-attached storage</strong>
<ul>
<li>Network-attached storage (NAS) is storage made available over a network rather than over a local connection (such as a bus).</li>
<li>NFS and CIFS are common protocols.</li>
<li>Implemented via remote procedure calls (RPCs).</li>
<li>New iSCSI protocol uses IP network to carry the SCSI protocol.</li>
</ul></li>
<li><strong>Storage area network</strong>
<ul>
<li>Special/dedicated network for accessing block level data storage.</li>
<li>Multiple hosts attached to multiple storage arrays (flexible).</li>
</ul></li>
</ul>
<h3 id="disk">Disk</h3>
<ul>
<li>Drives rotate at 60 to 200 times per second.</li>
<li>Positioning time is time to move disk arm to desired cylinder (seek time), plus time for desired sector to rotate under the disk head (rotational latency).</li>
<li>Effective bandwidth: Average data transfer rate during a transfer, that is, the number of bytes divided by transfer time. Data rate incldues positioning overhead.</li>
<li>Disk latency = seek time + rotation time + transfer time.
<ul>
<li>Seek time: Time to move disk arm over track (1-20ms). Fine-grained position adjustment necessary for head to &quot;settle&quot; head switch time ~ track switch time (on modern disks).</li>
<li>Rotation time: Time to wait for disk to rotate under disk head (4-15ms). On average, only need to wait half a rotation.</li>
<li>Transfer time: Time to transfer data onto/off of disk. Disk head transfer rate: 50-100MB/s (5-10 usec/sector). Host traansfer rate dependent on I/O connector (USB, SATA, ...).</li>
</ul></li>
<li>Example: 7200 RPM, 54 MB/s, seek time 10.5 ms. How long to complete 500 random disk reads in FIFO order? Each reads one sector (512 bytes).
<ul>
<li>Seek average 10.5 ms.</li>
<li>Rotation time = 1/120 (time per rotation) * 0.5 (average rotation) = 4.15 ms.</li>
<li>Transfer time: 54 MB/s to transfer 512 bytes per sector = 0.005 ms.</li>
<li>In total: 500 * (105 + 4.15 + 0.005) / 1000 = 7.3 seconds.</li>
<li>Effective bandwidth: 500 sectors * 512 bytes / 7.3 seconds = 0.034 MB/s. Copying 1 GB of data takes 8.37 hours.</li>
</ul></li>
<li>Same question. How long to complete 500 sequential disk reads?
<ul>
<li>Seek time: 10.5 ms (to reach the right track).</li>
<li>Rotation time: 4.15 ms (to reach first sector).</li>
<li>Transfer time for all 500 sectors = 500 * 512 / 128 = 2 ms.</li>
<li>Total: 10.5 + 4.15 + 2 = 16.7 ms.</li>
<li>Effective bandwidth: 500 * 512 / 16.7 = 14.97 MB/s. This is 11.7% of the maximum transfer rate with 250 KB data transferring.</li>
</ul></li>
</ul>
<h3 id="disk-scheduling">Disk scheduling</h3>
<ul>
<li><strong>Objective</strong>: Given a set of I/O requests. Coordinate disk access of multiple I/O requests for faster performance and reduced seek time.
<ul>
<li>Seek time ~ seek distance.</li>
<li>Measured by total head movement in terms of cylinders from one request to another.</li>
</ul></li>
<li><strong>FCFS scheduling</strong> (first come first serve).</li>
<li><strong>SSTF scheduling</strong> (shortest seek time first). Selects the request with the minimum seek time from the current head position.</li>
<li><strong>SCAN: elevator algorithm</strong>. Move disk arm in one direction until all requests satisfied, then reverse direction.</li>
<li><strong>C-SCAN</strong> (circular SCAN). Provides a more uniform wait time than SCAN by treating cylinders as a circular list. The head moves from one end of the disk to the other, servicing requests as it goes. When it reaches the other end, it immediately returns to the beginning of the disk, without servicing any requests on the return trip.</li>
</ul>
<h3 id="solid-state-disks-ssds">Solid state disks (SSDs)</h3>
<ul>
<li>Use NAND multi-level cell (2-bit/cell) flash memory. Non-volatile storage technology. Sector (4 KB page) addressable, but stores 4-64 &quot;pages&quot; per memory block. No moving parts (no rotate/seek motors). Very low power and lightweight</li>
<li>Transfer time: transfer 4KB page. Limited by controller and disk interface (SATA: 300-600 MB/s).</li>
<li>Latency = queuing time + controller time + transfering time.</li>
<li><strong>SSD architecture: Writes</strong>
<ul>
<li>Writing data is complex (~200s-1.7ms).</li>
<li>Can only write empty pages in a block.</li>
<li>Erasing a block takes ~1.5 ms.</li>
<li>Controller maintains pool of empty blocks by coalescing used pages (read, erase, write), also reverses some % of capacity.</li>
<li>Data written in 4KB pages, and erased in 256 KB blocks.</li>
<li>Controller garbage collects obsolete pages by copying valid pages to new (erased) block.</li>
<li>Typical steady state behavior when SSD is almost full. One erase every 64 or 128 writes.</li>
<li>Write and erase cycles require high voltage. Damages memory cells, limits SSD lifespan. Controller uses ECC, performs wear leveling. Wear leveling and garbage collection cause data to be rewritten on the SSD.</li>
<li>Result is very workload dependent performance.</li>
<li>Latency = queuing time + controller time (find free block) + transfering time.</li>
<li>Rule of thumb: Wrties 10x more expensive than reads, and writes 10x mroe expensive than writes.</li>
</ul></li>
<li>Pros (vs. HDD): Low latency, high throughput. No moving parts. Read at memory speeds.</li>
<li>Cons: Small storage, very expensive. Assymetric block write performance: read page erase/write page. Limited drive lifetime.</li>
</ul>
<h3 id="hybird-disk-drive">Hybird disk drive</h3>
<ul>
<li>A hybrid disk uses a small SSD as a buffer for a larger drive.</li>
<li>All dirty blocks can be flushed to the actual hard drive based on: Time, Threshold, Loss of power/computer shutdown.</li>
</ul>
<h2 id="reliable-storage">Reliable Storage</h2>
<ul>
<li><strong>Availability</strong>. The probability that the system can accept and process requests.
<ul>
<li>Often measured in &quot;nines&quot; of probability.</li>
<li>Key idea here is independence of failures.</li>
</ul></li>
<li><strong>Durability</strong>. The ability of a system to recover data despite faults.
<ul>
<li>This idea is fault tolerance applied to data.
<ul>
<li>Mean time before failure (MTBF). Inverse of annual failure rate.</li>
<li>Mean time to repair (MTTR) is a basic measure of the maintainability of repairable items. It represents the average time required to repair a failed component or device.</li>
</ul></li>
<li>Doesn't necessarily imply availability: information on pyramids was very durable, but could not be accessed until discovery of Rosetta Stone.</li>
<li>How to make file system durabile?
<ul>
<li>Disk blocks contain Reed-Solomon error correcting codes (ECC) to deal with small defects in disk drive. Can allow recovery of data from small media defects.</li>
<li>Make sure writes survive in short term. Either abandon delayed writes or use special, battery-backed RAM (called non-volatile RAM or NVRAM) for dirty blocks in buffer cache.</li>
<li>Make sure that data survives in long term. Need to replicate. Independence of failure.</li>
</ul></li>
</ul></li>
<li><strong>Reliability</strong>. The ability of a system or component to perform its required functions under stated conditions for a specified period of time (IEEE definition).
<ul>
<li>Usually stronger than simply availability: means that the system is not only &quot;up&quot;, but also working correctly.</li>
<li>Includes availability, security, fault tolerance/durability.</li>
<li>Must make sure data survives system crashes, disk crashes, etc.</li>
</ul></li>
</ul>
<h3 id="raid-redundant-array-of-inexpensive-disks">RAID (redundant array of inexpensive disks)</h3>
<ul>
<li>Multiple disk drives provide reliability via <em>redundancy</em>.
<ul>
<li>Increases the mean time to failure.</li>
<li>Improve reliability by storing redundant data.</li>
<li>Improve performance with disk striping (use a group of disks as one storage unit).</li>
</ul></li>
<li>RAID is arranged into six different levels.
<ul>
<li>Mirroring (RAID 1) keeps duplicate of each disk.</li>
<li>Striped mirrors (RAID 1+0) or mirrored stripes (RAID 0+1) provides high performance and high reliability.</li>
<li>Block interleaved parity (RAID 4,5,6) uses much less redundan.</li>
</ul></li>
<li>RAID 0: Non-redundant disk array.
<ul>
<li>Files are strped across disks, no redundant info.</li>
<li>High read throughput, best write throughput.</li>
<li>Any disk failure results in data loss.</li>
<li>1 I/O requests to read a byte in a disk block.</li>
<li>2 I/O requests to write a byte in a block. (Read block, modify, and write back.)</li>
</ul></li>
<li>RAID 1: Mirrored disks.
<ul>
<li>Data is written to two places. On failure, just use surviving disk and easy to rebuild.</li>
<li>On read, choose fastest to read. 2x performance.</li>
<li>Write performance is same as single drive.</li>
<li>Expensive (high space overhead).</li>
<li>1 I/O requests to read a byte in a disk block.</li>
<li>3 I/O requests to write a byte in a disk block.</li>
</ul></li>
<li>RAID 0+1: Stripe on a set of disks. Then mirror of data blocks is striped on the second set.</li>
<li>RAID 1+0: Pair mirrors first. Then stripe on a set of paired mirrors. Better reliability than RAID 0+1.</li>
<li>RAID 2: Memory-style error-correcting codes.</li>
<li>RAID 3: Bit-interleaved parity.</li>
<li>RAID 4: Block-interleaved parity.</li>
<li>RAID 5: Block-interleaved distributed parity, with redundancy to recover from a single disk failure.
<ul>
<li>Files are striped as blocks. Blocks are distributed among disks. Parity blocks are added.</li>
<li>1 I/O requests to read a byte in a disk block.</li>
<li>4 I/O requests to write a byte in a block. (Read old data block, read old parity block, write new data block, write new parity block = old data XOR old parity XOR new data).</li>
</ul></li>
<li>RAID 6: RAID 5 with extra redundancy to recover from two disk failures.</li>
</ul>
<h3 id="more-reliable-file-systems">More reliable file systems</h3>
<ul>
<li><strong>Carefully order operation sequence</strong>
<ul>
<li>Read data structures to see if there were any operations in progress. Clean up/finish as needed.</li>
<li>FAT and FFS (fsck) to protect filesystem structure/metadata.</li>
<li>Many app-level recovery schemes (e.g., MS Word, emacs autosaves).</li>
</ul></li>
<li><strong>Copy on write file layout</strong>
<ul>
<li>To update file system, write a new version of the file system containing the update.</li>
<li>NetApp's Write Anywhere File Layout (WAFL).</li>
<li>ZFS (Sun/Oracle) and OpenZFS.</li>
</ul></li>
<li><strong>Transactional file systems</strong>. Better reliability through use of log.
<ul>
<li>Applies updates to system metadata using transactions committed once it is written to the disk log.</li>
<li>Updates to non-directory files (i.e., user stuff) can be done in place (without logs), full logging optional.</li>
<li>File system may not be updated immediately, data preserved in the log.</li>
<li>Example: NTFS, Apple HFS+, Linux XFS, JFS, ext3, ext4.</li>
</ul></li>
</ul>
<h3 id="transactional-file-systems">Transactional file systems</h3>
<ul>
<li>Typical structure
<ul>
<li>Begin a transation: Get transaction ID.</li>
<li>Do a bunch of updates. If any fail along the way, rollback. Or, if any conflicts with other transactions, rollback.</li>
<li>Commit the transaction.</li>
</ul></li>
<li>An <em>atomic sequence</em> of actions on a storage system (or database).</li>
<li>It extends concept of atomic update from memory to stable storage.
<ul>
<li>Atomically update multiple persistent data structures.</li>
<li>That takes it from one <em>consistent state</em> to another.</li>
</ul></li>
<li>The ACID properties of transactions.
<ul>
<li><strong>Atomicity</strong>. Operations appear to happen as a group, or not at all (at logical level). At physical level, only single disk/flash write is atomic.</li>
<li><strong>Consistency</strong>. Transactions maintain data integrity, e.g., file size cannot be negative. If the DB starts out consistent, it ends up consistent at the end of transaction.</li>
<li><strong>Isolation</strong>. Execution of one transaction is isolated from that of all others; no problems from concurrency.</li>
<li><strong>Durability</strong>. If a transaction commits, its effects persist despite crashes.</li>
</ul></li>
<li><strong>Logging file systems</strong>
<ul>
<li>Full logging file system: All updates to disk are done in transcations.</li>
<li>Instead of modifying data strucytures on disk directly, write changes to a journal/log.
<ul>
<li>Intention list: set of changes we intend to make.</li>
<li>Log/journal is append-only.</li>
</ul></li>
<li>Once changes are on log, safe to apply changes to data structures on disk.
<ul>
<li>Recovery can read log to see what changes were intended.</li>
</ul></li>
<li>Once chances are copied, safe to remove log.</li>
</ul></li>
<li><strong>Redo logging</strong>
<ul>
<li>Prepare: Write all changes (in transaction) to log.</li>
<li>Commit: Single disk write to make transaction durable.</li>
<li>Redo: Copy changes to disk.</li>
<li>Garbage collection: Reclaim space in log.</li>
<li>Recovery: Read log; redo any operations for committed transactions; garbage collect log.</li>
</ul></li>
<li><strong>Example: Createing a file</strong>
<ul>
<li>Find free data blocks. Find free inode entry. Find directory entry insertion point.</li>
<li>[log] Write map (i.e., mark used). [log] Write inode entry to point to block(s). [log] Write dir entry to point to indoe.</li>
<li>Crash during logging: Scan the log. Detect transaction start with no commit. Discard log entries. Disk remains unchanged.</li>
<li>Recovery after commit: Scan log, find start. Find matching commit. Redo it as usual.</li>
</ul></li>
</ul>
<h2 id="cloud-computing-with-kv-storages-and-distributed-file-systems">Cloud Computing with KV Storages and Distributed File Systems</h2>
<h3 id="cluster-computing">Cluster computing</h3>
<ul>
<li>Motivations: Large-scale data processing on clusters.</li>
<li>Cost-efficiency: Commodity nodes/network. Automatic fault-tolerance. Easy to use.</li>
<li>Functions: Automatic parallelization and distribution. Fault-tolerance.</li>
<li>Typical cluster: 40 nodes/rack, 1k-4k nodes in cluster. 1Gbps bandwidth in rack, 8Gbps out of rack. Each node has 8-16 cores, 32GB RAM, 8x1.5TB disks.</li>
<li>Why colud computing?
<ul>
<li>Cloud refers to large Internet services running on many machines.</li>
<li>Cloud computing refers to services by these companies that let external customers rent cycles.</li>
<li>Attractive features: Scalability, elastic computing, ease of use.</li>
</ul></li>
</ul>
<h3 id="key-value-storage">Key value storage</h3>
<ul>
<li>Handle huge volumes of data. Store (key, value) pair. Used sometimes as a simpler but more scalable database.</li>
<li>Simple interface: <code>put(key, value)</code>, <code>value = get(key)</code>.</li>
<li>The KV-tables used for column-based storage, as opposed to row-based storage typical in older DBMS.</li>
<li>Example: Amazon DynamoDB/S3, BigTable/HBase/Hypertable, Cassandra, Memcached, BitTorrent distributed file location, Redis, Oracle, etc.</li>
<li>Key value store is also called distributed hash tables (DHT).
<ul>
<li>Main idea: Partition set of key-values across many machines.</li>
<li>Challenges: Fault tolerance, scalability, consistency, heterogeneity.</li>
</ul></li>
<li>Dictonary-based architecture: Have a node maintain the mapping between keys and machines (nodes) that store the values associated with the keys.
<ul>
<li>Recursive query: Having the master relay the requests.
<ul>
<li>Pros: Faster, as typically master/directory closer to nodes. Easier to maintain consistency, as master/directory can serialize puts and gets.</li>
<li>Cons: Scalability bottlenect, as all values go through master.</li>
</ul></li>
<li>Iterative query: Return node to requester and let requester contact node.
<ul>
<li>Pros: Scalability.</li>
<li>Cons: Slower, harder to enfore data consistency.</li>
</ul></li>
</ul></li>
<li><strong>Fault tolerance</strong>: Replicate value on several nodes. Usually place replicas on different racks in a datacenter to guard against rack failures.</li>
<li><strong>Scalability</strong>
<ul>
<li>More storage: Use more nodes.</li>
<li>More requests: Can serve requests from all nodes which a value is stored in parallel. Master can replicate a popular value on more nodes.</li>
<li>Master/directory scalability: Replicate it. Partition it, so different keys are served by different masters/directories.</li>
<li>Load balacning
<ul>
<li>Directory keeps track of the storage availability at each node. Preferentially insert new values on nodes with more storage available.</li>
<li>When a new node is added. Cannot insert only new values on new node. Move values from heavy loaded nodes to the new node.</li>
<li>When a node fails. Need to replicate values from fail node to other nodes.</li>
</ul></li>
</ul></li>
<li><strong>Consistency</strong>
<ul>
<li>Need to make sure that a value is replicated correctly. In general, slow puts and faster gets.</li>
<li>If concurrent updates, may need to make sure that updates happen in the same order to avoid inconsistent write/write.</li>
<li>Read cannot guaranteed to return value of lastest write. Can happen if the master processes requests in different threads. Inconsistent write/read.</li>
<li><strong>Atomic consistency</strong> (linearizability): reads/writes (gets/puts) to replicas appear as if there was a single underlying replica (single system image). Think one updated at a time. Transactions</li>
<li><strong>Eventual consistency</strong>: Given enough time all updates will propagate through the system. One of the weakest form of consistency; used by many systems in practice. Must eventually converge on single key/value.</li>
<li>And more: Causual consistency, sequential consistency, strong consistency, etc.</li>
</ul></li>
<li><strong>Quorum consensus</strong>
<ul>
<li>Improve <code>put()</code> and <code>get()</code> operation performance.</li>
<li>Define a replica set of size N. <code>put()</code> waits for acks from at least W replicas. The writer returns after it hears from these replicas. <code>get()</code> waits for responses from at least R replicas. Make sure W+R&gt;N.</li>
<li>There is at least one node that contains the update.</li>
</ul></li>
</ul>
<h3 id="distributed-file-systems">Distributed file systems</h3>
<ul>
<li>The interface is the same as a single-machine file system.</li>
<li>Distribute file data to a number of machines. Support replication. Support concurrent data access.</li>
<li>Google file system and HDFS (Hadoop). Optimized for batch processing. Provides redundant storage of massive amounts of data on cheap and unreliable computers.</li>
<li><strong>HDFS API</strong>
<ul>
<li>Copy data from the local to HDFS: <code>copyFromLocal src dest</code>.</li>
<li>Copy data from HDFS to local: <code>copyToLocal src dest</code>.</li>
</ul></li>
<li><strong>Assumptions of GFS/HDFS</strong>
<ul>
<li>High component failure rates. Inexpensive commodity components fail all the time.</li>
<li>Modest number of huge files. Just a few million. Each is 100 MB or larger.</li>
<li>Files are write-once, mostly appended to. Perhaps concurrently.</li>
<li>Large streaming reads.</li>
<li>High sustained through favored over low latency.</li>
</ul></li>
<li><strong>HDFS architecture</strong>
<ul>
<li>Files split into 64MB blocks.</li>
<li>Blocks are replicated across several datanodes (default 3) as slaves.</li>
<li>Namenode stores metadata (file names, locations, etc) as a master.</li>
<li>Files are appended-only. Optimized for large files, sequential reads.
<ul>
<li>Read use any copy, write append to all replicas.</li>
</ul></li>
<li>Namenode: Maps a file to a fileid and list of block ids and data nodes.</li>
<li>Datanode: Maps a blockid to a physical location on disk.</li>
<li>Secondary namenode: Backup. Periodic merge of transaction log.</li>
</ul></li>
<li><strong>Namenode metadata</strong>
<ul>
<li>The entire metadata is in memory. No demand paging of meta-data.</li>
<li>Types of metadata: List of files/blocks for each file/datanodes for each block. File attributes.</li>
<li>A transaction log: Records file creations, file deletions, etc.</li>
</ul></li>
<li><strong>Datanode</strong>
<ul>
<li>A block server: Stores data in local file system (e.g., ext3). Stores metadata of a block (e.g., CRC). Serves data and metadata to clients.</li>
<li>Block report: Periodically sends a report of all existing blocks to the name node.</li>
<li>Facilitates pipelining of data: Forwards data to other specified data nodes.</li>
<li>Block placement current strategy: One replica on local node, second on a remote rack, third on the same remote rack. Additional ones are placed randomly.</li>
</ul></li>
<li><strong>Datanode failure detecture with heartbeat</strong>
<ul>
<li>A network partition can cause a subset of Datanodes to lose connectivity with the Namenode.</li>
<li>Namenode detects this condition by the absence of a Heartbeat message.</li>
<li>Namenode marks Datanodes without Hearbeat and does not send any IO requests to them.</li>
<li>Any data registered to the failed Datanode is not available to the HDFS.</li>
<li>Also the death of a Datanode may cause replication factor of some of the blocks to fall below their specified value.</li>
</ul></li>
<li><strong>Data pipelining during data block write</strong>
<ul>
<li>Client retrieves a list of DataNodes on which to place replicas of a block.</li>
<li>Client writes block to the first DataNode.</li>
<li>The first DataNode forwards the data to the next DataNode in the Pipeline.</li>
<li>When all replicas are written, the Client moves on to write the next block in file.</li>
</ul></li>
<li><strong>Ensuring data correctness during reading</strong>
<ul>
<li>Use checksums (e.g., CRC32) to validate data.</li>
<li>File creation: Client computes checksum per 512 bytes. Datanode stores the checksum.</li>
<li>File accesS: Client retrieves the data and checksum from datanode. If validation fails, client tries other replicas.</li>
</ul></li>
<li><strong>HDFS properties</strong>
<ul>
<li>HDPS provides a write-once-read-many, append-only access model for data.</li>
<li>HDFS is optimized for sequential reads of large files with large blocks (e.g. 64MB)</li>
<li>HDFS maintains multiple copies of the data for fault tolerance.</li>
<li>HDFS is designed for high-throughput, rather than low-latency.</li>
<li>Hadoop jobs (e.g. MapReduce) tend to execute over several minutes and hours.</li>
</ul></li>
</ul>
<h2 id="problem-set">Problem Set</h2>
<h3 id="exercise-1">Exercise 1</h3>
<p><strong>Q1</strong> Which of the following services is considered as a common service provided by OS?</p>
<ol style="list-style-type: decimal">
<li>Compiler</li>
<li>User graphical interface</li>
<li><strong>Process and memory management</strong></li>
</ol>
<p><strong>Q2</strong> Which of the following instructions should be privileged and executed in a kernel mode?</p>
<ol style="list-style-type: decimal">
<li>Set value of application timer based on CPU clock value.</li>
<li>Read the clock.</li>
<li>Issue a trap signal instruction.</li>
<li>Access I/O device.</li>
</ol>
<p><strong>Q3</strong> What is the purpose of system calls?</p>
<ol style="list-style-type: decimal">
<li>They are functions which can be executed in an OS.</li>
<li>They allow user-level processes to request services of the operating system.</li>
<li>They provide the multi-threading or processing capability.</li>
</ol>
<p><strong>Q4</strong> When a process creates a new process using the <code>fork()</code> operation, which of the following state is shared between the parent process and the child process?</p>
<ol style="list-style-type: decimal">
<li>Stack</li>
<li>Heap</li>
<li><strong>Shared memory segments</strong>. Only the shared memory segments are shared between the parent process and the newly forked child process. Copies of the stack and the heap are made for the newly created process.</li>
</ol>
<p><strong>Q5</strong> Which of the following statements is true on user-level threads and kernel-level threads?</p>
<ol style="list-style-type: decimal">
<li>User-level threads and kernel threads are both aware by the kernel.</li>
<li><strong>User threads are scheduled by the thread library and the OS kernel schedules kernel threads.</strong></li>
<li>Kernel or user threads are associated with only one process.</li>
</ol>
<p><strong>Q6</strong> Which of the following statements is true on context-switch among threads?</p>
<ol style="list-style-type: decimal">
<li><strong>Context switching between threads require saving the value of the CPU registers from the thread being switched out.</strong></li>
<li>Context switching between kernel threads typically do not require saving the value of the CPU registers from the thread being switched out.</li>
<li>Context switch restores the CPU registers of the new thread being scheduled if this new thread is a user thread.</li>
</ol>
<p><strong>Q7</strong> Trace this program without actually running in a machine.</p>
<div class="sourceCode"><pre class="sourceCode c++"><code class="sourceCode cpp"><span class="dt">void</span> main() {
    <span class="dt">char</span> *cmd[] = {echo, hello,<span class="dv">0</span>};
    <span class="dt">int</span> pid, x = <span class="dv">10</span>, y;
    pid = fork();
    <span class="cf">if</span> (pid == <span class="dv">0</span>) {
        y=<span class="dv">1</span>; x=x+y;
        printf(<span class="st">&quot;x is </span><span class="sc">%d</span><span class="st">. y is </span><span class="sc">%d</span><span class="st"> </span><span class="sc">\n</span><span class="st">&quot;</span>, x,y);
        execvp(cmd[<span class="dv">0</span>], cmd);
        printf(<span class="st">&quot;x is </span><span class="sc">%d</span><span class="st">. y is </span><span class="sc">%d</span><span class="st"> </span><span class="sc">\n</span><span class="st">&quot;</span>, x,y);
    } <span class="cf">else</span> {
        y=<span class="dv">2</span>; x=x+y;
        wait(NULL);
        printf(<span class="st">&quot;x is </span><span class="sc">%d</span><span class="st">. y is </span><span class="sc">%d</span><span class="st"> </span><span class="sc">\n</span><span class="st">&quot;</span>, x,y);
    }
}</code></pre></div>
<ul>
<li>Output: <code>x = 11, y = 1; hello; x = 13, y = 2</code>.</li>
<li>The child process's x value is not shared with parent. Thus the modification is not reflected in the parent process.</li>
<li>Function excevp() loads a new program and thus wipes out the parent's code. But if for any reason, loading this program (shell) fails, the print statement (x=11,y=1) does appear.</li>
<li>The order of printing is fixed because the parent process waits the child process.</li>
</ul>
<p><strong>Q8</strong> How many times does this program print &quot;hello&quot;? (6 times)</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">void</span> main() { fork(); printf(<span class="st">&quot;hello</span><span class="sc">\n</span><span class="st">&quot;</span>); fork(); printf(<span class="st">&quot;hello</span><span class="sc">\n</span><span class="st">&quot;</span>); }</code></pre></div>
<p><strong>Q9</strong> Given this program started from empty file tmpfile1</p>
<pre><code>int main() {
    int pid, fd; char *s1;
    pid = fork();
    fd = open(&quot;tmpfile1&quot;, O_WRONLY|O_CREAT, 0666);
    if (pid &gt; 0) {
      s1 = &quot;Parent&quot;;
    } else {
      sleep(1);
      s1 = &quot;Child&quot;;
    }
    write(fd, s1, strlen(s1));
    close(fd);
}</code></pre>
<p>Output is <code>childt</code>. The child process writes the same file from position 0 after the parent writes from offset position 0 also.</p>
<p><strong>Q10</strong> Given this program started from empty file tmpfile1</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">int</span> main() {
    <span class="dt">int</span> pid, fd; <span class="dt">char</span> *s1;
    fd = open(<span class="st">&quot;tmpfile1&quot;</span>, O_WRONLY| O_CREAT, <span class="bn">0666</span>);
    pid = fork();
    <span class="cf">if</span> (pid &gt; <span class="dv">0</span>) {
        s1 = <span class="st">&quot;Parent&quot;</span>;
    } <span class="cf">else</span> {
        sleep(<span class="dv">1</span>);
        s1 = <span class="st">&quot;Child&quot;</span>;
    }
    write(fd, s1, strlen(s1)); close(fd);
}</code></pre></div>
<p>Output is <code>ParentChild</code>. The child process shares the file descriptors of the parent and thus shares the file seek pointer (offset) also.</p>
<h3 id="exercise-2">Exercise 2</h3>
<p><strong>Q1</strong> Choose one which does not answer the question on &quot;what is a race condition&quot;?</p>
<ol style="list-style-type: decimal">
<li>When the behavior of a program relies on the interleaving of operations of different threads.</li>
<li>Two or more processes (threads) are reading and writing on shared data and the final result depends on who runs precisely &amp; when.</li>
<li><strong>Two or more processes (threads) are reading and writing on shared data and the final result depends on who runs precisely &amp; when.</strong></li>
</ol>
<p><strong>Q2</strong> A semaphore puts a thread to sleep</p>
<ol style="list-style-type: decimal">
<li>If it increments the semaphore's value above 0.</li>
<li><strong>If it tries to decrement the semaphore's value below 0.</strong></li>
<li>Until another thread issues a signal to notify this semaphore.</li>
<li>Until the semaphore's value reaches a certain number.</li>
</ol>
<p><strong>Q3</strong> Given three statements</p>
<ol style="list-style-type: decimal">
<li><ol start="6" style="list-style-type: upper-alpha">
<li>A semaphore can be implemented using conditional variables with locks.</li>
</ol></li>
<li><ol start="20" style="list-style-type: upper-alpha">
<li>A condition variable can be implemented using semaphores.</li>
</ol></li>
<li><ol start="20" style="list-style-type: upper-alpha">
<li>A lock can be implemented using a semaphore.</li>
</ol></li>
</ol>
<p><strong>Q4</strong> Which statement is false about starvation and deadlock</p>
<ol style="list-style-type: decimal">
<li>Starvation implies that a thread cannot make progress because other threads are using resources it needs.</li>
<li>Starvation can be recovered, for example when the other processes finish.</li>
<li>Deadlock is a circular wait without preemption that can never be recovered from.</li>
<li><strong>Starvation is a deadlock.</strong> (P.S. But deadlock is a starvation)</li>
</ol>
<p><strong>Q5</strong> Trace the following pthreads C program running on a time sharing OS in dynamic context switching is possible, select one answer on the possible output of x.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">int</span> x=<span class="dv">0</span>;
<span class="dt">pthread_mutex_t</span> Lock;
<span class="dt">void</span> * tcode(<span class="dt">void</span> *arg) {
    pthread_mutex_lock(&amp;Lock);
    x++;
    pthread_mutex_unlock(&amp;Lock);
}

<span class="dt">void</span> main() {
    <span class="dt">pthread_t</span> t1,t2;
    pthread_mutex_init(&amp;Lock,NULL);
    pthread_create(&amp;t1, NULL, tcode, NULL);
    pthread_create(&amp;t2, NULL, tcode, NULL);
    x++;
    pthread_join(t1, NULL);
    pthread_join(t2, NULL);
    printf(<span class="st">&quot;x: </span><span class="sc">%d</span><span class="st"> </span><span class="sc">\n</span><span class="st">&quot;</span>,x);
}</code></pre></div>
<ol style="list-style-type: decimal">
<li>0, 1, 2, 3</li>
<li><strong>1, 2, 3</strong></li>
<li>2, 3</li>
<li>3</li>
</ol>
<p>The two threads that perform x++ can make the x value increase from 0 to 2. The main program does x++ also, which is not synchronized and can create a race condition. The main thread can either get x as 0, 1, 2 to increase, which can make the final x value as 1,2, or 3.</p>
<p><strong>Q6</strong> The following C code is a simplified version of lock implementation using a Pthread semaphore..</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="kw">typedef</span> <span class="kw">struct</span> locks {
    <span class="dt">sem_t</span> sem;
    <span class="dt">pthread_t</span> thread;
} <span class="dt">lock_t</span>;

<span class="dt">int</span> mutex_init(<span class="dt">lock_t</span> *lockp) {
    <span class="co">//return 0 when successful</span>
    lockp-&gt;thread= pthread_self();
    <span class="cf">return</span> sem_init(&amp;lockp-&gt;sem,<span class="dv">0</span>,<span class="dv">1</span>); <span class="co">// (1)</span>
}

<span class="dt">int</span> mutex_lock(<span class="dt">lock_t</span> *lockp) {
    <span class="co">// return 0 when successful</span>
    <span class="cf">if</span> (pthread_equal(lockp-&gt;thread, pthread_self()))
        <span class="cf">return</span> sem_wait( &amp;lockp-&gt;sem); <span class="co">// (2)</span>
    <span class="cf">return</span> <span class="dv">-1</span>;
}

<span class="dt">int</span> mutex_unlock(<span class="dt">lock_t</span> *lockp) {
    <span class="co">// return 0 when successful</span>
    <span class="cf">if</span> (pthread_equal(lockp-&gt;thread, pthread_self()))
        <span class="cf">return</span> sem_post(&amp;lockp-&gt;sem); <span class="co">// (3)</span>
    <span class="cf">return</span> <span class="dv">-1</span>;
}</code></pre></div>
<p><strong>Q7</strong> The following program manages a set of free memory blocks used by multiple threads. Each thread calls <code>allocate()</code> to get a free block to use and calls <code>free()</code> to release a specific block so others can use. The program maintains an array <code>available[NB]</code> whose elements are non-zero if the corresponding blocks are available (<code>NB</code> is a constant indicating the total memory blocks available initially).</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">int</span> available[NB];
<span class="dt">int</span> allocate() { <span class="co">/* Returns index of available free blocks. */</span>
    <span class="cf">while</span>(<span class="dv">1</span>)
        <span class="cf">for</span> (<span class="dt">int</span> i=<span class="dv">0</span>; i &lt; NB; i++)
            <span class="cf">if</span> (available [i] != <span class="dv">0</span>) { available[i] = <span class="dv">0</span>; <span class="cf">return</span> i; }
}
free (<span class="dt">int</span> i) { available[i] = <span class="dv">1</span>; }</code></pre></div>
<p>Modify the above code using 1) locks and condition variables so that the multiple threads are well symphonized in accessing the critical section and busy waiting is minimized. 2) using semaphores only. You can express your solution using either Pthreads or Nachos synchronization primitives.</p>
<p>Lock and condition variable solution</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="co">// Next condition indicates there is no memory available and threads are waiting</span>
Condition* noMemory = <span class="kw">new</span> Condition(<span class="st">&quot;noMemory&quot;</span>);
<span class="co">// Next lock is used for mutual exclusion in checking the available array.</span>
Lock *mutex = <span class="kw">new</span> Lock(<span class="st">&quot;mutex&quot;</span>);

<span class="dt">int</span> allocate() {
    mutex-&gt;Acquire();
    <span class="cf">while</span>(<span class="dv">1</span>) {
      <span class="cf">for</span> (<span class="dt">int</span> i = <span class="dv">0</span>; i &lt; NB; i++)
          <span class="cf">if</span> (available [i] != <span class="dv">0</span>) { available[i] = <span class="dv">0</span>; mutex-&gt;Release(); <span class="cf">return</span> i; }
      noMemory-&gt;Wait(mutex);
    }
}
<span class="dt">int</span> free(<span class="dt">int</span> i) {
    mutex-&gt;Acquire(); available[i] = <span class="dv">1</span>; noMemory-&gt;Signal(mutex); mutex-&gt;Release();
}</code></pre></div>
<p>Semaphore solution</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="co">// next semaphore indicates if there are free memory blocks available.</span>
<span class="co">// Initially there are NB blocks available</span>
Semaphore *freeBlocks = <span class="kw">new</span> Semaphore(<span class="st">&quot;Free Blocks&quot;</span>, NB);
<span class="co">// Next semaphore is used for mutex exclusion in accessing available array</span>
Semaphore *mutex = <span class="kw">new</span> Semaphore(<span class="st">&quot;mutex&quot;</span>, <span class="dv">1</span>);

<span class="dt">int</span> allocate() {
    <span class="cf">while</span>(<span class="dv">1</span>) {
        freeBlocks-&gt;P();
        mutex-&gt;P();
        <span class="cf">for</span> (<span class="dt">int</span> i = <span class="dv">0</span>; i &lt; NB; i++)
            <span class="cf">if</span> (available [i] != <span class="dv">0</span>) { available[i] = <span class="dv">0</span>; mutex-&gt;V(); <span class="cf">return</span> i; }
        mutex-&gt;V();
    }
}
<span class="dt">void</span> free(<span class="dt">int</span> i) {
    mutex-&gt;P(); available[i] = <span class="dv">1</span>; freeBlocks-&gt;V(); mutex-&gt;V();
}</code></pre></div>
<h3 id="exercise-3">Exercise 3</h3>
<p><strong>Q2</strong> What is the possible output of running this nachos program <code>ThreadTest()</code>? Give an explanation without actually running this code on a machine.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">int</span> x = <span class="dv">10</span>;
<span class="dt">void</span> SimpleThread(<span class="dt">int</span> y) {
    printf(<span class="st">&quot; Before yield, Thread </span><span class="sc">%d</span><span class="st">: x is </span><span class="sc">%d</span><span class="st">.</span><span class="sc">\n</span><span class="st">&quot;</span>, y, x);
    currentThread-&gt;Yield();
    x = x + y;
    printf(<span class="st">&quot; After yield, Thread </span><span class="sc">%d</span><span class="st">: x is </span><span class="sc">%d</span><span class="st">. </span><span class="sc">\n</span><span class="st">&quot;</span>, y, x);
}
<span class="dt">void</span> ThreadTest() {
    Thread *t = <span class="kw">new</span> Thread(<span class="st">&quot;forked thread&quot;</span>);
    t-&gt;Fork(SimpleThread, <span class="dv">1</span>);
    SimpleThread(<span class="dv">2</span>);
}</code></pre></div>
<ul>
<li><strong>Before yield, Thread 2: x is 10. Before yield, Thread 1: x is 10. After yield, Thread 2: x is 12. After yield, Thread 1: x is 13.</strong></li>
<li>Before yield, Thread 1: x is 10. Before yield, Thread 2: x is 10. After yield, Thread 1: x is 11. After yield, Thread 2: x is 13.</li>
</ul>
<p>The parent thread places the child thread in the ready queue, and executes <code>SimpleTread(2)</code> first and prints, then it yields. The child thread starts now and prints one sentence, then yields.</p>
<p><strong>Q4</strong> Which of the following steps is not necessary to run a program.</p>
<ol style="list-style-type: decimal">
<li>Obtain CPU cycle and allocate memory.</li>
<li>Load code/data into memory. Setup stack/heap.</li>
<li><strong>Copy the allocated in-memory content to disk in case the program is being swapped out.</strong></li>
<li>Load starting address and begin execution.</li>
<li>Provide the service with address translation and system calls, and control the execution.</li>
</ol>
<p><strong>Q5</strong> Which statement is not true regarding difference between logical and physical addresses.</p>
<ol style="list-style-type: decimal">
<li>A logical address does not refer to an actual existing address; rather, it refers to an abstract address in an abstract address space.</li>
<li><strong>A physical address refers to an address of a data item used by a program.</strong></li>
<li>A logical address is generated by the CPU and is translated into a physical address by the memory management unit(MMU).</li>
<li>Physical addresses are generated by the MMU.</li>
</ol>
<p><strong>Q6</strong> Consider a logical address space of 64 pages of 1KB each, mapped onto a physical memory of 32 pages. How many bits are there in the logical address? How many bits are there in the physical address?</p>
<ol style="list-style-type: decimal">
<li>Logical address: 64 bits. Physical address: 32 bits</li>
<li><strong>Logical address: 16 bits. Physical address: 15 bits</strong></li>
<li>Logical address: 16 bits. Physical address: 16 bits</li>
<li>Logical address: 15 bits. Physical address: 15 bits</li>
</ol>
<p><strong>Q7</strong> Given a one-level paging scheme for memory address mapping, if TLB access takes 10 nanosecond and each physical memory access takes 100 nanoseconds, what will be the desired TLB hit ratio to have the effective memory access time within 120 nanoseconds to complete both logical address translation and physical memory data fetching?</p>
<p>Solve <span class="math inline">\(10 + (1-x) * 100 + 100 = 120\)</span> for <span class="math inline">\(x = 0.9\)</span>.</p>
<p><strong>Q8</strong> Given a one-level paging scheme where physical memory is divided into a set of pages with uniform size 8K bytes and each page table entry uses 4 bytes, what is the maximum size of a logical memory space for each process with this paging scheme? Explain your reason.</p>
<p>The one-level page table has to fit into one page and thus at most 8K bytes. Since each entry uses 4 bytes, there are 8K/4 = 2K entries in a page table. Thus there are total of 2K pages, the maximum size of logical memory space is 2K*8KB = 16MB.</p>
<p><strong>Q9</strong> Design a 2-level paging scheme for a 38-bit logical space and illustrate the address translation steps. Each physical page is of size 16K bytes and each page table entry uses 4 bytes. Each 38-bit logical address is translated into a 38-bit physical address.</p>
<ul>
<li>The scheme is 2-level paging and 38-bit will be split to 12, 12, 14.</li>
<li>Physical page size is 16K bytes, and bits for a page use log(16K) = 14 bits.</li>
<li>Each physical page can host 16K/4 =4K entries. Thus a page table using one physical page can map at most 4K x 16K=64MB physical space. A two-level table can map at most 64MB*4K=256GB space which is 2^38.</li>
<li>The outer table has 4K entries, and log(4K)=12. That needs 12 bits to identify a particular entry. Thus the outer table part of the logical address needs 12 bits.</li>
<li>Consequently, the inner table part of logical address needs 12 bits.</li>
</ul>
<p><strong>Q10</strong> Design a 3-level paging scheme for a 64-bit logical space. Each physical page is of size 4K bytes and each page table entry uses 4 bytes.</p>
<p>(10, 10, 10, 12)</p>
<p><strong>Q11</strong> What is the maximum size of logical and physical spaces that can be mapped by the 3-level paging scheme in Question 10?</p>
<ul>
<li>Logical: 4TB.</li>
<li>Physical: 16TB = 2^32 x 2^12 = 2^34.</li>
</ul>
<p><strong>Q12</strong> The following C program access a C array with a million of integer elements, running on a Linux machine. How does increasing of the page size affect the TLB miss rate? Explain the reason.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">int</span> i, sum = <span class="dv">0</span>;
<span class="cf">for</span> (i = <span class="dv">0</span>; i &lt; <span class="dv">10000000</span>; i++)
    sum += a[i];</code></pre></div>
<ol style="list-style-type: decimal">
<li><strong>The increasing of the page size can decrease TLB miss rate</strong></li>
<li>The decreasing of the page size can decrease TLB miss rate</li>
<li>The increasing of the page size does not affect TLB miss rate</li>
<li>The decreasing of the page size does not affect TLB miss rate</li>
</ol>
<p>For example, the typical size of Linux OS page is 4KB and each integer in C takes 4 bytes. Thus each page can host 1K integers. This program accesses the array a[] in a consecutive way. Thus when TLB misses the first element of a page, it will hit the rest of integer numbers in this page. The TLB miss rate is 1 out of 1K. When increasing the page size to 8K, the TLB miss rate decreases to 1 out of 2K.</p>
<h3 id="exercise-4">Exercise 4</h3>
<p><strong>Question 1</strong> Given the following C code, assume x is allocated to a register. Initially the binary code (text) and stack segments of this program is in memory, but others segments are not in memory. All OS kernel pages are in memory.</p>
<pre><code>int x=1; // Line 1
x = open(&quot;tmpfile1&quot;, O_WRONLY, 0666); //Line 2
close(x); // Line 3</code></pre>
<p>The execution of the following lines in above code triggers a page fault:</p>
<ol style="list-style-type: decimal">
<li>Line 1, Line 2, Line 3</li>
<li>Line 2, Line 3</li>
<li><strong>Line 2</strong> accesses &quot;tmpfile1&quot; which sits in the initialized data segment which is not in memory.</li>
<li>Line 3</li>
</ol>
<p><strong>Question 2</strong> Which statement is not true when a page fault occurs?</p>
<ol style="list-style-type: decimal">
<li>The process that causes this page fault may enter a sleep mode.</li>
<li>A free frame is located and I/O is requested to read the needed page into the free frame.</li>
<li>Upon completion of page fault handling, the page table of the process that triggers page fault is modified. The corresponding instruction is restarted again.</li>
<li><strong>The process of the victim page will be locked until this page is available again in memory</strong>.</li>
</ol>
<p><strong>Question 3</strong> Which of the following data access patterns are often good for a demand-paged environment in terms of exploiting data locality for good time efficiency? Which are often bad?</p>
<ol style="list-style-type: decimal">
<li>Stack-based assignments (Good: Stack with push and pop calls. Access of a consecutive part of a stack exhibits a good spatial locality.)</li>
<li>Sequential search (Good: Scan a list of elements sequentially and one can use an array to implement. Thus it tends to have a good spatial locality.)</li>
<li>Binary search (Bad: Binary search using a tree accesses a set of data objects scattered in memory and there is no access locality. Binary search of a big sorted array jumps from one memory address to another, which has no locality.)</li>
<li>Object traversal with many pointer indirections (e.g., linked list or graph traversal) (Bad: Jumping around objects does not exhibit spatial or temporal locality normally.)</li>
</ol>
<p>Explanation: Physical memory is a cache for virtual pages. If a program accesses data has some temporal or spatial locality, such a program has a better performance (content of a virtual page loaded to memory can be used again and again).</p>
<p><strong>Question 4</strong> Consider a demand-paging system that costs about 10,000 microseconds to deal with a page fault. Addresses are translated through a TLB and a one-level page table in the main memory. The cost for accessing TLB is considered as 0 and cost of each memory access is 0.1 microsecond. Assume that 99 percent of the accesses is in the TLB and virtual pages translated by TLB are always in memory. For the remaining 1% of the accesses, address translation is completed through the page table, but virtual pages may not reside in memory. What is the effective memory access time when the page fault rate is 0.01%.</p>
<ol style="list-style-type: decimal">
<li>1 ms</li>
<li><strong>1.101 ms</strong></li>
<li>1.2 ms</li>
</ol>
<p>If there is no page fault, average memory access time in microseconds is <span class="math inline">\(0.99 (0 + 0.1) + 0.01 (0.1 + 0.1) = 0.101\)</span>. Effective memory access when possibility of pages fault is considered: <span class="math inline">\(0.99 (0.101) + 0.01 (0.101 + 10000) = 1.101\)</span>.</p>
<p><strong>Question 5</strong> Consider the following page-replacement algorithms. Rank these algorithms from the best to the worst according to their page-fault rate: LRU replacement, FIFO replacement, Optimal (MIN) replacement, Second-chance replacement.</p>
<ol style="list-style-type: decimal">
<li><strong>MIN, LRU, Second-chance, FIFO</strong></li>
<li>MIN, Second-chance, LRU, FIFO</li>
<li>MIN, FIFO, LRU, Second-chance</li>
<li>MIN, LRU, FIFO, Second-chance</li>
</ol>
<p><strong>Question 6</strong> Given 3 empty physical pages as the total memory available, an application access memory in the following page reference sequence (x,y,z,w,y,u,y). Each letter represents a distinct virtual memory page. What is the number of page faults with the LRU, second-chance, and FIFO replacement algorithms respectively?</p>
<ol style="list-style-type: decimal">
<li><strong>5, 5, 6</strong></li>
<li>5, 6, 6</li>
<li>5, 5, 7</li>
<li>6, 6, 6</li>
</ol>
<ul>
<li>LRU: xF, yF, zF, wF (replace x), y, uF (replace z), y</li>
<li>SC: xF, yF, zF, wF, y, uF, y</li>
<li>FIFO: xF, yF, zF, wF (replace x), y, uF (replace y), yF (replace z)</li>
</ul>
<p><strong>Question 7</strong> Given the following C program which access a 2D integer array <code>a[1024][1024]</code>. A demand-paging memory is divided into a set of uniform 1K byte pages. Each integer uses 4 bytes. Assume that the binary instructions for the following program are always in separate memory space. The LRU page replacement algorithm is used to manage a 512KB memory that stores the array. Initially data array <code>a[][]</code> is not in memory.</p>
<pre><code>for (i = 0; i &lt; 1024; i++) for (j = 0; j &lt; 1024; j++) a[j][i] = 100;</code></pre>
<p>What is the number of page faults in running this program?</p>
<ol style="list-style-type: decimal">
<li>1024</li>
<li><strong>1024 * 1024</strong></li>
<li>0</li>
</ol>
<p>The C program stores this array in a row-wise format. Each row needs 1024*4/1K = 4 memory pages. 512KB memory can hold 512 pages. The code accesses column 1, requiring the loading of 1K pages with 1K page faults. For column 2, old data loaded is swapped out, and thus it takes another 1K page faults. In summary, 1024x1024 page faults occur.</p>
<p><strong>Question 8</strong> For 7, if the memory increases to 1MB, what is the number of page faults in running this program?</p>
<ol style="list-style-type: decimal">
<li>1024</li>
<li>1024 * 1024</li>
<li><strong>4096</strong></li>
<li>0</li>
</ol>
<p>If memory is 1MB, then memory can hold 1K pages. Each row of the matrix requires 4 pages as it has size 4KB. After column 1 access, data in memory can still be used for column 2, column 3, ..., up to column 256. Then the miss starts again. Thus we ca reason that accessing each row generates 4 page faults. The total number of page faults will be 1024*4= 4K.</p>
<p><strong>Question 9</strong> Which statement is false on OS scheduling?</p>
<ol style="list-style-type: decimal">
<li>Preemptive scheduling allows a process to be interrupted in the midst of its execution, taking the CPU away and allocating it to another process.</li>
<li>Nonpreemptive scheduling ensures that a process relinquishes control of the CPU only when it finishes with its current CPU burst.</li>
<li><strong>Round robin scheduling improves job response time, and average turn-around time.</strong></li>
<li>Different time-quantum sizes are used at different queues at a multilevel feedback queuing system.</li>
</ol>
<p><strong>Question 10</strong> Suppose that the following processes arrive for execution at the times indicated. Each process will run for the amount of time listed in the burst time column. .Assume the context switch cost = 0.</p>
<ul>
<li>P1: Arrival time = 0, Burst time = 8</li>
<li>P2: Arrival time = 1, Burst time = 4</li>
<li>P3: Arrival time = 2, Burst time = 1</li>
</ul>
<p>What is the average turnaround time for these processes with the FIFO non-preemptive, SJF non-preemptive, RR preemptive scheduling algorithm with quantum 1?</p>
<ol style="list-style-type: decimal">
<li><strong>10, 9, 7.33</strong></li>
<li>11, 10, 7</li>
<li>10, 6, 6</li>
</ol>
<ul>
<li>FIFO: <span class="math inline">\((8-0 + 12-1 + 13-2) / 3\)</span></li>
<li>SJF: <span class="math inline">\((8-0 + 9-2 + 13-1) / 3\)</span></li>
<li>RR: <span class="math inline">\((13-0 + 9-1 + 3-2) / 3\)</span></li>
</ul>
<p>Remember that turnaround time is finishing time minus arrival time, so you have to subtract the arrival times to compute the turnaround times. FIFO's turnaround time is 11 if you forget to subtract arrival time. Another mistake is that SJF schedules all tasks following the job length without considering their arrival time. SJF (P3, P2, P1). Turnaround time would become <span class="math inline">\((3-2 + 5-1 + 13-0)/3 = 6\)</span> which is wrong.</p>
<h3 id="exercise-5">Exercise 5</h3>
<p><strong>Q1</strong> Select a statement which is not true</p>
<ol style="list-style-type: decimal">
<li>The <code>open()</code> operation informs the system that the named file is about to become active.</li>
<li>The <code>close()</code> operation informs the system that the named file is no longer in active use by the user who issued the close operation.</li>
<li><strong>The OS always have file descriptors available for new files to be opened even other programs do not close files.</strong></li>
<li>The files may not be saved properly if a program does not close all files written while exiting abnormally as OS often buffers writing before writing data out to disk.</li>
</ol>
<p><strong>Q2</strong> Given two applications: 1) Print the entire content of a file; 2) Access a record in a file given its index. The position of this record in this file is indexed. They should be accessed:</p>
<ol style="list-style-type: decimal">
<li><strong>sequentially, randomly</strong></li>
<li>randomly, sequentially</li>
<li>both sequentially</li>
<li>both randomly</li>
</ol>
<p><strong>Q3</strong> Consider a system that supports 5,000 users. Suppose that you want to allow 4,990 of these users to be able to access one file. How would you specify this protection scheme in UNIX/Linux?</p>
<ol style="list-style-type: decimal">
<li>Specify 10 users as non-accessible.</li>
<li><strong>Put these 4990 users in one group and set the group access accordingly.</strong></li>
<li>Give a soft link of this file to these 4990 users so they are able to access while other 10 users cannot access as they do not know the file name.</li>
<li>Not possible.</li>
</ol>
<p><strong>Q4</strong> We use indexed allocation to store a file on a disk and we assume that each disk file block is of size 16KB. Each data pointer entry in an i-node takes 4 bytes. Answer the following questions with an explanation.</p>
<p>What is the maximum size of a file with one-level direct mapping (one index block containing a set of direct pointers to data blocks) can map?</p>
<ol style="list-style-type: decimal">
<li>16 KB</li>
<li><strong>64 MB</strong></li>
<li>256 MB</li>
</ol>
<p><strong>Q5</strong> For Q4, What is the minimum levels of index indirection required to map a file of 16 gigabytes?</p>
<ol style="list-style-type: decimal">
<li>One-level direct mapping</li>
<li><strong>Two-level single indirection</strong></li>
<li>Three-level double indirection</li>
</ol>
<p><span class="math inline">\(16K/4B=4K\)</span> entries per index block. The maximum size of a file with single indirect block = <span class="math inline">\(4K*16KB= 64MB\)</span>. Maximum size for two levels: <span class="math inline">\(4K*4K*16K = 256GB\)</span>. 16GB would require 2 levels.</p>
<p><strong>Q6</strong> Given an i-node design as UNIX file control block structure, what is the least number of the set of disk blocks that must be read into memory in order to access a UNIX file &quot;/cs170/final.txt&quot;?</p>
<ol style="list-style-type: decimal">
<li>1</li>
<li>2</li>
<li>4</li>
<li><strong>6</strong></li>
</ol>
<ul>
<li>Read in file header for root / (fixed spot on disk).</li>
<li>Read in first data block for root / as its directory content and search cs170.</li>
<li>Read in file header for directory &quot;cs170&quot;</li>
<li>Read in first data block for directory &quot;cs170&quot;; search for &quot;final.txt&quot;</li>
<li>Read in the file header i-node for &quot;final.txt&quot;</li>
<li>Read in first data block for &quot;final.txt&quot;</li>
</ul>
<p><strong>Q7</strong> Suppose this disk drive spins at 12000 RPM, and has a sector size of 512 bytes, and holds 50 sectors per track. What is the maximum transfer rate of this driver accomplishable without seek overhead.</p>
<ol style="list-style-type: decimal">
<li><strong>5 MB/s</strong></li>
<li>6 MB/s</li>
<li>12 MB/s</li>
</ol>
<p><strong>Q8</strong> For Q7, Assume average seek time for the above drive is 7.3 milliseconds, what is the effective transfer rate for random-access of two consecutive sectors on a disk track? (Namely find the position first, then transfer two sectors together).</p>
<ol style="list-style-type: decimal">
<li><strong>0.1 MB/s</strong></li>
<li>0.133 MB/s</li>
<li>5 MB/s</li>
<li>6 MB/s</li>
</ol>
<ul>
<li>Average rotational cost is time to travel half track: 1/200 * 0.5 = 2.5ms.</li>
<li>Transfer time = 7.3ms to seek + 0.2ms to read = 10 ms.</li>
<li>Effective transferring rate: 1KB/0.01s = 0.1MB/s.</li>
</ul>
<p><strong>Q9</strong> Suppose that a disk drive has 200 cylinders, numbered from 0 to 199. The drive is currently serving a request at cylinder 120, and the previous request was at cylinder 100. The queue of pending requests, in FIFO order is: 80, 190, 40, 110. Illustrate the disk arm movement using the FCFS and SCAN algorithms. Compute the total movement in cylinders for each algorithm.</p>
<ol style="list-style-type: decimal">
<li>220, 370</li>
<li><strong>370, 220</strong></li>
<li>388, 189</li>
<li>189, 388</li>
</ol>
<ul>
<li>FCFS: 120, 80, 190, 40, 110</li>
<li>SCAN: 120, 190, 110, 80, 40</li>
</ul>
<p><strong>Q10</strong> Assume the annual failure rate of a disk drive is 2% and each server machine has 5 disk drives installed. Given 1000 servers installed in a computer cluster, how often does one of their disk drives fail?</p>
<ol style="list-style-type: decimal">
<li>Once a day</li>
<li>Once a week</li>
<li>Once a month</li>
<li><strong>Twice a week</strong></li>
</ol>
<p><strong>Q11</strong> Which statement is false?</p>
<ol style="list-style-type: decimal">
<li>RAID 0 improves access bandwidth (parallel read or write), but there is no extra redundancy added to deal with a disk failure.</li>
<li>RAID 1 improves reliability and can tolerate one disk fail, and it doubles read throughput. But 50% of disk space is wasted.</li>
<li><strong>RAID 5 improves access bandwidth while it can tolerate up to 5 disk failures.</strong></li>
<li>RAID 1+0 improves access bandwidth while it can tolerate one disk failure.</li>
</ol>
<p><strong>Q12</strong> Given one data block contains content in binary value 0011, and another block 1011. What is the binary value of the parity block for these two blocks? If the parity block of two blocks A and B is 0011 and block A is 1011, B is lost during a failure. What is B in the binary value that can be recover from the parity block and block A?</p>
<ol style="list-style-type: decimal">
<li>0111, 0111</li>
<li><strong>1000, 1000</strong></li>
<li>0111, 1000</li>
<li>1000, 01111</li>
</ol>
<p><strong>Q13</strong> Considering A and B with initial value 1. Transaction T is: A=3; B=4; commit. Select one of following transaction processing that violates ACID properties.</p>
<ol style="list-style-type: decimal">
<li><strong>T crashes after A=3. The final state after recovery is A=3, B=1.</strong></li>
<li>T crashes after A=3. The final state after recovery is A=1, B=1.</li>
<li>T execution is successful. The final state is A=3, B=4.</li>
</ol>
<p><strong>Q14</strong> Answer true or false for each of the following statements</p>
<ul>
<li><strong>T</strong>. For a key-vale store with multiple replicas, using quorum consensus a writer can return before the data has been written to all replicas.</li>
<li><strong>T</strong>. The directory server as master can be a bottleneck point for a key-value store.</li>
</ul>
<p><strong>Q15</strong> Answer true or false for each of the following statements</p>
<ul>
<li><strong>F</strong>. Hadoop distributed file system allocates 3 replicas by default and accesses a replica for reading randomly.</li>
<li><strong>T</strong>. Hadoop distributed file system is optimized for handling large files with large file blocks and heavy sequential read workload.</li>
</ul>
<h3 id="f12-midterm">F12 midterm</h3>
<p><strong>Q1.1</strong> What are the key OS actions taken during context switching between two processes? What are their process states before and after switching?</p>
<ul>
<li>Suspend the current process.</li>
<li>Save current process control information in a process control block (PCB).</li>
<li>Restore the process control information for the new process from its PCB.</li>
<li>Setup memory mapping (e.g. page table) and activate the execution of the new process.</li>
<li>The PCB information contains registers (including stack pointer, program counter), page table, and other process-oriented OS information.</li>
<li>The state change: Run-&gt;Ready for the old process and Ready-&gt;Run for the new process.</li>
</ul>
<p><strong>Q1.2</strong> Given the following c program that uses Linux <code>fork()/wait()</code> system calls</p>
<pre><code>int main() {
    int pid, x=10, y;
    pid = fork();
    if (pid == 0) {
        y = 1;
        x = x + y;
        printf(&quot;x is %d. y is %d \n&quot;, x,y);
    } else {
        y = 2;
        x = x + y;
        wait(NULL);
        printf(&quot;x is %d. y is %d \n&quot;, x,y);
    }
}</code></pre>
<pre><code>x is 11, y is 1
x is 12, y is 2</code></pre>
<p><strong>Q2.1</strong> What is the main dierence between a process and a thread?</p>
<p>Threads allocate separate stack space within the same process resource, share memory for code/data segments. Processes don't share memory normally. Separate resource (address space) is allocated from the system for each process. Each process is recognized by OS during resource allocation and scheduling while a thread created within a process may or may not be recognized by the OS during scheduling.</p>
<p><strong>Q2.2</strong> Explain what Nachos OS does in the following four statements during Nachos thread forking.</p>
<pre><code>Thread::Fork(VoidFunctionPtr func, int arg) {
    StackAllocate(func, arg);
    IntStatus oldLevel = interrupt-&gt;SetLevel(IntOff);
    scheduler-&gt;ReadyToRun(this);
    (void) interrupt-&gt;SetLevel(oldLevel);
}</code></pre>
<ul>
<li>Allocate stack space and setup thread control block properly used for thread context switch (e.g. setup stack pointer, the thread starting address, and argument location).</li>
<li>Disable interruption.</li>
<li>Put this thread into a queue with a ready state Enable interruption.</li>
<li>Enable interruption.</li>
</ul>
<p><strong>Q3.1</strong> What is a race condition?</p>
<p>Multiple threads or processes may operate on the same shared data at the same time; the result depends on their data access time and execution order.</p>
<p><strong>Q3.2</strong> For Project 1, if we change <code>ThreadTest()</code> in file <code>threadtest.cc</code> as follows:</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">int</span> x = <span class="dv">10</span>;
<span class="dt">void</span> SimpleThread(<span class="dt">int</span> y) {
    currentThread-&gt;Yield();
    x = x + y;
    printf(<span class="st">&quot;x is </span><span class="sc">%d</span><span class="st">. y is </span><span class="sc">%d</span><span class="st"> </span><span class="sc">\n</span><span class="st">&quot;</span>, x, y);
}
<span class="dt">void</span> ThreadTest() {
    Thread *t = <span class="kw">new</span> Thread(<span class="st">&quot;forked thread&quot;</span>);
    t-&gt;Fork(SimpleThread, <span class="dv">1</span>);
    SimpleThread(<span class="dv">2</span>);
}</code></pre></div>
<p>In this case, Nachos threads only yield to each other when executing <code>Yield()</code>. List and explain the values of &quot;x&quot; and &quot;y&quot; printed in executing <code>ThreadTest()</code>.</p>
<pre><code>x = 12, y = 2
x = 13, y = 1</code></pre>
<p>The parent thread executes <code>SimpleTread(2)</code> first, but yields. The child thread starts <code>SimpleTread(1)</code>, but yields. Then parent thread continues and prints. After its completion, the child thread resumes and prints.</p>
<p><strong>Q4.1</strong> What is the benet of TLB for a paging scheme?</p>
<p>Speed up looking up of address translation using a fast hardware cache.</p>
<p><strong>Q4.2</strong> Given a one-level paging scheme for memory address mapping, if TLB access takes 10 nanosecond and each physical memory access takes 100 nanoseconds, what will be the desired TLB hit ratio to have the eective memory access time within 120 nanoseconds to complete both logical address translation and physical memory data fetching?</p>
<p>Solve <span class="math inline">\(10 + (1-x)*100 + 100 = 120\)</span> for <span class="math inline">\(x = 0.9\)</span>.</p>
<p><strong>Q5.1</strong> Given a one-level paging scheme where physical memory is divided into a set of pages with uniform size 8K bytes and each page table entry uses 4 bytes, what is the maximum size of a logical memory space for each process with this paging scheme? Explain your reason.</p>
<p>The page table is also 8K bytes, each entry uses 4 bytes, thus 8K/4 = 2K entries in a page table; thus there are total of 2K pages, the maximum size of memory space is 2K*8KB = 16MB.</p>
<p><strong>Q5.2</strong> Design a 2-level paging scheme for a 38-bit logical space and illustrate the address translation steps. Each physical page is of size 16K bytes and each page table entry uses 4 bytes.</p>
<p>(12, 12, 14)</p>
<ul>
<li>Physical page size is 16K bytes, and offset for a page use log(16K) = 14 bits.</li>
<li>Each physical page can host 16K/4 = 4K entries. Thus a page table using one physical page can map at most 4Kx16K=64MB physical space. A two-level table can map at most 64MB*4K=256GB space which is 2^38.</li>
<li>The level-1 outer table has 4K entries, and log(4K)=12. That needs 12 bits to identify a particular entry. Thus the outer table part of the logical address needs 12 bits.</li>
<li>Consequently the level-2 inner table part of logical address needs 12 bits. The scheme is 2-level paging and 38 bits will be split as 12, 12, and 14.</li>
</ul>
<p><strong>Q6</strong> The following pseudo code for a producer-consumer problem needs a synchronization.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="co">// Consumer thread</span>
<span class="cf">while</span> (<span class="dv">1</span>) { <span class="co">/* consume next data item */</span> }
<span class="co">// Producer thread</span>
<span class="cf">while</span> (<span class="dv">1</span>) { <span class="co">/* produce next data item */</span> }</code></pre></div>
<p>Extend the above code and use condition variables and a lock to synchronize the critical section and avoid busy waiting so that a producer waits if there is no buer space to store data and a consumer waits if there is no data to consume.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="co">// Consumer thread</span>
<span class="cf">while</span>(<span class="dv">1</span>){
    lock-&gt;acquire();
    <span class="cf">while</span> (queue is empty) {
        condData-&gt;wait(lock)
    }
    Consume next data item;
    condSpace-&gt;signal(lock);
    lock-&gt;release();
}
<span class="co">// Producer thread</span>
<span class="cf">while</span> (<span class="dv">1</span>) {
    lock-&gt;acquire();
    <span class="cf">while</span> (queue is full) {
        condSpace-&gt;wait(lock)
    }
    Produce next data item;
    condData-&gt;signal(lock);
    lock-&gt;release();
}</code></pre></div>
<h3 id="f12-final">F12 final</h3>
<p><strong>Q1.1</strong> What is the benefit of incorporating the copy-on-write feature in process forking?</p>
<p>Delay page duplication when creating a child process until this page is modified by the child process. If no pages are modied in the child process, and then cost of duplication can be eliminated.</p>
<p><strong>Q1.2</strong> Given the following C program that uses Linux <code>fork()/wait()</code> system calls, Assume <code>printf()</code> is an atomic operation. Show all possible results printed when running a multiprogramming Linux system.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">int</span> main() {
    <span class="dt">int</span> pid, x = <span class="dv">100</span>, y = <span class="dv">1</span>;
    pid = fork();
    <span class="cf">if</span> (pid == <span class="dv">0</span>) {
        x = x + <span class="dv">1</span>;
        y = y + x;
        printf(<span class="st">&quot;x is </span><span class="sc">%d</span><span class="st">. y is </span><span class="sc">%d</span><span class="st"> </span><span class="sc">\n</span><span class="st">&quot;</span>, x,y);
    } <span class="cf">else</span> {
        x = x + <span class="dv">2</span>;
        y = y + x;
        printf(<span class="st">&quot;x is </span><span class="sc">%d</span><span class="st">. y is </span><span class="sc">%d</span><span class="st"> </span><span class="sc">\n</span><span class="st">&quot;</span>, x,y);
    }
}</code></pre></div>
<pre><code>x is 101. y is 102
x is 102. y is 103

x is 102. y is 103
x is 101. y is 102</code></pre>
<p><strong>Q2.1</strong> Explain the advantages and disadvantages of Shortest-Job-First (SJF) scheduling. Explain why the Multi-Level-Feedback-Queue can be considered as an approximation to SJF while addressing the weakness of SJF?</p>
<ul>
<li>Advantage of SJF: reduce aveage job waiting time.</li>
<li>Disadvantage of SJF: there could a starvation for some long jobs.</li>
<li>In multi-queue scheduling, short jobs can be assigned in one queue with more resource. Long jobs can be assigned to another queue. Allocating resource proportionaly among queues avoids starvation.</li>
</ul>
<p><strong>Q2.2</strong> Consider the following set of processes, with the length of the CPU-burst time given in milliseconds. The processes are assumed to have arrived in the order P1,P2,P3,P4,P5, all at time 0. Draw 3 charts illustrating the execution of these processes using FCFS (First-come-rst-serve), SJF (shortest-job-rst), and RR (round-robing with quantum=1) scheduling. What is the waiting time and turnaround time of process P1 for each of the scheduling algorithms?</p>
<ul>
<li>P1, arrival time = 0, burst time = 10</li>
<li>P2, arrival time = 0, burst time = 1</li>
<li>P3, arrival time = 0, burst time = 2</li>
<li>P4, arrival time = 0, burst time = 1</li>
<li>P5, arrival time = 0, burst time = 5</li>
<li>FCFS: P1 P2 P3 P4 P5, turnaround = 10, waiting = 0</li>
<li>SJF: P2 P4 P3 P5 P1, turnaround = 19, waiting = 9</li>
<li>RR: P1 P2 P3 P4 P5 P1 P3 P5 P1 P5 P1 P5 P1 P5 P1, turnaround = 19, waiting = 9</li>
</ul>
<p><strong>Q3.1</strong> What is the role of dirty-bits in Nachos virtual memory implementation in Project 3? How does this help in improving execution performance.</p>
<p>Dirty bits indicate if a memory page has been modied. It can be used to check if a page should be written back the swapping store or not when a page is evicted from memory. It can save disk I/O and avoid unncessary writes.</p>
<p><strong>Q3.2</strong> A demand-paging memory system translates an address using a TLB and a one-level page table in the main memory. The TLB hit ratio is 99% and it takes 0 nanosecond for TLB access. Each physical memory access takes 100 nanoseconds. If this virtual page is not in memory, then page fault handling takes 10,000 microseconds. What will be the maximum page fault rate that makes the overall average effective memory access time to be less than 1 microsecond?</p>
<p>If there is no page fault, average memory access time in ms is <span class="math inline">\(0.99*(0+0.1) + 0.01*(0.1+0.1) = 0.101\)</span>. Then solve <span class="math inline">\((1-f)*0.101 + f(0.101+10000) = 1\)</span> for <span class="math inline">\(f = 0.008999%\)</span>.</p>
<p><strong>Q3.3</strong> Given 3 empty physical pages, draw gures to illustrate how a page replacement scheme deals with the following reference string (1, 2, 3, 4, 2, 5, 2) using the LRU strategy. Compute the number of page faults.</p>
<p>Five faults: 1F, 2F, 3F, 4F(replace 1), 2, 5F(replace 3), 2.</p>
<p><strong>Q3.4</strong> Given the following C program which access a 2D integer array <code>a[1024][1024]</code>. A demand-paging memory is divided into a set of uniform 1K byte pages. Each integer uses 4 bytes. Assume that the binary instructions for the following program are always in separate memory space. The LRU page replacement algorithm is used to manage a 512KB memory that stores the array. Initially data array <code>a[][]</code> is not in memory. Derive and explain the number of page faults in running this program. If the memory increases to 1MB, what is the number of page faults in running this program?</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="cf">for</span> (i = <span class="dv">0</span>; i &lt; <span class="dv">1024</span>; i++) <span class="cf">for</span> (j = <span class="dv">0</span>; j &lt; <span class="dv">1024</span>; j++) a[j][i] = <span class="dv">100</span>;</code></pre></div>
<p>The C program stores this array in a row-wise format. Each row needs 1024*4/1K = 4 memory pages. 512KB memory means 512 pages. The code accesses column 1, requiring the loading of 1K pages with 1K page faults. For column 2, old data loaded is swapped out, and thus it takes another 1K page faults.</p>
<p>In summary, 1024x1024 page faults occur.</p>
<p>If memory is 1MB, then memory can hold 1K pages. After column 1 access, data in memory can be used for column 2 access. Thus total faults will be 1024*4 = 4K.</p>
<p><strong>Q4.1</strong> What are advantages and disadvantages of RAID 0 and RAID 1?</p>
<p>RAID 0 improves access bandwidth (paralell read or write), but there is no extra redudancy added to deal with a disk failure. RAID 1 improves reliability and can torelate one disk fail, and it doubles read throughput. But 50% of disk space is wasted.</p>
<p><strong>Q4.2</strong> The original Nachos file system implementation (before Project 3 extension) uses a single-level indexed allocation. Given a file currently consisting of 1 disk block, how many disk block read or write operations are required to add a block to the end of this file?</p>
<p>Total 2 operations. 1 IO operation to load the file header. Then the kernel modifies the file header to add one block. Then it does 1 IO operation to store the added data block. (If they answer 3, then the third has to be writing the index header to the disk. Then it is ne).</p>
<p><strong>Q4.3</strong> We use indexed allocation to store a file on a disk and we assume that each disk block is of size 16K bytes. Each entry in an index block takes 4 bytes. What is the maximum size of a file that can be mapped by a single-level indexed allocation? Illustrate with a gure to show the levels of index indirection required to map a file of size 16 gigabytes?</p>
<ul>
<li>16K/4=4K entries per index block.</li>
<li>The maximum size of a le with single indirect block = 4K x 16K bytes = 64MB.</li>
<li>Maximum size for two levels: 4K x 4K x 16K = 256GB.</li>
<li>16GB would require 2 levels.</li>
</ul>
<p><strong>Q6.1</strong> Suppose that a disk drive has 200 cylinders, numbered from 0 to 199. The drive is currently serving a request at cylinder 120, and the previous request was at cylinder 100. The queue of pending requests, in FIFO order is: 80, 190, 40, 110. Illustrate the disk arm movement using the FCFS and SCAN algorithms. Compute the total movement in cylinders for each algorithm.</p>
<ul>
<li>FCFS: 120, 80, 190, 40, 110. In total 370.</li>
<li>SCAN: 120, 190, 110, 80, 40. In total 220.</li>
</ul>
<p><strong>Q6.2</strong> Suppose this disk drive spins at 12000 RPM, and has a sector size of 512 bytes, and holds 50 sectors per track. What is sustained transfer rate of this drive in megabytes per second? Assume average seek time for the above drive is 7.3 milliseconds, what is the eective transfer rate for random-access of two consecutive sectors on a disk track? (Namely nd the position rst, then transfer two sectors together)</p>
<ul>
<li>Each second spins 12000/60 = 200 times.</li>
<li>Each spin transfers a track of 25 KB (50 sectors x 0.5 KB).</li>
<li>Sustained average transfer rate is 200 x 25 KB = 5 MB/s.</li>
<li>Average rotational cost is time to travel track: 1/100 x 0.5 = 2.5 ms.</li>
<li>Transfer time is 7.3 ms to seek + 2.5 ms rotational latency + 0.2 ms (reading two sectors takes 0.001 MB / 5 MB) = 10 ms.</li>
<li>Effective transferring rate: 1KB / 0.01s = 0.1 MB/s.</li>
</ul>
<p><strong>Q7</strong> The following pseudo code is executed by each thread and the code uses <code>Swap()</code> atomic instruction to implement a critical section solution to synchronize threads.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp">ThreadFunction() {
    <span class="dt">int</span> key = TRUE;
    <span class="dt">int</span> lock = TRUE;
    <span class="cf">while</span> (key == TRUE)
        Swap (&amp;lock, &amp;key);
    <span class="co">// Perform critical section code</span>
    lock = FALSE;
}</code></pre></div>
<p><strong>Q7.1</strong> Should variables <code>lock</code> and <code>key</code> be shared among threads? Correct the above code and initialize variables properly.</p>
<p>lock is shared, key is not.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">int</span> lock = FALSE;
ThreadFunction() {
    <span class="dt">int</span> key = TRUE;
    <span class="cf">while</span> (key == TRUE)
        Swap (&amp;lock, &amp;key);
    <span class="co">// Perform critical section code</span>
    lock = FALSE;
}</code></pre></div>
<p><strong>Q7.2</strong> Does the above corrected solution satisfy each of the following three properties: mutual exclusion, progress, bounded waiting?</p>
<p>Mutual exclusion: yes. Progress: yes. Bounded waiting: no.</p>
<p><strong>Q7.3</strong> Chapter 5 of the text book discussed a solution for the readers-writers problem. In this problem, a data set is shared among a number of threads. A synchronization solution is required so that multiple readers can read at the same time. But when one writer is accessing a shared data item, other writers or readers cannot access this shared item.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp">Lock *w = <span class="kw">new</span> Lock(<span class="st">&quot;Write-Lock&quot;</span>);

<span class="co">// Writer thread:</span>
<span class="cf">while</span>(<span class="dv">1</span>) {
  w-&gt;Acquire();
  <span class="co">// Writing is performed</span>
  w-&gt;Release();
}

<span class="co">// Reader thread:</span>
<span class="cf">while</span>(<span class="dv">1</span>) {
  w-&gt;Acquire();
  <span class="co">// Reading is performed</span>
  w-&gt;Release();
}</code></pre></div>
<p>Explain if the following solution meets the requirement? If not, explain and provide a solution with pseudo code.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp">Semaphore mutex initialized to <span class="dv">1</span> (they may use a lock)
Semaphore wrt initialized to <span class="dv">1</span> (it is wrong to use a lock <span class="cf">for</span> wrt)
Integer readcount initialized to <span class="dv">0</span>

<span class="co">// Writer thread</span>
<span class="cf">while</span>(<span class="dv">1</span>) {
  wrt-&gt;P();
  <span class="co">// Writing is performed</span>
  wrt-&gt;V();
}

<span class="co">// Reader thread</span>
<span class="cf">while</span>(<span class="dv">1</span>) {
  mutex.P();
  readcount++;
  <span class="cf">if</span> (readcount == <span class="dv">1</span>) wrt.P();
  mutex.V()
  <span class="co">// reading is performed</span>
  mutex.P();
  readcount--;
  <span class="cf">if</span> (readcount == <span class="dv">0</span>) wrt.V();
  mutex.V();
}</code></pre></div>
<p>Extend this solution and allow at most 10 readers that can read at the same time.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp">Semaphore mutex initialized to <span class="dv">1</span>
Semaphore wrt initialized to <span class="dv">1</span>
Semaphore rd initialized to <span class="dv">10</span>
Integer readcount initialized to <span class="dv">0</span>

<span class="co">// Writer thread</span>
<span class="cf">while</span>(<span class="dv">1</span>) {
    wrt-&gt;P();
    <span class="co">// Writing is performed</span>
    wrt-&gt;V();
}

<span class="co">// Reader thread</span>
<span class="cf">while</span>(<span class="dv">1</span>) {
    rd.P();
    mutex.P();
    readcount++;
    <span class="cf">if</span> (readcount == <span class="dv">1</span>) wrt.P();
    mutex.V()
    <span class="co">// reading is performed</span>
    mutex.P();
    readcount--;
    <span class="cf">if</span> (readcount == <span class="dv">0</span>) wrt.V();
    mutex.V();
    rd.V();
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp">Lock mutex
Semaphore wrt initialized to <span class="dv">1</span>
Integer readcount initialized to <span class="dv">0</span>
Cond as condition variable

<span class="co">// Writer thread</span>
<span class="cf">while</span>(<span class="dv">1</span>) {
    wrt-&gt;P();
    <span class="co">// Writing is performed</span>
    wrt-&gt;V();
}

<span class="co">// Reader thread</span>
<span class="cf">while</span>(<span class="dv">1</span>) {
    mutex.Acquire();
    <span class="cf">while</span> (readcount &gt;= <span class="dv">10</span>) Cond-&gt;Wait(mutex);
    readcount++;
    <span class="cf">if</span> (readcount == <span class="dv">1</span>) wrt.P();
    mutex.Relase();
    <span class="co">// reading is performed</span>
    mutex.Acquire();
    readcount--;
    Cond-&gt;Signal(mutex);
    <span class="cf">if</span> (readcount == <span class="dv">0</span>) wrt.V();
    mutex.Relase();
}</code></pre></div>
<h2 id="code">Code</h2>
<h3 id="rwlock-implementation">RWLock implementation</h3>
<pre><code>//----------------------------------------------------------------------
// Semaphore::Semaphore
//  Initialize a semaphore, so that it can be used for synchronization.
//
//  &quot;debugName&quot; is an arbitrary name, useful for debugging.
//  &quot;initialValue&quot; is the initial value of the semaphore.
//----------------------------------------------------------------------

Semaphore::Semaphore(char* debugName, int initialValue)
{
    name = debugName;
    value = initialValue;
    queue = new List;
}

//----------------------------------------------------------------------
// Semaphore::~Semaphore
//  De-allocate semaphore, when no longer needed.  Assume no one
//  is still waiting on the semaphore!
//----------------------------------------------------------------------

Semaphore::~Semaphore()
{
    delete queue;
}

//----------------------------------------------------------------------
// Semaphore::P
//  Wait until semaphore value &gt; 0, then decrement.  Checking the
//  value and decrementing must be done atomically, so we
//  need to disable interrupts before checking the value.
//
//  Note that Thread::Sleep assumes that interrupts are disabled
//  when it is called.
//----------------------------------------------------------------------

void
Semaphore::P()
{
    IntStatus oldLevel = interrupt-&gt;SetLevel(IntOff);   // disable interrupts

    while (value == 0) {            // semaphore not available
        queue-&gt;Append((void *)currentThread);   // so go to sleep
        currentThread-&gt;Sleep();
    }
    value--;                    // semaphore available, consume its value

    (void) interrupt-&gt;SetLevel(oldLevel);   // re-enable interrupts
}

//----------------------------------------------------------------------
// Semaphore::V
//  Increment semaphore value, waking up a waiter if necessary.
//  As with P(), this operation must be atomic, so we need to disable
//  interrupts.  Scheduler::ReadyToRun() assumes that threads
//  are disabled when it is called.
//----------------------------------------------------------------------

void
Semaphore::V()
{
    Thread *thread;
    IntStatus oldLevel = interrupt-&gt;SetLevel(IntOff);

    thread = (Thread *)queue-&gt;Remove();
    if (thread != NULL)    // make thread ready, consuming the V immediately
        scheduler-&gt;ReadyToRun(thread);
    value++;
    (void) interrupt-&gt;SetLevel(oldLevel);
}

//----------------------------------------------------------------------
// Lock::Lock
//  Initialize a lock, so that it can be used for synchronization.
//  A lock is very much like a semaphore with initial value 1.
//p
//  &quot;debugName&quot; is an arbitrary name, useful for debugging.
//----------------------------------------------------------------------

Lock::Lock(char* debugName)
{
    name = debugName;
    value = 1;
    queue = new List;
}

//---------------------------------------------------------------------
// Lock::~Lock
//   Dedestuct a lock, when no longer needed. Assume no one is still
//   holding a lock!
//---------------------------------------------------------------------
Lock::~Lock()
{
    delete queue;
}

//---------------------------------------------------------------------
// Lock::Acquire
//   This function waits for a lock to become free and then acquires
//   the lock for the current thread.
//---------------------------------------------------------------------
void Lock::Acquire()
{
    IntStatus oldLevel = interrupt-&gt;SetLevel(IntOff);
    // if the lock is acquired by someone else, put the current thread
    // to the waiting queue and sleep
    while (value == 0) {
        queue-&gt;Append((void*)currentThread);
        currentThread-&gt;Sleep();
    }
    // acquire the lock
    value = 0;
    owner = currentThread;
    (void)interrupt-&gt;SetLevel(oldLevel);
}

//---------------------------------------------------------------------
// Lock::Release
//   This function releases a lock that was previously acquired by the
//   current thread, and wakes up one of the threads waiting for the
//   lock.
//---------------------------------------------------------------------
void Lock::Release()
{
    ASSERT(isHeldByCurrentThread());
    IntStatus oldLevel = interrupt-&gt;SetLevel(IntOff);
    // release the lock
    value = 1;
    owner = NULL;
    // put a thread in the waiting queue (if any) to the ready queue
    Thread *thread = (Thread*)queue-&gt;Remove();
    if (thread != NULL)
        scheduler-&gt;ReadyToRun(thread);
    (void)interrupt-&gt;SetLevel(oldLevel);
}

//---------------------------------------------------------------------
// Lock::isHeldByCurrentThread
//   True if the current thread holds this lock. It is useful for
//   checking in Release, and in condition variable ops below.
//---------------------------------------------------------------------
bool Lock::isHeldByCurrentThread()
{
    return value == 0 &amp;&amp; currentThread == owner;
}


//---------------------------------------------------------------------
// Condition::Condition
//   Initializes a condition object.
//---------------------------------------------------------------------
Condition::Condition(char* debugName)
{
    name = debugName;
    queue = new List;
}

//---------------------------------------------------------------------
// Condition::~Condition
//   Destruct a condition object, when it is no longer needed.
//---------------------------------------------------------------------
Condition::~Condition()
{
    delete queue;
}

//---------------------------------------------------------------------
// Condition::Wait
//   Waits for a condition to become free and then acquires the
//   condition for the current thread.
//---------------------------------------------------------------------
void Condition::Wait(Lock* conditionLock)
{
    ASSERT(conditionLock-&gt;isHeldByCurrentThread());

    IntStatus oldLevel = interrupt-&gt;SetLevel(IntOff);
    conditionLock-&gt;Release();
    queue-&gt;Append((void*)currentThread);
    currentThread-&gt;Sleep();
    conditionLock-&gt;Acquire();
    (void)interrupt-&gt;SetLevel(oldLevel);
}

//---------------------------------------------------------------------
// Condition::Signal
//   Wakes up one of the threads that is waiting on the condition.
//---------------------------------------------------------------------
void Condition::Signal(Lock* conditionLock)
{
    ASSERT(conditionLock-&gt;isHeldByCurrentThread());

    Thread *thread = (Thread*)queue-&gt;Remove();
    if (thread != NULL)
        scheduler-&gt;ReadyToRun(thread);
}

//---------------------------------------------------------------------
// Condition::Broadcast
//   Wakes up all threads that are waiting for the condition.
//---------------------------------------------------------------------
void Condition::Broadcast(Lock* conditionLock)
{
    ASSERT(conditionLock-&gt;isHeldByCurrentThread());

    Thread *thread;
    while ((thread = (Thread*)queue-&gt;Remove()))
        scheduler-&gt;ReadyToRun(thread);
}</code></pre>
<h3 id="rwlock-usage">RWLock usage</h3>
<pre><code>#include &quot;rwlock.h&quot;
#pragma GCC diagnostic ignored &quot;-Wwrite-strings&quot;

RWLock::RWLock() {
#ifdef P1_SEMAPHORE
    read_lock = new Semaphore(&quot;read lock&quot;, 1);
    write_lock = new Semaphore(&quot;write lock&quot;, 1);
    blocking_writer = 0;
#endif

#ifdef P1_LOCK
    read_lock = new Lock(&quot;read lock&quot;);
    write_lock = new Lock(&quot;write lock&quot;);
    blocking_writer = 0;
#endif

#ifdef P1_RWLOCK
    active_writer = 0;
    active_reader = 0;
    waiting_writer = 0;
    waiting_reader = 0;
    mutex = new Lock(&quot;mutex&quot;);
    ok_to_read = new Condition(&quot;read cv&quot;);
    ok_to_write = new Condition(&quot;write cv&quot;);
#endif
}

RWLock::~RWLock() {
#ifdef P1_SEMAPHORE
    delete read_lock;
    delete write_lock;
#endif

#ifdef P1_LOCK
    delete read_lock;
    delete write_lock;
#endif

#ifdef P1_RWLOCK
    delete mutex;
    delete ok_to_read;
    delete ok_to_write;
#endif
}

void RWLock::startWrite() {
#ifdef P1_SEMAPHORE
    write_lock-&gt;P();
#endif

#ifdef P1_LOCK
    write_lock-&gt;Acquire();
#endif

#ifdef P1_RWLOCK
    mutex-&gt;Acquire();
    while (active_writer + active_reader &gt; 0) {
        waiting_writer++;
        ok_to_write-&gt;Wait(mutex);
        waiting_writer--;
    }
    active_writer++;
    mutex-&gt;Release();
#endif
}

void RWLock::doneWrite() {
#ifdef P1_SEMAPHORE
    write_lock-&gt;V();
#endif

#ifdef P1_LOCK
    write_lock-&gt;Release();
#endif

#ifdef P1_RWLOCK
    mutex-&gt;Acquire();
    active_writer--;
    if (waiting_writer &gt; 0)
        ok_to_write-&gt;Signal(mutex);
    else
        ok_to_read-&gt;Broadcast(mutex);
    mutex-&gt;Release();
#endif
}

void RWLock::startRead() {
#ifdef P1_SEMAPHORE
    read_lock-&gt;P();
    blocking_writer++;
    // first reader locks write_lock
    if (blocking_writer == 1)
        write_lock-&gt;P();
#endif

#ifdef P1_LOCK
    read_lock-&gt;Acquire();
    blocking_writer++;
    // first reader locks write_lock
    if (blocking_writer == 1)
      write_lock-&gt;Acquire();
#endif

#ifdef P1_RWLOCK
    mutex-&gt;Acquire();
    while (active_writer &gt; 0 || waiting_writer &gt; 0) {
        waiting_reader++;
        ok_to_read-&gt;Wait(mutex);
        waiting_reader--;
    }
    active_reader++;
    mutex-&gt;Release();
#endif
}

void RWLock::doneRead() {
#ifdef P1_SEMAPHORE
    blocking_writer--;
    // last reader unlocks write_lock
    if (blocking_writer == 0)
        write_lock-&gt;V();
    read_lock-&gt;V();
#endif

#ifdef P1_LOCK
    blocking_writer--;
    // last reader unlocks write_lock
    if (blocking_writer == 0)
      write_lock-&gt;Release();
    read_lock-&gt;Release();
#endif

#ifdef P1_RWLOCK
    mutex-&gt;Acquire();
    active_reader--;
    if (active_reader == 0 &amp;&amp; waiting_writer &gt; 0)
        ok_to_write-&gt;Signal(mutex);
    mutex-&gt;Release();
#endif
}</code></pre>
<h2 id="project-readme">Project README</h2>
<h3 id="project-2-multiprogramming">Project 2 Multiprogramming</h3>
<h4 id="first-part">First part</h4>
<p>In the first part of this assignment, you are to implement the <code>Fork()</code>, <code>Yield()</code>, <code>Exit()</code>, <code>Exec()</code>, and <code>Join()</code> system calls that act as follows:</p>
<ul>
<li>The <code>Fork(func)</code> system call creates a new user-level (child) process, whose address space starts out as an exact copy of that of the caller (the parent), but immediately the child abandons the program of the parent and starts executing the function supplied by the single argument. Notice this definition is different from the one in the <code>syscall.h</code> file in Nachos.</li>
<li>The <code>Yield()</code> call is used by a process executing in user mode to temporarily relinquish the CPU to another process.</li>
<li>The <code>Exit(int)</code> call takes a single argument, which is an integer status value as in Unix. The currently executing process is terminated and returns the exit code.</li>
<li>The <code>Exec(filename)</code> system call spawns a new user-level process with a new address space that begins executing a new program given by the object code in the Nachos file whose name is supplied as an argument to the call. It should return to the parent a <code>SpaceId</code> which can be used to uniquely identify the newly created process. <code>syscall.h</code> defines 4 arguments and please ignore the other 3 arguments.</li>
<li>The <code>Join()</code> call waits and returns only after a process with the specified ID (supplied as an argument to that call) has finished. It collects and returns the exit code from the targeted process.</li>
</ul>
<h4 id="second-part">Second part</h4>
<p>In the second part of this assignment, you are to implement the file system calls: <code>Creat()</code>, <code>Open()</code>, <code>Read()</code>, <code>Write()</code>, and <code>Close()</code>. The semantics of these calls are specified in <code>syscall.h</code>. You should extend your file system implementations to handle the console as well as normal files.</p>
<h4 id="issues-to-consider">Issues to consider</h4>
<p>Here is an outline of the the major issues you will have to deal with to make Nachos into a multiprogrammed system:</p>
<p>In order to handle the various system calls, you will need to modify the <code>ExceptionHandler()</code> function in <code>exception.cc</code> to determine which system call or exception occurred, and to transfer control to an appropriate function. You might want to consider introducing &quot;stubs&quot; (functions with empty bodies or with debugging printout so you can tell when they are called) for all the system calls right away, and then postpone their actual implementation until a bit later. This strategy will help you understand better how control is transferred from user mode to system mode when a system call is executed.</p>
<p>The Nachos code you have been given is extremely simple-minded about memory management. In particular, the constructor function <code>AddrSpace::AddrSpace()</code> simply determines the amount of memory that will be required by the application to be run and then allocates that much space contiguously starting at address zero in physical memory. The page tables (which control the address translation hardware) are set up so that the logical addresses (what the user program sees) are identical to the physical addresses (where the data is actually stored).</p>
<p>The above scheme is inadequate for running more than one application at a time. You will need to design and implement a scheme for allocating and freeing physical memory, and you will need to arrange to set up the page tables so that the logical address space seen by a user application is a contiguous region starting from address zero, even though the data is stored at different physical addresses. You will want to implement a memory management scheme that is flexible enough to extend to virtual memory later in the semester. We suggest implementing a C++ class with methods for allocating and freeing physical memory one page at a time. By setting up the page tables properly, you can give the user application a contiguous logical address space even though each page of actual data might be stored anywhere in physical memory.</p>
<p>The <code>Fork()</code> system call is the most difficult part of this assignment. It is different from the system call <code>Exec</code> in that <code>Fork</code> will start a new process that runs a user function specified by the argument of the call, while <code>Exec</code> will start a process that runs a different executable file. The parameter types for <code>Fork()</code> and <code>Exec()</code> also differ. <code>Fork(func)</code> takes an argument func which is a pointer to a function. The function must be compiled as part of the user program that is currently running. By making this system call <code>Fork(func)</code>, the user program expects the following: a new thread will be generated for use by the user program; and this thread will run func in an address space that is an exact copy of the current one. This implementation of Fork makes it possible to have and to access multiple entry points in an executable file.</p>
<p>To make the system call <code>Fork(func)</code> work for the user program, you will need to know how to find the entry point of the function that is passed as the parameter. The parameter convention is determined by the cross-compiler which produces executable code from the user source program. Look at the file <code>exception.cc</code> to see that this entry point, which is an address in the executable code's address space, is already loaded into register 4 when the trap to the exception handler occurs. All you need to do is to insert code into the exception handler (or call a new function of your own) which does the following: set up an address space which is a copy of the address space of the current thread, and load the address that is in register 4 into the program counter. After these steps, use <code>Thread::Fork()</code> to create a new thread, initialize the MIPS registers for the new process, and have both the new and old processes return to user mode. The parent should return to user mode by returning from the exception handler, the child process should continue to run from the address that is now in the program counter, which is the entry point of the function. To implement Fork, you will need to introduce modifications to the AddrSpace class in addrspace.cc so that you can make a `<code>clone'' of a running user application program. We suggest adding a function that will create a new address space as an exact copy of the original. You will have to allocate additional physical memory for this copy, set up the page tables properly for the new address space, and copy the data from the old address space to the new. Once the physical memory has been allocated and the page tables set up, you will use</code>Thread::Fork()<code>to create a new kernel thread, initialize the MIPS registers for the new process, and then have both the old and the new processes return to user mode. The child process should continue by finishing the</code>Fork()<code>system call. The parent should return to user mode merely by returning from the</code>ExceptionHandler()` function.</p>
<p>The <code>Exit()</code> system call should work by calling <code>Thread::Finish()</code>, but only after deallocating any physical memory and other resources that are assigned to the thread that is exiting.</p>
<p>In order to implement the <code>Exec()</code> system call, you will need a method for transferring data (the name of the executable, supplied as the argument to the system call) between the user address space and the kernel. You are not to use functions <code>Machine::ReadMem()</code> and <code>Machine::WriteMem()</code> in <code>machine/translate.cc</code>. Instead, you will have to code your own functions that take into account the address translations described by the page tables to locate the proper physical address for any given logical address. (Recall that strings in C are stored as sequences of characters in successive memory locations, terminated by a null character.)</p>
<p>Once the name of the executable has been copied into the kernel, and the file has been verified to exist, the executable file should be consulted to determine the amount of physical memory required for the new program. This physical memory should be allocated and initialized with data from the executable file, the page tables thread should be adjusted for the new program, the MIPS registers should be reinitialized for starting at the beginning of the new program, and control should return to user mode. File progtest.cc contains a sample for executing a binary program.</p>
<p>If you use <code>machine-&gt;Run</code> to execute a user program, it terminates the current thread. Since <code>Exec()</code> needs to return a space ID to the caller, you should find a way to do that.</p>
<p>NOTE: The object code produced by the MIPS cross-compiler assumes that the data segment begins at the physical address immediately following the text segment. In particular, there is no page alignment, so that if the text segment ends in the middle of a page, then the data segment will start just after it and the page will contain both code and data.</p>
<p><code>Yield()</code> system call will call <code>Thread::Yield()</code> after making sure to save any necessary state information about the currently executing process.</p>
<p>Be sure to synchronize your code correctly. You will need to put lock operations in your code to ensure that it will work properly. Your locking should be fine grained enough to eliminate any spurious latency problems caused by coarse grained locking. For example, any time a thread accesses the disk or the console it should not hold a lock that would prevent another thread from accessing some other I/O device or piece of data.</p>
<p>You should buffer user file reads in a disk buffer called diskBuffer (defined in <code>system.cc</code>). All of your user-level file I/O must go through the <code>diskBuffer</code>.</p>
<p>You will need to solve a synchronization problem that occurs when multiple processes try to read or write from a file at the same time.</p>
<h4 id="threads-and-processes">Threads and processes</h4>
<p><code>Thread::Fork()</code> spawns a new kernel thread that uses the same AddrSpace as the thread that spawned it. If that address space is duplicated and not shared, it is no longer a kernel thread but a Forked Process. If that AddrSpace instead contains code and data loaded in from a separate file, it is no longer a forked process, but an executed process. The details of <code>Fork()</code> and <code>Exec()</code> are covered in steps 1) and 7) below.</p>
<h4 id="system-calls">System calls</h4>
<p>In <code>code/userprog/exception.cc</code>, put function calls for each system call. Just print debug statements in these functions for now, so you can see when system calls get executed. The function may need to return or take an argument. Because the argument lies in user space, you will need to transfer it over using machine register reads and writes. A sample stub illustrating this is shown below.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="cf">case</span> SC_Join:
    <span class="dt">int</span> result = myJoin(machine-&gt;ReadRegister(<span class="dv">4</span>));
    machine-&gt;WriteRegister(<span class="dv">2</span>, result);
    <span class="cf">break</span>;</code></pre></div>
<p>If the system calls: <code>Fork()</code>, <code>Yeild()</code>, <code>Exec()</code>, <code>Join()</code> and <code>Exit()</code> are implemented in that order, you will not have to worry about the call you are currently working on depending upon unimplemented calls.</p>
<p>Remember that the last thing the <code>ExceptionHandler</code> function needs to do when executing a system call is increment the program counter. Write a helper function to do this - it needs to update <code>PCreg</code>, <code>NextPCreg</code>, and <code>PrevPCreg</code>. They should all incriment by 32 bits (4 bytes).</p>
<h4 id="steps-to-a-multi-programmed-nachos">10 steps to a multi-programmed Nachos</h4>
<ol style="list-style-type: decimal">
<li><p>Implement <code>Fork()</code>. Fork will create a new kernel thread and set it's AddrSpace to be a duplicate of the CurrentThread's space. It sets then <code>Yields()</code>. The new thread runs a dummy function that will will copy back the machine registers, PC and return registers saved from before the yield was performed. You did save the PC, return and other machine registers didn't you?</p>
<p>Duplicating the AddrSpace requires the implementation of a Memory Manager detailed in steps 2-4. <code>Fork()</code> will not work completely until the completion of step 4. Don't get stuck on step 1), steps 2-4 are much more important.</p></li>
<li><p>Write a Memory Manager that will be used to facilitate contiguous virtual memory. The amount of memory available to the user space is the same as the amount of physical memory, it's not until project 3 that you will have to implement swapping virtual memory.</p>
<p>You will need just two methods at first 1) <code>allocFrame()</code> allocates the first free frame (in physical memory) and 2) <code>freeFrame(int i)</code> takes the index of a frame and marks it free. You can use a <code>Bitmap</code> (in <code>code/userprog/bitmap.*</code>) with one bit per page to track allocation or use your own integer array, which ever you prefer.</p>
<p>Modify <code>AddrSpace</code> (<code>code/userprog/addrspace.*</code>) to use the memory manager. first, modify the page table constructors to use pages allocated by your memory manager for the physical page index. The later modification will come in step 4.</p></li>
<li><p>Write the<code>AddrSpace::Translate</code> function, which converts a virtual address to a physical address. It does so by breaking the virtual address into a page table index and an offset. It then looks up the physical page in the page table entry given by the page table index and obtains the final physical address by combining the physical page address with the page offset. It might help to pass a pointer to the space you would like the physical address to be stored in as a paramter. This will allow the function to return a boolean TRUE or FALSE depending on whether or not the virtual address was valid. If confused, consult the text book on memory management and page table or <code>Machine::Translate()</code> in <code>machine/translate.cc</code>.</p></li>
<li><p>Write the <code>AddrSpace::ReadFile</code> function, which loads the code and data segments into the translated memory, instead of at position 0 like the code in the AddrSpace constructor already does. This is needed not only for <code>Exec()</code> but for the initial startup of the machine when executing any test program with virtual memory.</p>
<p>You should buffer user file reads in a disk buffer called diskBuffer (defined in <code>system.h</code>). All of your user-level file I/O must go through the diskBuffer. Be sure to to under or over run the buffer during the copy. Also be sure not to write too much of the file into memeory. You can use the following prototype for the function.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"> <span class="dt">int</span> AddrSpace::ReadFile(<span class="dt">int</span> virtAddr,
                         OpenFile* file,
                         <span class="dt">int</span> size,
                         <span class="dt">int</span> fileAddr);</code></pre></div>
<p>You will also need to use the functions: <code>File::ReadAt(buff,size,addr)</code> and <code>bcopy(src,dst,num)</code> as well as the memory locations at <code>machine-&gt;mainMemory[physAddr]</code>.</p>
<p>At this point, test programs should work the same as before. That is, the halt program and other system calls will still operate the way they did before you modified AddrSpace. Also <code>Fork()</code> should be working. Test your implementation appropriately.</p></li>
<li><p>Write the PCB and a process manager. Create a PCB class that will store the necessary information about a process. Don't worry about putting everything in it right now, you can always add more as you go along. To start, it should have a <code>PID</code>, <code>parent PID</code>, and <code>Thread*</code>. The process manager should do the same thing as the memory manager - it has <code>getPID</code> and <code>clearPID</code> methods, which return an unused process id and clear a process id respectively. Again, use a bitmap or similar integer array. You'll also need an array of <code>PCB*</code> to store the PCBs. Modify the AddrSpace constructors to include a PCB as an attribute.</p></li>
<li><p>With the process manager implemented you will have to modify <code>progtest.cc</code> so that the first process started by the system is given a PCB. The function <code>StartProcess()</code> is defined in <code>progtest.cc</code> and called from <code>threads/main.cc</code>. It loads and runs the first program (the one you passed in with -x). Modify <code>StartProcess()</code> so that a PCB is created for the first process. This process won't have a parent -- be sure to handle this accordingly.</p></li>
<li><p>Implement <code>Yield()</code>. Given that forked processes are almost the same as kernel threads, this one should be trivial.</p></li>
<li><p>Implement <code>Exec()</code>. Exec is creating a new process (kernel thread) with new code and data segments loaded from the OpenFile object constructed from the filename passed in by the user. In order to get that file name you will have to write a function that copies over the string from user space. This function will start copying memory from the physical address pointed to by the virtual address in <code>machine-&gt;ReadRegister(4)</code>. It should go until it hits a <code>NULL</code> byte.</p>
<p>Fork the new thread to run a dummy function that sets the machine registers straight and runs the machine. The calling thread should yield to give control to the newly spawned thread. At this point you should be able to call test files from other test using <code>Exec(someTestFile)</code> from <code>someOtherTestFile.c</code> (in the <code>test/</code> dir).</p></li>
<li><p>Implement <code>Join()</code>. Join should force the current running thread to wait for some process to finish. The PCB manager can keep track of who is waiting for who using a condition variable for each PCB.</p></li>
<li><p>Implement <code>Exit(int status)</code>. This function should set set the status in the PCB being exited. It should also force any threads waiting on the exiting process to wake up.</p></li>
<li><p>Test your system calls using your own test programs, then the ones in <code>~cs170/nachos-projtest/proj2</code> that don't rely on part 2. If you have time, test using some programs to do crazy or malicious things.</p></li>
</ol>
<h4 id="part-2-implementation-suggestions">Part 2 implementation suggestions</h4>
<p>We will be using Nachos files exclusivly in part 2. This will be good preparation for project 3. Nachos files are similar to Unix files except they are stored on avirtual disk that is implemented as one big Unix file. The interface to <code>Create</code>, <code>Open</code>, <code>Close</code>, <code>WriteAt</code> and <code>ReadAt</code> to those files are defined in <code>filesys/filesys.cc</code>, <code>filesys/openfile.cc</code> and<code>filesys/filehdr.h</code>. You may want to look through those files when getting ready to call these functions for the first time.</p>
<h3 id="steps-to-an-io-enabled-nachos">10 steps to an I/O enabled Nachos</h3>
<ol style="list-style-type: decimal">
<li><p>Create a <code>SysOpenFile</code> object class that contains a pointer to the file system's <code>OpenFile</code> object as well as the systems <code>FileID</code> and <code>fileName</code> for that file and the number of user processes accessing currently it. Declare an array of <code>SysOpenFile</code> objects for use by all system calls implemented in part 2.</p></li>
<li><p>Create a <code>UserOpenFile</code> object class that contains the <code>fileName</code>, an index into the global <code>SysOpenFile</code> table and an integer offset represeting a processes current position in that file.</p></li>
<li><p>Modify the <code>AddrSpace</code>'s' PCB to contain an array of <code>OpenUserFiles</code>. Limit the number to something reasonable, but greater than 20. Write a method (in <code>PCBManager</code>) that returns an <code>OpenUserFile</code> object given the filename.</p></li>
<li><p>Implement <code>Create(char *fileName)</code>. This is a straight forward call that should simply get the <code>fileName</code> from user space then use <code>fileSystem-&gt;Create(fileName,0)</code> to create a new instance of an <code>OpenFile</code> object. Until a user opens the file for IO it is not necessary to do anything further.</p></li>
<li><p>Implement <code>Open(char *fileName)</code>. This function will use an <code>OpenFile</code> object created previously by <code>fileSystem-&gt;Open(fileName)</code>. Once you have this object, check to see if it is already open by some other process in the global <code>SysOpenFile</code> table. If so, incriment the <code>userOpens</code> count. If not, create a new <code>SysOpenFile</code> and store it's pointer in the global table at the next open slot. You can obtain the <code>FileID</code> by looking up the name in your <code>SysOpenFile</code> table.</p>
<p>Then create a new instance of an <code>OpenUserFile</code> object (given a <code>SysOpenFile</code> object) and store it in the current thread's PCB's <code>OpenUserFile</code> array.</p>
<p>Finally, return the <code>FileID</code> to the user.</p></li>
<li><p>Implement a function to read/write into <code>MainMem</code> and a buffer given a staring virtual address and a size. It should operate in the same way <code>AddrSpace::ReadFile</code> writes into the main memory one <code>diskBuffer</code> at a time. It may help to put the section of the code in <code>ReadFile</code> into a <em>helper</em> function called <code>userReadWrite()</code> that is general enough that both <code>ReadFile()</code>, <code>myRead()</code> and <code>myWrite()</code> can call. It need only be parameterized by the type of operation to be performed (<code>Read</code> or <code>Write</code>).</p>
<p>When called by <code>Write()</code>, it will read from the <code>MainMem</code> addressed by the virtual addresses. It writes into the given (empty) buffer. <code>Write()</code>, will then put that buffer into an <code>OpenFile</code>. When called by <code>Read()</code>, It will write into <code>MainMem</code> the data in the given (full) buffer that <code>Read()</code> read from an <code>OpenFile</code>.</p></li>
<li><p>Implement <code>Write(char *buffer, int size, OpenFileId (int)id)</code>. First you will need to get the arguments from the user by reading registers 4-6. If the <code>OpenFileID</code> is <code>ConsoleOutput</code> (defined in <code>syscall.h</code>), then you call <code>PutChar()</code> in <code>console.cc</code> to print the buffer content. Otherwise, grab a handle to the <code>OpenFile</code> object from the user's openfile list pointing to the global file list. Why can't you just go directly to the global file list? Becuase the user may not have opened that file before trying to write to it. Once you have the <code>OpenFile</code> object, you should fill up a buffer using your <code>userReadWrite()</code> function. Then simply call <code>OpenFileObject-&gt;Write(myOwnBuffer, size)</code>;</p></li>
<li><p>Implement <code>Read(char *buffer, int size, OpenFileId id)</code>. Get the arguments from the user in the same way you did for <code>Write()</code>. If the <code>OpenFileID</code> is <code>ConsoleInput</code>, use a routine (<code>GetChar()</code> in <code>console.cc</code>) to read into a buffer one character at a time. Otherwise, grab a handle to the <code>OpenFile</code> object in the same way you did for <code>Write()</code> and use <code>OpenFileObject-&gt;ReadAt(myOwnBuffer,size,pos)</code> to put <em>n</em> characters into your buffer. <code>pos</code> is the position listed in the <code>UserOpenFile</code> object that represents the place in the current file you are writing to. The number read is returned from <code>ReadAt()</code>.</p>
<p>Now that your buffer is full of the read, you must write that buffer into the user's memory using the <code>userReadWrite()</code> function. Finally, return the number of bytes written.</p></li>
<li><p>Test your system calls using your own test programs, then the ones in <code>~cs170/nachoos-projtest/proj2</code> Now that both parts are finished all test programs should work.</p></li>
<li><p>Test some more.</p></li>
</ol>
<h4 id="faq">FAQ</h4>
<blockquote>
<p><strong>Question</strong>: How can I test part one (why doesn't printf work in my test programs)?</p>
</blockquote>
<p><strong>Answer</strong>: Until you have part 2 done you will have to test part 1 based on flow of execution. For example: to test if Fork/Exec works you Fork/Exec a function/executable and then call Halt() in the function/executable. If Nachos halts when you run your test program then your system call is probably working. Once you have the Write system call implemented you can further test part 1 by putting your Write statements where ever you wish to print something out. This will be useful for testing Join.</p>
<blockquote>
<p><strong>Question</strong>: What does a process control block contain?</p>
</blockquote>
<p><strong>Answer</strong>: A process control block (pcb) contains the attributes of a process. Some of the major attributes are the thread, the pid (SpaceID), open files, etc (remember that the thread has a pointer to the addrspace which is an indirect attribute of a pcb). There should be a global table of process control blocks as well. Remember that you will also want to be able to get a particular process's condition so that it can be waited on in Join (if necessary) and broadcast in Exit. It may not seem like you are using the process control blocks much in part 1 of the project (except for adding and deleting them) but you will use them more in part 2 of the project).</p>
<blockquote>
<p><strong>Question</strong>: How do I translate the name of the executable when implementing Exec?</p>
</blockquote>
<p><strong>Answer</strong>: You will want to put a translate function in addrspace that is similar to the translate function in Machine. You will use this function to translate a virtual address into a physical address (one page at a time).</p>
<blockquote>
<p><strong>Question</strong>: Is there a difference between the parameters of Exec and Fork?</p>
</blockquote>
<p><strong>Answer</strong>: Yes there is. Exec takes the string representing the relative path (from where you run Nachos - userprog in this case) to the test program you wish to exec [ie. Exec(&quot;../test/myExecedProgram&quot;)]. Fork takes the name of the function you wish to fork. It is not a string, but a function pointer so it has the form Fork(myFunction) where you have implemented void myFunction() previously in you test program. In Exec you are translating the string that is passed in and in Fork you are using the pointer to myFunction as the PCReg value for when you run the new thread. If you try to pass a string to Fork or a name (not in string form) to Exec you will have serious problems</p>
<blockquote>
<p><strong>Question</strong>: Does a Forked thread use the same addrspace as the thread who Forked it?</p>
</blockquote>
<p><strong>Answer</strong>: No it does not. It uses a COPY of the addrspace from the thread who Forked it. This means you must create a new addrspace that has the same size page table as the Forking thread and then you must copy the Forking thread's pages in memory into the Forked threads pages in memory.</p>
<blockquote>
<p><strong>Question</strong>: The project spec says we need to add a function ReadFile to Addrspace. What is this used for?</p>
</blockquote>
<p><strong>Answer</strong>: This is used for copying the executable's code and data segments into memory (in the constructor for Addrspace (that Exec calls). noffH.code.virtualAddr is the logical address of the executable's code segment and noffH.initData.virtualAddr is the logical address of the executable's data segment. You no longer want to zero out (bzero) the memory. You need to copy these sections page by page into main memory. You will use your Addrspace::Translate to translate the logical data/code address to the physical address (in memory). Then you can use the C/C++/Unix &quot;bcopy&quot; to copy it over.</p>
<blockquote>
<p><strong>Question</strong>: Where should we put the global structures/classes?</p>
</blockquote>
<p><strong>Answer</strong>: You can put these in system.h in the threads directory. Only put the global variables (their declaration) in this file. Make sure you actually instantiate the global structures (like memory manager) in the system.cc file in the initialize function under the correct #ifdef's (for userprog in this case).</p>
<blockquote>
<p><strong>Question</strong>: Does the Exit system call take a parameter? What is that parameter?</p>
</blockquote>
<p><strong>Answer</strong>: The Exit system call should take an integer parameter. This parameter is the exit value of the process. This is the same as the exit values in Unix (0 -&gt; good, 1 -&gt; bad). For our purposes you can use any integer here. It only matters that if another process was &quot;Join&quot;ing on your process that the Join call would return the value that your process exited with (as per the project spec). This means that you need to save your exit value some how in the Exit system call so that Join can return it if necessary (even if you've already exited when someone calls Join on your process).</p>
<blockquote>
<p><strong>Question</strong>: What should we do when we can't allocate enough physical pages in Addrspace constructor for a new thread?</p>
</blockquote>
<p><strong>Answer</strong>: You should let Fork/Exec know that you don't have enough memory so that it can let the user know (return pid of -1) that it didn't Fork/Exec a new process. Make sure that you check if there is enough free pages in physical memory to facilitate the number of pages for your new process before trying to allocate the physical pages for each logical page. If you do not do this one of your allocates will return -1 and you will have to deallocate all the pages you just allocated before you can let Fork/Exec know of the error.</p>
<blockquote>
<p><strong>Question</strong>: Do Exec and Fork call thread-&gt;Fork?</p>
</blockquote>
<p><strong>Answer</strong>: Yes they do. After you have set all other information up (as in the project spec) you need to thread-&gt;fork a dummy function (that's in exception.cc). In this dummy function you will want to initialize and restore the registers (for Fork and Exec) and then set up the PC registers and return register address (for Exec only). After this you will call machine-&gt;Run() from the dummy function (for both Fork and Exec).</p>
<h3 id="project-3-virtual-memory-management">Project 3 Virtual memory management</h3>
<h4 id="additional-information">Additional information</h4>
<p>In this assignment, we will leverage the following three bits maintained in each page table entry defined in <code>machine/translate.h</code>: <strong>valid</strong>, <strong>dirty</strong>, and <strong>use</strong> bits.</p>
<p>The valid bit indicates if a page is resident in physical memory or not. If the valid bit in a particular page table entry is set, the MIPS machine assumes that the corresponding virtual page is loaded into physical memory in the physical page frame specified by the physicalPage field. Whenever the program generates an access to that virtual page, the MIPS machine accesses the physical page. If the valid bit is not set, the system generates a page fault exception whenever the program accesses that virtual page. Your exception handler will then find a free physical page frame, read the page in from the backing store SWAP to that physical page frame, update the page table to reflect the new virtual to physical mapping, and restart the program. The MIPS machine will then resume the execution of the program at the instruction that generated the fault. This time the access should go through, since the page has been loaded into physical memory.</p>
<p>The dirty bit indicates if this page has been modified or not. To find a free frame, your page fault handler may need to eject an in-memory victim page. If the ejected victim page has been modified in the physical memory indicated by the dirty bit in the corresponding page table entry, the page fault handling code must write the page out to the backing store before reading the accessed page into its physical page frame.</p>
<p>The use bit indicates if this page has been recently accessed. On a page fault, the kernel must decide which page to replace; ideally, it will throw out a page that will not be referenced for a long time, keeping pages in memory those that are soon to be referenced. You are asked to implement the second chance algorithm leveraging this use bit. This bit is set to <code>TRUE</code> every time a user program reads or writes this page. The second chance algorithm follows a clockwise order to find the next victim whose use bit is <code>FALSE</code>. If it sees a page with the use bit value <code>TRUE</code>, it gives this page the second chance to stay, and the use bit of this page should be set as <code>FALSE</code>.</p>
<h4 id="qa">Q&amp;A</h4>
<blockquote>
<p>Where to modify for Task 1?</p>
</blockquote>
<p>For this part, you will extend <code>virtualmemorymanager.cc</code> to rewrite Function <code>swapPageIn()</code>.</p>
<blockquote>
<p>What are relevant files to read?</p>
</blockquote>
<p>Files in <code>vm</code> directory, <code>userprog</code> and <code>machine</code> directories. The given starter code is one possible approach and other designs are also possibles.</p>
<blockquote>
<p>Where to start?</p>
</blockquote>
<p>First, understand how address translation works in method <code>Machine::Translate</code> in <code>machine/translate.cc</code>, where this method is used, and how <code>PageFaultException</code> can be thrown in this process.</p>
<p>Figure out how memory access is implemented and where the virtual to physical translation is performed in using the above method. You can trace the MIPS simulator (in <code>machine/mipssim.cc</code>) in executing instructions which trigger Page Fault.</p>
<p>It is during the translation process that the machine can determine if the virtual address it is trying to access belongs to a page which does not reside in physical memory. Figure out how, when and where the <code>PageFaultException</code> is thrown.</p>
<blockquote>
<p>Where is the page fault handled?</p>
</blockquote>
<p>The page fault exception is handled in a manner similar to system calls in <code>exception.cc</code>. As a part of raising the page fault exception, the MIPS machine saves the faulting virtual address in a register that will be used by <code>exception.cc</code> to call <code>VirtualMemoryManager::swapPageIn()</code>.</p>
<blockquote>
<p>When is the process image stored on the disk and how is SWAP managed?</p>
</blockquote>
<p>When a binary file is loaded, its process image including binary code and data sections is stored in the SWAP area.</p>
<p>For Project 3, SWAP is essentially a Linux file implemented through the stub interface of the Nachos file system (look into <code>filesys/filesys.h</code> and <code>filesys/openfile.h</code>). SWAP contains 512 sectors and each sector has 128 bytes with the same size of a memory page.</p>
<p>A swap bitmap (<code>swapSectorMap</code>) is allocated in <code>VirtualMemoryManager</code> to keep track the use of each sector of SWAP with a bit. Function <code>virtualMemoryManager::allocSwapSector()</code> is used by <code>addrspace.cc</code> to allocate a sector to store a page of the process image during binary loading.</p>
<blockquote>
<p>How many memory pages are allocated to each process when it starts during initial loading (Nachos <code>-x</code> option), <code>Exec()</code>, or <code>Fork()</code>?</p>
</blockquote>
<p>Zero. As this process executes an instruction or accesses a user-space data address, this generates page fault exception and triggers the page fault exception handling.</p>
<blockquote>
<p>In Task 1, what are main steps of page fault handling in Function <code>SwapPageIn()</code>?</p>
</blockquote>
<p>The number of C++ code lines you write for this task is within 50.</p>
<ul>
<li>Perform the second-chance algorithm as approximated LRU by checking the use bit of the next page.</li>
<li>If the next page's use bit is <code>TRUE</code>, give a second chance by setting the use bit as <code>FALSE</code>.</li>
<li>If the next's use bit is <code>FALSE</code>, you find a victim page.</li>
<li>Check if the selected victim page is dirty, initiate I/O to write the contents of the victim page to SWAP.</li>
<li>Adjust the pagetable for the process to which the victim page belongs to reflect the fact that it is no longer resident in memory.</li>
<li>Initiate I/O to load the desired page for which the fault was generated from the backing store SWAP.</li>
<li>Adjust the pagetable for the faulting process to reflect the fact that this selected physical page points to the new process space and the desired virtual page is now resident in memory.</li>
</ul>
<blockquote>
<p>What are main data structures and functions that can be leveraged in Function <code>SwapPageIn()</code>?</p>
</blockquote>
<ul>
<li>The basic data structure manipulated by this function is page table entries. The fields of a page table entry defined in <code>machine/translate.h</code> of this starter code include the virtual page number of a process space (<code>virtualPage</code>), the physical page number (<code>physicalPage</code>), the use bit (<code>use</code>), the dirty bit (<code>dirty</code>), the valid bit (<code>valid</code>). The page table entry defined in the starter code also includes The offset of the corresponding sector in SWAP by bytes (<code>locationOnDisk</code>). We ask you to remove this storage location field and store such information in the process address space in <code>addrspace.h</code>.</li>
<li><code>FrameInfo</code> data structure for a physical page includes the process space this physical page belongs to and the virtual page number (<code>pageTableIndex</code>) in that space.</li>
<li>Other key functions in <code>VirtualMemoryManger</code> include <code>allocSwapSector()</code> to get a free SWAP sector, <code>writeToSwap()</code> that performs disk I/O to write back a page to a sector in SWAP, <code>loadPagetoCurrVictim()</code> that loads the corresponding virtual page to the assigned physical page.</li>
<li><code>VirtualMemoryManager::getPageTableEntry(FrameInfo *physPageInfo)</code> returns the page table entry of process space that currently owns this physical page.</li>
</ul>
<blockquote>
<p>How to do Task 2?</p>
</blockquote>
<p>The total number of line changes is expected to be small.</p>
<ul>
<li>Use <code>grep LocationOnDisk */*.cc</code> or compile after removing <code>LocationOnDisk</code> field in <code>machine/translate.h</code> to see where this filed has been used. Understand how it has been used in storing/loading a process page to/from the disk.</li>
<li>Add a field in <code>AddrSpace</code> class that is initialized as a pointer to an integer array that gives the SWAP section location of each virtual page.</li>
<li>Implement <code>putLocationOnDisk(int virtualPage)</code> and <code>getLocationOnDisk(int   virtualPage)</code> methods.</li>
<li>Replace the read/write access of <code>LocationOnDisk</code> field in all files with the above two methods.</li>
</ul>
<blockquote>
<p>How to print more debugging info?</p>
</blockquote>
<p>Use <code>-d v</code> argument. For example, <code>../vm/nachos -d v -x testvmfork</code>. The output format is explained as follows. For the following outputs, <code>pid</code> is the id of the process on which behalf the operation is performed. <code>virtualPage</code> is the involved virtual page number (i.e. the page index into the process virtual address space) and <code>physicalPage</code> is the involved physical page number (i.e., the page index into the physical memory of the Nachos virtual machine).</p>
<ol style="list-style-type: decimal">
<li><p>Whenever a page is loaded into physical memory, print</p>
<pre><code>L [pid]: [virtualPage] -&gt; [physicalPage]</code></pre></li>
<li><p>Whenever a page is evicted from physical memory and written to the swap area, print</p>
<pre><code>S [pid]: [physicalPage]</code></pre></li>
<li><p>Whenever a page is evicted from physical memory and not written to the swap area, print</p>
<pre><code>E [pid]: [physicalPage]</code></pre></li>
<li><p>Whenever a process obtains a zero-filled demand page for the first time (i.e., when you allocate and zero the page out), print</p>
<pre><code>Z [pid]: [virtualPage]</code></pre></li>
</ol>
<blockquote>
<p>Is synchronization needed for concurrent processes in executing system calls?</p>
</blockquote>
<p>You should worry about synchronization issues. In general, two kernel activities such as system calls or exception handling may be handled concurrently and context switch is possible. For example, one process handles page fault, which triggers asynchronous I/O for dirty page disk writing or page loading and can sleep in the kernel while another process may initiate I/O on the same page at the same time. Synchronization such as locks can be needed for each memory page.</p>
<p>In Project 3, the concern is less as long as there is no explicit kernel yield or sleep involved to trigger context switching during the middle of a kernel service. Notice that Nachos disk I/O involved in Projects 2 and 3 uses the Linux file system in the lower level synchronously, which does not trigger kernel thread yield/sleep, and that is not true in a real OS.</p>
<blockquote>
<p>Is TLB used in Nachos?</p>
</blockquote>
<p>The current implementation disable s the TLB feature available in Nachos.</p>
</body>
</html>
